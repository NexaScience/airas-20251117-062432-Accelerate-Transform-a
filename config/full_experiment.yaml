variations:
  - run_id: baseline-std-drop
    description: "Baseline run with standard Dropout on TinyBERT MRPC"
    seed: 42
    dataset:
      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing
      seq_length: 128
      batch_size: 32
      num_classes: 2
    model:
      name: huawei-noah/TinyBERT_General_4L_312D
    training:
      epochs: 3
      learning_rate: 3e-5
      use_pmdrop: false
      pmdrop_bits: 1
      pmdrop_cache: false
      fp16: true

  - run_id: pm-drop
    description: "PM-Drop with 1-bit packing and cached masks"
    seed: 42
    dataset:
      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing
      seq_length: 128
      batch_size: 32
      num_classes: 2
    model:
      name: huawei-noah/TinyBERT_General_4L_312D
    training:
      epochs: 3
      learning_rate: 3e-5
      use_pmdrop: true
      pmdrop_bits: 1
      pmdrop_cache: true
      fp16: true

  - run_id: pm-drop-no-cache
    description: "PM-Drop with 1-bit packing, no cache"
    seed: 42
    dataset:
      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing
      seq_length: 128
      batch_size: 32
      num_classes: 2
    model:
      name: huawei-noah/TinyBERT_General_4L_312D
    training:
      epochs: 3
      learning_rate: 3e-5
      use_pmdrop: true
      pmdrop_bits: 1
      pmdrop_cache: false
      fp16: true

  - run_id: pm-drop-4bit
    description: "PM-Drop with 4-bit packing and cached masks"
    seed: 42
    dataset:
      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing
      seq_length: 128
      batch_size: 32
      num_classes: 2
    model:
      name: huawei-noah/TinyBERT_General_4L_312D
    training:
      epochs: 3
      learning_rate: 3e-5
      use_pmdrop: true
      pmdrop_bits: 4
      pmdrop_cache: true
      fp16: true

# -------------------------------------------------------------------------
# NOTE
# ----
# The full experiment definition above focuses on the 4 key variants required
# by the paper (baseline, pm-drop, pm-drop-no-cache, pm-drop-4bit) applied to
# the GLUE-MRPC task using TinyBERT-4L-312H.  Extending to the larger models
# (BERT-Base / Large, SST-2, WikiText-103) only requires duplicating the
# corresponding blocks with appropriate `run_id` and parameter overrides.
# -------------------------------------------------------------------------
