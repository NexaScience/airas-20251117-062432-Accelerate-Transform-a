{
  "research_topic": "Accelerate Transformer training",
  "queries": [
    "transformer training acceleration"
  ],
  "research_study_list": [
    {
      "title": "Tempo: Accelerating Transformer-Based Model Training through Memory Footprint Reduction",
      "abstract": "Training deep learning models can be computationally expensive. Prior works have shown that increasing the batch size can potentially lead to better overall throughput. However, the batch size is frequently limited by the accelerator memory capacity due to the activations/feature maps stored for the training backward pass, as larger batch sizes require larger feature maps to be stored. Transformer-based models, which have recently seen a surge in popularity due to their good performance and applicability to a variety of tasks, have a similar problem. To remedy this issue, we propose Tempo, a new approach to efficiently use accelerator (e.g., GPU) memory resources for training Transformer-based models. Our approach provides drop-in replacements for the GELU, LayerNorm, and Attention layers, reducing the memory usage and ultimately leading to more efficient training. We implement Tempo and evaluate the throughput, memory usage, and accuracy/loss on the BERT Large pre-training task. We demonstrate that Tempo enables up to 2x higher batch sizes and 16% higher training throughput over the state-of-the-art baseline. We also evaluate Tempo on GPT2 and RoBERTa models, showing 19% and 26% speedup over the baseline.",
      "full_text": "Tempo: Accelerating Transformer-Based Model Training through Memory Footprint Reduction Muralidhar Andoorveedu1, Zhanda Zhu2,3, Bojian Zheng1,3, Gennady Pekhimenko1,3 1University of Toronto, Toronto, Canada 2Shanghai Jiao Tong University, Shanghai, China 3Vector Institute, Toronto, Canada {andoorve, zhanda, bojian, pekhimenko}@cs.toronto.edu Abstract Training deep learning models can be computationally expensive. Prior works have shown that increasing the batch size can potentially lead to better overall throughput. However, the batch size is frequently limited by the accelerator memory capacity due to the activations/feature maps stored for the training backward pass, as larger batch sizes require larger feature maps to be stored. Transformer-based models, which have recently seen a surge in popularity due to their good performance and applicability to a variety of tasks, have a similar problem. To remedy this issue, we propose Tempo, a new approach to efﬁciently use accelerator (e.g., GPU) memory resources for training Transformer-based models. Our approach provides drop-in replacements for the GELU, LayerNorm, and Attention layers, reducing the memory usage and ultimately leading to more efﬁcient training. We implement Tempo and evaluate the throughput, memory usage, and accuracy/loss on the BERTLARGE pre-training task. We demonstrate that Tempo enables up to 2 × higher batch sizes and 16% higher training throughput over the state-of-the-art baseline. We also evaluate Tempo on GPT2 and RoBERTa models, showing 19% and 26% speedup over the baseline. 1 Introduction Transformer-based models such as BERT [ 12] and GPT-2 [49] have found success in numerous general natural language processing tasks including question answering [ 51], paraphrasing [ 13], natural language inference [68], and even areas outside language tasks such as image recognition [14]. However, training such models can be highly expensive in terms of time, monetary resources and carbon footprint [24, 60]. For instance, the pre-training of BERTLARGE takes 4 days to complete on 16 Cloud TPUs (64 TPU chips total) [12], which costs about $10,000 [56]. Training a more recent Transformer-based model, GPT-3, has an even more astonishing price tag - $12 million[66]. Hence, even a small decrease in the end-to-end training time of Transformer-based models matters. Although there has been signiﬁcant progress made in accelerating Transformers using specialized hardware (e.g., Google TPUs [30], NVIDIA Tensor Cores [39]) in the past few years, a fundamental issue with Transformer-based models is that they are limited by the memory capacity of hardware accelerators. For example, even a batch size of 1 does not ﬁt into a modern GPU with 12GB of memory when training BERT with sequence length 512 [15]. Reducing memory footprint [48, 8, 52] is a viable option to allow larger batch training, leading to better hardware utilization and ultimately improved training throughput [73]. Many existing approaches to memory footprint reduction (e.g., ofﬂoading [52, 65, 48], checkpoint- ing [8, 73, 33, 28], and data compression/encoding [26, 6]) either have high computational overhead or do not apply to Transformer-based models directly. Prior approaches fall into two main categories, Preprint. Under review. arXiv:2210.10246v2  [cs.LG]  24 Jan 2023neither of which are satisfactory for the Transformer-based model case. First, these techniques may be too general [48, 6, 50, 33, 28] to utilize the speciﬁcs of Transformer-based models well, such as the multi-headed attention mechanism used in Transformers [63], or optimization opportunities available in speciﬁc layers such as the LayerNorm [4] layer. For example, although checkpointing [8, 28] can signiﬁcantly enlarge batch size, it also brings high overhead (e.g., 30% performance degradation observed in some prior works [8]). Second, if prior works are speciﬁc, they focus on other types of models/layers with ideas not being applicable to Transformers. For example, Gist and In-Place ABN deal with CNNs [26, 53]. In our work, we demonstrate that low overhead memory footprint reduction can lead to a positive improvement in throughput. In addition, unlike prior works which do not leverage the speciﬁcs of Transformer-based models, we propose a new approach speciﬁcally tailored for Transformer-based models, called Tempo. This approach includes three new techniques: (i) In-place GELU, (ii) In-place LayerNorm, and (iii) Sub-Layer Dropout Recomputation. In-place GELU and In-place LayerNorm both use alternative derivations for the computation of the backward passes of these layers. These derivations allow some activations that are normally retained during the forward pass (to be later used in the backward pass) to be discarded, leading to a more memory-efﬁcient implementation. Sub-Layer Dropout Recomputation discards activations within the high memory footprint attention mechanism during the forward pass, then recomputes these during the backward pass without recomputing extra unnecessary tensors. Tempo is able to increase the training throughput with larger batch sizes by reducing the total memory footprint of the models during training. To our best knowledge, this is the ﬁrst work to explore memory footprint optimizations speciﬁcally for Transformer-based layers that show not just footprint reduction, but the actual increase in throughput using the extra memory savings. Tempo reduces the memory footprint of training Transformer-based models by targeting a major part of the total footprint – the activation memory [74] (the saved feature maps during the forward pass of the model that are required for backpropagation [54]). All the proposed techniques provide a large memory footprint reduction with very low throughput degradation (as low as 1%). Our results show up to 2×improvement in batch size for BERTLARGE pre-training at a sequence length of 512 on modern GPUs while increasing the training throughput by up to 16% . 2 Background and Motivation 2.1 Memory Footprint of BERT BERT [12] is a popular natural language processing model that is based on the Transformer ar- chitecture [63]. The model has been successfully applied to a variety of tasks such as question answering (SQuAD [51]), paraphrasing (MRPC [13]), natural language inference (MNLI [68]), and others [57, 72] through a two step training process. The training process entails ﬁrst training on a general unlabelled data set ( pre-training) [12]. The faster second part of the training process (ﬁne-tuning) takes the parameter weights produced by the pre-training section and further trains on a downstream task such as question answering [51] or sentiment analysis [57] which it accomplishes through the addition of a specialized output layer [12]. The BERT architecture allows for multiple different conﬁgurations depending on model hyperparam- eters selected, some being derived from the original Transformers paper; these include the hidden layer size (H), sequence length (S), number of attention heads (A) and number of layers (L). In the context of this work, we point out some of the relevant parts of the model and their activation memory footprint with respect to these hyperparameters referring to Figure 1. 1⃝At this point, where attention [63] is calculated we observe that the size of each of the feature maps goes as O(S2)−there are a variety of previous techniques and models that have been explored in the literature to deal with this problem [ 61]. Additionally, at this point note that we store three feature maps of size [B×A×S2]. Calculations based on Figure 1 at the BERTBASE parameters show that at a sequence length of 512 these three feature maps account for 56% of the encoder layer activation memory. 2⃝At these two points, we store the input to the two LayerNorm layers of size [B×S×H] 3⃝Here a GELU [21] layer is used as the activation function for the preceding fully-connected layer of size [B×S×4H]. The activation memory for this function stores almost 17% of the total layer activation memory of BERTBASE at a sequence length of 128. 2Figure 1: A diagram of a single Transformer encoder [63] layer used in BERT [12]. This is based on the Huggingface implementation of BERT [69]. As in the BERT paper, Arepresents the number of attention heads, and Hrepresents the hidden size. We represent the batch size by Band the sequence length by S. Sizes of intermediate tensors (both retained activations and unretained intermediates) are annotated. 2.2 Why Activation Memory Matters As iterated in previous works [52, 26, 48, 73, 6] there are multiple beneﬁts to reducing the memory footprint of models. First, it allows for larger models which can positively affect the model’s performance on downstream tasks [12]. Second, memory footprint reduction can allow for a larger batch size. This, in turn, could lead to better utilization of the GPU hardware [ 17], increasing the overall throughput [73]. In order to verify this possibility for Transformer-based models, we conduct our own experiments using Huggingface’s BERT implementation [69] to train BERTLARGE on the MRPC [13] ﬁne-tuning task. Figure 2 shows the throughput on this task for sequence lengths of 128 and 512. From the ﬁgure, we conclude that there is a steady improvement in batch size when the sequence length is 128. This is also the case when the sequence length is 512, however, in this situation the trend ends more abruptly as the memory consumption of the model exceeds the GPU memory capacity, showing a clear opportunity to take advantage of memory footprint reduction. 0 2 4 6 8 10 12 14 16 10 20 30 Batch Size Throughput (Sequences/s) S = 128 S = 512 Figure 2: Plots of throughput (sequences/s) vs batch size for BERTLARGE [12] ﬁne-tuning on the MRPC [13] task at sequence lengths 128 and 512 on four 2080Ti [40] GPUs. The maximum batch sizes are respectively 16 and 2. We note that previous works on Transformer-based models show that although the model parameters contribute to the memory footprint, the main memory capacity consumer during training is actually the activation feature maps [ 74, 28, 8, 48, 26, 33, 6]. In addition, the majority of this activation memory will be used in each of the BERT Transformer encoder layers. Proﬁling the Huggingface BERTBASE implementation [69] on the MRPC [ 13] ﬁne-tuning task at a batch size of 32 and 3sequence length of 128 shows that 66% of the total memory is taken up by these encoder activations. More details on this are shown in Appendix A. 2.3 Key Prior Works There are three major prior techniques used in training memory footprint reduction of deep learning models. The ﬁrst of these is Checkpointing [8, 33, 28, 73]. This technique involves discarding certain feature maps in the forward pass while retaining others. Later, in the backward pass, these discarded feature maps may be recomputed from the retained feature maps, and thus used in the computation of the gradients. The second technique is Ofﬂoading [48, 52, 65]. In this case, the main idea involves taking feature maps that would be stored in the GPU memory, and instead ofﬂoading them to the CPU memory. These techniques can also involve pre-fetching tensors from the CPU memory in anticipation of their use. Ofﬂoading suffers from a dependence on system variables such as the communication channel bandwidth [52, 48]. It also requires extensive engineering effort to avoid high overhead [6]. Finally, Compression/encoding; this can be divided into two different categories, lossless and lossy [26, 6]. However, the fundamental idea is to compress, or reduce the space taken up by feature maps in the forward pass, then decompress it for use in the backward pass. These techniques are usually largely orthogonal to one another as was shown in prior works where both ofﬂoading and checkpointing are used simultaneously [48, 65]. We expand on these techniques in Appendix C. 2.4 Why Tempo? Although the techniques in the previous section show good performance on a variety of models, they suffer from a variety of issues. Checkpointing’s scope is often too broad to consider certain layer-speciﬁc optimizations and alternative derivations that can provide lower overhead [53]. Fur- thermore, overhead can be high (as much as 30%) [ 8]. Ofﬂoading can be system- dependent and requires signiﬁcant engineering effort, while compression can be lossy or not applicable to the Transformer case. Hence, there is a clear need for a deeper look at activation memory optimizations for Transformer-based neural networks in particular. To our best knowledge, our work is the ﬁrst to explore such optimizations tuned to improving the throughput of Transformer-based models. Table 1 shows a summary comparison of Tempo and various other techniques, with the major points that differentiates our technique from prior work. Feature CapuchinCheckmateActNN Gist Tempo Layer-Speciﬁc \u0017 \u0017 \u0017 \u0013 \u0013 Transformer-Speciﬁc \u0017 \u0017 \u0017 \u0017 \u0013 Lossless \u0013 \u0013 \u0017 ∼1 ∼2 Drop-In Layer Replacement \u0017 \u0017 \u0013 \u0013 \u0013 Online \u0013 \u0017 \u0013 \u0013 \u0013 Table 1: Comparison between Tempo and Capuchin [48], Checkmate [28], ActNN [6], and Gist [26]. 3 Tempo: Key Ideas We now present the major ideas that lays behind the design of Tempo: (1) In-place GELU, (2) In-place LayerNorm, and (3) Sub-Layer Dropout Recomputation. The major theme behind all of these ideas is to compute the backward pass as normal, while using less storage to do so. To this end, In-place GELU and In-place LayerNorm compute the output of each layer in-place; instead using the output activation to compute the gradient. Sub-Layer Dropout Recomputation also discards the output, and through a closer look at the structure of the Dropout layer is able to recompute the output without excessive recomputation. We strongly suggest reading Appendix E for the implementation details. We also add in this appendix a new optimization of softmax that we use that further reduces memory [18]. 1Some of the Gist [26] optimizations are lossy. 2Accuracy of our lossy optimization is tunable, offering a ﬂexible tradeoff between the accuracy and the hardware cost. 43.1 In-place GELU The GELU layer is used as an activation function for the feed-forward section of the BERT layer ( 3⃝ in Figure 1) [12]. A plot of this function is shown in Figure 3a. Referring to the baseline in Figure 3b, note that both X and Y are stored for the backward pass. Y is needed for the downstream fully connected layer, while X is stored for the GELU layer itself [46]. Prior work has demonstrated that certain activation functions such as ReLU may be computed in-place [26]. This can be done without affecting the calculation of the backward pass. If we were able to compute the GELU function in-place, potentially by recovering the input from the output on the backward pass, we could save the storage required for X. However, this is impossible to do directly. A key observation to make with respect to the GELU function is that it is not bijective – hence there is no function that will be able to compute the input from the output without additional information. However, we observe that the GELU function is both continuous and has only one extremum, a minimum value at x ≈−0.75179 as can be seen in Figure 3a. Notably, this implies that just one additional piece of information: which side of the minimum the input originates from, allows us to compute the inverse of the GELU. This is because on each side of the minimum the function is one-to-one, and hence the input is recoverable from the output in each section. Based on this key observation, we can discard the input, and simply retain the output of the GELU, as well as the additional information on whether the input is greater than or equal to the value at which the minimum occurs. Figure 3b illustrates the difference between our method and the baseline. In order to execute this efﬁciently on a real system, we note that the original derivative in terms of the input can be composed with the function inverse in order to create a composite kernel. This kernel consists of a polynomial approximation of this composite function, the approximation being necessary since GELU is transcendental, and therefore the inverse cannot be solved in terms of elementary functions [58]. Further details are discussed in Appendix E. Figure 3(a): A plot of the GELU[21] function near the origin, along with the marked minimum point. Figure 3(b): Saved feature maps between the base- line and Tempo. Note that our method only saves a 8-bit mask 3 that denotes whether the input is greater or less than the minimum value, instead of the full 32-bit input feature map. 3.2 In-place LayerNorm The LayerNorm layer is used at multiple points in the Transformer encoder layer [ 63], which we denote by 2⃝in Figure 1. Usually, the gradient computation of LayerNorm relies on the gradient input from the next layer, as well as the input feature map which is stashed for this computation [46]. Similar to GELU, we are able to derive an expression for the gradient of the LayerNorm layer as a function of its output. In this context, the output of LayerNorm must be stashed to compute the gradient of the successive fully connected layer anyways. Using this approach, the memory footprint overhead of LayerNorm is just the intermediate mean and variance computed in the forward pass. The full derivation is presented in Appendix E which is extended from the treatment of BatchNorm in [53]. Comparison with Checkpointing : Note that although In-place GELU requires more memory compared to recomputing Y from X, it will have increased overhead due to the recomputation. 3Pytorch boolean masks use 8-bits per value [46]. Masks can also be implemented as 1-bit manually but this brings extra overhead due to unpacking and packing bit tensors. 5Additionally, our technique is orthogonal to conventional Checkpointing, as it could take advantage of the fact that no recomputation is required for the input X for both In-place GELU and In-place LayerNorm. 3.3 Sub-Layer Dropout Recomputation In this section we explore the idea of sub-layer granularity checkpointing, or partial recomputation applied to the Dropout layer [59] found in 1⃝in Figure 1. The function of a dropout layer is to set the output of p% of entries in the incoming feature map to zero (“drop” the outputs) and then scale the remaining outputs by the factor 1 1−p, which makes the network less sensitive to any output of the preceding feature map, thereby making it more robust [59]. We deﬁne sub-layer recomputation as a technique where recomputation of only some of the feature maps is necessary for the backward pass that may be produced by a given layer’s output. We observe that better recomputation strategies are possible if we carefully deconstruct layers in the case where they store multiple outputs. This observation can be directly applied to the Dropout layer. In the computation of Dropout, both a mask (which records the entries which are set to zero in implementations of Dropout [ 46, 7]) and output are produced. If a layer-based checkpointing implementation [16] was used, it would cause both the mask and the output to be recomputed in the backward pass if the layer is checkpointed, thus requiring higher overhead. However, we notice that nothing precludes us from simply doing only one of these recomputations. Storing the mask would only reduce the recomputation (including memory transfer) time, while the fact that the mask itself only has Boolean values allows us to keep most of the memory beneﬁt of recomputation. In this way, we can save the storage required for the output at the criticalO(S2) Attention section ( 3⃝in Figure 1) for the cost of a simple mask multiply. This technique is illustrated in Figure 4. Figure 4: Comparison of dropout implementation between the baseline, and our method. Note that we only save the mask, and recompute the other output. The representation on the left is not an exact copy of the PyTorch implementation, rather it is an illustrative representation. 3.4 Other Engineering Optimizations We note that PyTorch uses a memory-inefﬁcient implementation of the softmax function which retains both the input and output of the function for the backward pass [46]. Instead, only the output is necessary. This optimization has also previously been implemented as part of some models in the Huggingface library [18]. We use this optimization as well in our implementation of the attention mechanism to further reduce the activation memory pressure. 4 Evaluation 4.1 Methodology Infrastructure Our main test setup consists of 4 NVIDIA RTX 2080 Ti GPUs [40], each with 11 GB of memory connected over PCIe v3 [47]. We also use an Amazon Web Services p3.8xlarge [3] instance consisting of 4 NVIDIA Tesla V100 GPUs [ 39] each with 16 GB of memory connected 6using NVLink [42]. For our ablation studies, we employ a system with an NVIDIA A100 GPU [43] with 40 GB of memory. We summarize the detailed setup in Appendix G. Applications We evaluate our work using both the BERT pre-training and ﬁne-tuning tasks [12]. For pre-training, we employ the NVIDIA DeepLearningExamples library [ 41] with the English Wikipedia dataset [67]. We perform the training in two phases, the ﬁrst (i.e., longer) phase at a sequence length of 128, and the second (i.e., shorter) phase at a sequence length of 512 [12, 41]. For throughput and memory experiments, we use the BERTLARGE conﬁguration. For our ﬁne-tuning task, we use the MRPC [13] paraphrasing task on BERTBASE using the Huggingface library [69]. For our ablation studies, we also train both RoBERTa [ 34] and GPT2[49]. For the evaluation of RoBERTa, we use the Fairseq library [ 44], while GPT2 uses the Huggingface GPT2 model [ 69]. Both of these models use the WikiText Dataset for evaluation [35]. Metrics The ﬁrst metric we focus on is the total memory footprint of our method compared to the baselines. There are two ways to look at this metric. First, we compare the maximum batch size possible for each method. We compare this across sequence lengths of 128 and 512 5 on BERTLARGE for both 2080Ti and V100 GPUs. Second, we compare the total memory used by PyTorch at a given commonly used batch size for the same parameters. The second metric we use is the throughput for which we count the total number of sequences per second processed. Finally, we provide a comparison between our method and the baseline method on BERTBASE pre-training in order to compare the loss curves and show the change due to our lossy optimizations. We also provide ﬁne-tuning curves on the MRPC [13] task, training for 10 epochs to ensure no signiﬁcant accuracy deviations. Our ablation studies only use the throughput metric. 4.2 Results We use two major baselines. The ﬁrst baseline is the NVIDIA BERT LARGE model [41], with no memory footprint techniques applied which we refer to as the Baseline. The second one is the same model, with the default checkpointing applied, based on the PyTorch implementation, applied at the input of each Transformer encoder layer [46, 41] and is similar to the Huggingface implementation [69]. We refer to this baseline as Checkpoint. We refer to our method that uses In-Place GELU, In-Place LayerNorm, Sub-Layer Dropout Recomputation, and the softmax engineering optimization as Tempo. Impact on Memory Footprint Table 2 shows the maximum batch size and memory consumed at a ﬁxed batch size for all three methods. Additionally, the total memory used at a batch size of 15 at a sequence length of 128 is 11.3 GB, 8.3 GB and 9.2 GB respectively for Baseline, Checkpoint, and Tempo. From this, we conclude that Checkpoint reduces the memory footprint to a much higher degree than both Baseline and Tempo. This is expected, as Checkpoint discards most of the feature maps to be recomputed [69, 41] no matter the performance cost. Tempo still provides a signiﬁcant increase in batch size over Baseline at the sequence length of 512 – we see 2×and 1.5×larger batches over Baseline for the 2080 Ti and V100 respectively but, as the next section shows, with much better throughput. Technique Sequence Length Batch Size Baseline 128 15 Baseline 512 1 Checkpoint 128 50 Checkpoint 512 4 Tempo 128 24 Tempo 512 2 Technique Sequence Length Batch Size Baseline 128 28 Baseline 512 4 Checkpoint 128 96 Checkpoint 512 18 Tempo 128 41 Tempo 512 7 Table 2: The maximum batch size on both 2080 Ti (left) and V100 (right) for BERTLARGE. Impact on Throughput Figure 5 illustrates our main results with respect to throughput. From the ﬁgure, we can see that Tempo outperforms both Checkpoint and Baseline across both sequence 5These are the sequence lengths of Phase 1 and Phase 2 of pre-training [12, 41]. 7lengths and across different hardware setups. We observe an improvement of 16% over Baseline on the 2080 Ti at a sequence length of 512. At these settings, we also have an improvement of 8% over Checkpoint. We also observe up to 27% over Checkpoint on the V100 at a sequence length of 512, which also corresponds to a 5% improvement over Baseline. This is despite the fact that Checkpoint uses the largest batch size as per Table 2. This is because Checkpoint stores feature maps at the beginning of each Transformer encoder layer, and recomputes these layers [69, 41]. Hence, an increased batch size also means more recomputation. In contrast, Tempo is able to decrease the total memory footprint, and then convert this decrease into a substantial performance improvement over the Baseline due to the use of only low overhead mechanisms. 128 512 0 100 200 1.04× 1.08× Sequence Length Throughput (Sequences/s) Base. Chk. Tempo Figure 5(a): 2080 Ti 128 512 1.03× 1.04× Sequence Length Figure 5(b): V100 Figure 5: Throughput experiments at the maximum batch size annotated with the speedup over the best baseline. Impact on Loss and Accuracy We pre-train BERTBASE to ensure that our model’s loss curve is not affected by approximate optimizations (e.g., In-Place GELU). Figure 6a shows the loss curve of phase 1 of BERTBASE pre-training [12]. We observe almost complete overlap in the loss curves with no more than a 0.5% difference between Tempo and the baseline at the endpoint. We conclude that within that margin of error our method is satisfactory. Figure 6(a): Phase 1 BERTBASE pre-training curve on the English Wikipedia dataset [67]. Figure 6(b): Accuracy of BERTBASE ﬁne-tuning [12] on the MRPC [13] task. We run 10 trials of 10 epochs. The solid line represents the median accuracy of these trials, and the maximum and minimum along the train- ing process by the transparent curves’ boundaries. For the ﬁne-tuning accuracy, we use the pre-trained Huggingface [69] implementation. Figure 6b shows the results of BERT LARGE ﬁne-tuning [12] on the MRPC [ 13] task. The ﬁgure shows a consistent overlap between the maximum and minimum accuracy of Tempo and the baseline, so we can conclude that Tempo has little impact on the accuracy of the trained model. 4.3 Ablation Studies Ablation Study With Respect to Larger Model Parameters on Modern Hardware Platforms We also evaluate on other hardware platforms as well as model parameters. First, we use an increased hidden layer size for various conﬁgurations. These experiments are conducted on a platform with 8an NVIDIA A100 GPU [43] across sequence lengths of 128 and 512. We maintain the hidden layer size H to the number of attention heads Aratio of 64 which is in line with prior works [63, 12]. The results are shown in Figure 7. The ﬁgure demonstrates two important generalizations of Tempo. First, note that even on newer and more advanced GPUs, Tempo continues to provide a tangible beneﬁt. Second, across larger hidden layer sizes Tempo consistently demonstrates a clear improvement over the baseline (as shown in the ﬁgure, this can be as high as a 39% speedup over Baseline which corresponds to a 16% speedup over Checkpoint). The speedup over Checkpoint is as high as 20% . We conclude that Tempo will continue to be applicable to new hardware and larger models. 128 512 0 0.5 1 1.5 1.07× 1.07× Normalized Throughput 128 512 1.04× 1.11× 128 512 1.02× 0.98× 128 512 1.15× 1.16× Baseline Checkpoint Tempo Sequence Length Figure 7: Normalized throughput at the maximum batch size, with annotated speedup over the best baseline. From left to right the conﬁgurations are (a) BERT LARGE (H = 1024), (b) BERTBASE H = 2048, (c) BERTLARGE H = 2048, (d) BERTBASE H = 3072. We also conduct experiments on BERTLARGE (modiﬁed to use 12 Layers instead of 24 for more data points) for sequence lengths larger than 512. Figure 8 shows the results for this experiment, where we demonstrate that Tempo outperforms Baseline on longer sequence lengths as well which can be as high as a 27% speedup over Baseline. At the same settings, we observe 16% speedup over Checkpoint. Tempo also outperforms Checkpoint by as much as 20%. We conclude that yet again Tempo will be able to take advantage of modern hardware, as well as remain advantageous as sequence lengths increase. Note that the largest sequence length of 3072 on Baseline does not have enough memory to run. 128 512 1024 2048 3072 0 0.5 1 1.5 1.06× 1.10× 1.16× 1.16× 1.10× Sequence Length Normalized Throughput Base. Chk. Tempo Figure 8: Normalized throughput relative to the Baseline across different sequence lengths on the NVIDIA A100 GPU for BERTLARGE modiﬁed to use 12 layers. We annotate each bar group with Tempo’sspeedup over the best baseline. Results on Other Models We conduct experiments on other Transformer-based models as well: RoBERTa [34] and GPT2 [ 49]. For the evaluation of RoBERTa, we use the Fairseq library [ 44] as well as a sequence length of 512, while GPT2 uses the Huggingface GPT2 model [ 69]. These experiments are conducted on both 2080 Ti and V100 setups. We note that the improvement over the baseline is substantial (up to 19% and 26% for GPT2 and RoBERTa respectively on the 2080 Ti 9setup. This improvement corresponds to a increase in batch size of 3×and 2×. Furthermore, we also see speedups of 5% and 4% on the V100 setup as well. From these results, we conclude that Tempo generalizes well to other Transformer-based models besides BERT. 5 Extensions 5.1 Extending In-place GELU The ideas used in section 3 for In-place GELU can be extended to general elementwise layers. The generic steps required for this are listed below, from the the high-level mathematics to the low-level kernel based accelerator implementation. To the best of our knowledge, ours is the ﬁrst work that exposes this potential optimization. This is a generic strategy to reduce memory footprint in a multi-dimensional space. Consider an elementwise layer with n inputs that applies a function f inputs such that y = f(x1,x2,...,x n) to each corresponding element of the input tensors and where the output is re- tained for the backward pass of the subsequent layer. • Discard activation x1 without loss of generality. Determine a function g such that x1 = g(y,x2,...,x n). For bijective functions of one variable this is simply the inverse. • If such a function does not exist without ambiguity, construct functions g1,...,g j that can recover x1 on an interval. Construct a function g∗such that x1 = g(m,y,x 2,...xn) where mis an indicator that denotes the interval from which x1 from and thus the piecewise selection of one of g1,...,g j. Polynomially approximate each of g1,...,g j to construct a new piecewise function g∗p in the case that they cannot be expressed analytically. • For the implementation of the forward pass, fold the computation of minto the computation of f. In essence, construct a new function f∗such that (y,m) =f∗(x1,...,x n). This can be done in a single kernel call. • For the backward pass, fold the calculation of x1 = g∗p(m,y,x 2,...,x n) into the computations of ∂f ∂x2 ,..., ∂f ∂xn if the computation of these values requires x1 by composing these functions. In essence, fusing the kernels for the inverse and gradient operator. Then, we require nkernel calls to calculate the gradient with respect to the loss as before. We illustrate this strategy in appendix E in more detail. The crux of the idea is that m, if needed at all, can be stored with less memory than x1, while keeping the number of kernel calls to a minimum. 5.2 Auto-Tempo As part of exploratory future work, we consider the application of Tempo as an automatic compiler pass. We propose and prototype two different methods of automatically applying Tempo to trans- formers which are available at the link in section 6. The ﬁrst method is a fast method of proﬁling beforehand to determine whether memory footprint reduction would help, then applying Tempo to all applicable layers. The second method is a ﬁne-grained method applies Tempo to a subset of the applicable layers where the subset is determined through automatic proﬁle and search, analogous to binary search. 6 Conclusion We propose Tempo, a new mechanism that reduces the memory footprint of Transformer-based models at low cost. It shows an improvement in throughput of up to 16% over the state-of-the-art baseline for BERTLARGE pre-training task and also shows an improvement in maximum batch size of up to 2×on both V100 and 2080 Ti GPUs. Our technique also generalizes well to new models, more modern hardware, as well as diverse model parameters in terms of memory footprint and throughput, demonstrating the robustness of our technique. Our hope is that Tempo will be used with other footprint reduction methods to improve training efﬁciency of Transformer-based models. We open-source Tempo for an immediate positive impact on both machine learning researchers and practitioners here: https://github.com/UofT-EcoSystem/Tempo. 10References [1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorﬂow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation, (OSDI 2016). USENIX Association, 2016. https://www.usenix.org /conference/osdi16/technical-sessions/presentation/abadi. [2] Jorge Albericio, Patrick Judd, Tayler H. Hetherington, Tor M. Aamodt, Natalie D. Enright Jerger, and Andreas Moshovos. Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing. In 43rd ACM/IEEE Annual International Symposium on Computer Architecture, (ISCA 2016), 2016. https://doi.org/10.1109/ISCA.2016.11. [3] AWS. Amazon EC2 P3 Instance Product Details, 2019. https://aws.amazon.com/ec2/i nstance-types/p3. [4] Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. CoRR, 2016. http://arxiv.org/abs/1607.06450. [5] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-Document Trans- former. CoRR, abs/2004.05150, 2020. https://arxiv.org/abs/2004.05150. [6] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael W Mahoney, and Joseph E Gonzalez. ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training. In International Conference on Machine Learning (ICML), 2021. [7] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. MXNet: A Flexible and Efﬁcient Machine Learning Library for Heterogeneous Distributed Systems. CoRR, 2015. http://arxiv.org/abs/1512.01274. [8] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training Deep Nets with Sublinear Memory Cost. CoRR, 2016. http://arxiv.org/abs/1604.06174. [9] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, and Olivier Temam. DianNao: a small-footprint high-throughput accelerator for ubiquitous machine- learning. In Architectural Support for Programming Languages and Operating Systems, (ASP- LOS 2014). ACM, 2014. https://doi.org/10.1145/2541940.2541967. [10] Yu-Hsin Chen, Joel S. Emer, and Vivienne Sze. Eyeriss: A Spatial Architecture for Energy- Efﬁcient Dataﬂow for Convolutional Neural Networks. In43rd ACM/IEEE Annual International Symposium on Computer Architecture, (ISCA 2016), 2016. https://doi.org/10.1109/IS CA.2016.40. [11] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking Attention with Performers. In 9th International Conference on Learning Representations, (ICLR 2021), 2021. https://openreview.net/forum?id=Ua6zuk0WRH. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2019. \"https: //aclanthology.org/N19-1423\". [13] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. \"https://aclanthology.org/I05-5002\". [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In 9th International Conference on Learning Representations, (ICLR 2021), 2021. https://openreview.net/forum?id=YicbFdNTTy. [15] Google. Out-of-memory issues. https://github.com/google-research/bert#out-of- memory-issues. 11[16] Priya Goyal. [Re-checkpointing] Autograd container for trading compute for memory, 2018. https://github.com/pytorch/pytorch/blob/e1348973ac9a557aa6018e3fd2d548 9619dd81a7/torch/utils/checkpoint.py. [17] Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. CoRR, 2017. http://arxiv.org/abs/1706.02677. [18] Sylvain Gugger. PyTorch DeBERTa model, 2020. https://github.com/huggingface/t ransformers/blob/5b6bd4e7880cd51375c2d6c33bbd8173acfd920b/src/transfor mers/models/deberta/modeling_deberta.py. [19] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and William J. Dally. EIE: Efﬁcient Inference Engine on Compressed Deep Neural Network. In 43rd ACM/IEEE Annual International Symposium on Computer Architecture, (ISCA 2016), 2016. https://doi.org/10.1109/ISCA.2016.30. [20] Song Han, Huizi Mao, and William J. Dally. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding. In 4th International Conference on Learning Representations, (ICLR 2016), 2016. http://arxiv.org/abs/15 10.00149. [21] Dan Hendrycks and Kevin Gimpel. Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units. CoRR, abs/1606.08415, 2016. http://arxiv.org/abs/1606 .08415. [22] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the Knowledge in a Neural Network. CoRR, abs/1503.02531, 2015. http://arxiv.org/abs/1503.02531. [23] ISO. ISO/IEC 14882:2011 Information technology — Programming languages — C++ . Inter- national Organization for Standardization, 2012. http://www.iso.org/iso/iso_catalog ue/catalogue_tc/catalogue_detail.htm?csnumber=50372. [24] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoeﬂer. Data Movement Is All You Need: A Case Study on Optimizing Transformers. In Proceedings of Machine Learning and Systems (MLSys), 2021. https://proceedings.mlsys.org/paper/2021/f ile/c9e1074f5b3f9fc8ea15d152add07294-Paper.pdf. [25] Animesh Jain, Parker Hill, Shih-Chieh Lin, Muneeb Khan, Md. Enamul Haque, Michael A. Laurenzano, Scott A. Mahlke, Lingjia Tang, and Jason Mars. Concise loads and stores: The case for an asymmetric compute-memory architecture for approximation. In 49th Annual IEEE/ACM International Symposium on Microarchitecture, (MICRO 2016), pages 41:1–41:13. IEEE Computer Society, 2016. https://doi.org/10.1109/MICRO.2016.7783744. [26] Animesh Jain, Amar Phanishayee, Jason Mars, Lingjia Tang, and Gennady Pekhimenko. Gist: Efﬁcient data encoding for deep neural network training. In International Symposium on Computer Architecture (ISCA 2018), 2018. /https://www.microsoft.com/en-us/rese arch/publication/gist-efficient-data-encoding-deep-neural-network-trai ning/. [27] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. Checkmate, 2019. https://github.com/parasj/checkmate. [28] Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization. In Proceedings of Machine Learning and Systems (MLSys), 2020. /https: //proceedings.mlsys.org/paper/2020/file/084b6fbb10729ed4da8c3d3f5a3ae7 c9-Paper.pdf. [29] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, 2020. https://aclanthology.org/2020.findings-emnlp.372. [30] Norman P. Jouppi, Cliff Young, Nishant Patil, David A. Patterson, Gaurav Agrawal, Ramin- der Singh Bajwa, Sarah Bates, Suresh Bhatia, Nanette J. Boden, Al Borchers, Rick Boyle, Pierre luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert B. 12Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Daniel Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle A. Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, J. W. Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing unit. 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA), 2017. [31] Patrick Judd, Jorge Albericio, Tayler H. Hetherington, Tor M. Aamodt, and Andreas Moshovos. Stripes: Bit-serial deep neural network computing. In 49th Annual IEEE/ACM International Symposium on Microarchitecture, (MICRO 2016), 2016. https://doi.org/10.1109/MICR O.2016.7783722. [32] Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic Tensor Rematerialization (DTR) Prototype, 2020. https://github.com/uwsampl/dtr-prototype. [33] Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock. Dynamic Tensor Rematerialization. In 9th International Conference on Learning Representations, (ICLR 2021), 2021. [34] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR, abs/1907.11692, 2019. http://arxiv.org/abs/1907.11692. [35] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer Sentinel Mixture Models. In 5th International Conference on Learning Representations, (ICLR 2017) , 2017. https://openreview.net/forum?id=Byj72udxe. [36] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed Precision Training. In 6th International Conference on Learning Representations, (ICLR 2018), 2018. https://openreview.net/forum?id=r1gs9JgRZ. [37] John Nickolls, Ian Buck, Michael Garland, and Kevin Skadron. Scalable Parallel Programming with CUDA: Is CUDA the Parallel Programming Model That Application Developers Have Been Waiting For? Queue, 2008. https://doi.org/10.1145/1365490.1365500. [38] NVIDIA. NVIDIA Driver Downloads. https://www.nvidia.com/Download/index.aspx. [39] NVIDIA. Tesla V100 Data Center GPU, 2017. https://www.nvidia.com/en-us/data- center/v100/. [40] NVIDIA. GEFORCE RTX 2080 Ti, 2018. https://www.nvidia.com/en-us/geforce/g raphics-cards/rtx-2080-ti/ . [41] NVIDIA. BERT For PyTorch. https://github.com/NVIDIA/DeepLearningExamples/t ree/master/PyTorch/LanguageModeling/BERT/, 2019. [42] NVIDIA. NVLink, 2019. https://www.nvidia.com/en-us/data-center/nvlink/. [43] NVIDIA. NVIDIA A100 Tensor Core GPU, 2020. https://www.nvidia.com/en-us/dat a-center/a100/. [44] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019. [45] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan Venkate- san, Brucek Khailany, Joel S. Emer, Stephen W. Keckler, and William J. Dally. SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks. In Proceedings of the 44th Annual International Symposium on Computer Architecture, (ISCA 2017). ACM, 2017. https://doi.org/10.1145/3079856.3080254. 13[46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, (NeurIPS 2019), 2019. https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f 92f2bfa9f7012727740-Abstract.html. [47] PCI-SIG. Speciﬁcations, 2005. /https://pcisig.com/specifications. [48] Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong, Fan Yang, and Xuehai Qian. Capuchin: Tensor-Based GPU Memory Management for Deep Learning. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Lan- guages and Operating Systems (ASPLOS), ASPLOS ’20. Association for Computing Machinery, 2020. /https://doi.org/10.1145/3373376.3378505. [49] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. [50] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. ZeRO- inﬁnity: breaking the GPU memory wall for extreme scale deep learning. In SC ’21: The International Conference for High Performance Computing, Networking, Storage and Analysis, 2021. https://doi.org/10.1145/3458817.3476205. [51] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2016. \"https://aclanthology.org/D16-1264\". [52] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulﬁqar, and Stephen W. Keckler. vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efﬁcient Neural Network De- sign. In The 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 2016. [53] Samuel Rota Bulò, Lorenzo Porzi, and Peter Kontschieder. In-Place Activated BatchNorm for Memory-Optimized Training of DNNs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [54] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning Representations by Back-propagating Errors. Nature, 323(6088), 1986. http://www.nature.com/articles/ 323533a0. [55] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. http: //arxiv.org/abs/1910.01108. [56] Or Sharir, Barak Peleg, and Yoav Shoham. The Cost of Training NLP Models: A Concise Overview. CoRR, abs/2004.08900, 2020. https://arxiv.org/abs/2004.08900. [57] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In EMNLP, 2013. [58] Phillip Spencer. Solution to the Transcendental Equation 2x + 3x = 5, 1999. https: //www.math.toronto.edu/mathnet/questionCorner/transsol.html. [59] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting. Journal of Machine Learning Research, 15(56), 2014. http://jmlr.org/papers/v15/srivastava14a.html. [60] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and Policy Considerations for Deep Learning in NLP. In Proceedings of the 57th Conference of the Association for Computational Linguistics, (ACL 2019), 2019. https://doi.org/10.18653/v1/p19-1355. [61] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient Transformers: A Survey. CoRR, 2020. https://arxiv.org/abs/2009.06732. [62] Guido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, 2009. 14[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. /https://proceedings.neurips.cc/p aper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [64] Swagath Venkataramani, Ashish Ranjan, Subarno Banerjee, Dipankar Das, Sasikanth Avancha, Ashok Jagannathan, Ajaya Durg, Dheemanth Nagaraj, Bharat Kaul, Pradeep Dubey, and Anand Raghunathan. ScaleDeep: A Scalable Compute Architecture for Learning and Evaluating Deep Networks. In Proceedings of the 44th Annual International Symposium on Computer Architecture, (ISCA 2017). ACM, 2017. https://doi.org/10.1145/3079856.3080244. [65] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu, and Tim Kraska. Superneurons: Dynamic GPU Memory Management for Training Deep Neural Networks. In Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP) . Association for Computing Machinery, 2018. /https://doi.org/10.1145/3178487.3178491. [66] Kyle Wiggers. OpenAI’s massive GPT-3 model is impressive, but size isn’t everything, 2020. https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-si ze-isnt-everything/. [67] Wikipedia. English Wikipedia, 2021. /https://en.wikipedia.org/. [68] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics, 2018. \"htt ps://aclanthology.org/N18-1101\". [69] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of- the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, (EMNLP). Association for Computational Linguistics, 2020. \"https://www.aclweb.org/anthology/2020.emnlp- demos.6\". [70] Geoffrey X. Yu, Tovi Grossman, and Gennady Pekhimenko. Skyline: Interactive In-Editor Computational Performance Proﬁling for Deep Neural Network Training. In Proceedings of the 33rd ACM Symposium on User Interface Software and Technology (UIST’20), 2020. [71] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big Bird: Transformers for Longer Sequences. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, (NeurIPS 2020), 2020. https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d8 49725f31a9a7a361ab9-Abstract.html. [72] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SW AG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. In EMNLP, 2018. [73] Bojian Zheng, Nandita Vijaykumar, and Gennady Pekhimenko. Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training. In47th ACM/IEEE Annual International Symposium on Computer Architecture, (ISCA 2020), 2020. https://doi.org/10.1109/IS CA45697.2020.00092. [74] Hongyu Zhu, Mohamed Akrout, Bojian Zheng, Andrew Pelegris, Anand Jayarajan, Amar Phanishayee, Bianca Schroeder, and Gennady Pekhimenko. Benchmarking and Analyzing Deep Neural Network Training. In 2018 IEEE International Symposium on Workload Characteriza- tion (IISWC), 2018. 15Summary of Appendices These appendices contain additional details to cover different aspects of our work: motivation, related work, implementation, methodology, and extra ablation studies. Appendix A contains a more detailed breakdown of the memory usage of BERTBASE . Appendix B goes over relevant related works and the difference between them and our work. Appendix C goes more in depth into the major memory reduction footprint techniques, and compare them to our method. Appendix D covers details of the LayerNorm backward pass derivation. Appendix E includes implementation details of our optimizations. Appendix F includes additional information on the implementation of our optimizations. Appendix G includes a more detailed description of our experimental setups. Appendix H includes a memory footprint ablation study with respect to Tempo optimizations. Appendix I is the NeurIPS paper checklist. A A Closer Look at the Memory Breakdown of BERT BASE Figure 9 shows a detailed memory breakdown of the Huggingface BERTBASE implementation [69] on the MRPC [13] ﬁne-tuning task at a batch size of 32, proﬁled using the skyline tool [70]. From the ﬁgure, encoder layer activations are clearly the major contributor to the memory footprint compared to parameter weights, gradients, and optimizer states. Figure 9: GPU memory breakdown for BERTBASE [12] ﬁne-tuning on the MRPC [13] task using the Huggingface [69] at a sequence length of 128 and batch size of 32. 16B Related Works There has been a number of previous works [55, 29, 5, 11, 71] which focus on developing models that remain competitive with BERT and other Transformer-based models while requiring less memory and compute. One approach that has been taken is to tackle the O(S2) nature of the attention mechanism [61]. Out of these, a few interesting and relevant ideas in terms of memory footprint reduction involve sparsifying the attention mechanism [ 5, 71], or using a decomposable softmax layer in order to avoid doing the O(S2) computation in the ﬁrst place [11]. In contrast, our technique targets a broad spectrum of sequence lengths. Other models such as TinyBERT [ 29] and DistilBERT [55] take a different approach in reducing the total model size. In these works, the technique of Distillation is used to train a smaller student network from a larger teacher network [ 22]. These approaches are model level algorithm changes, and therefore orthogonal to our work. Two other important techniques are mixed precision training [ 36] and in-place activated Batch- Norm [53]. Mixed precision training involves training using both 32-bit and 16-bit IEEE ﬂoating point numbers depending on the numerical sensitivity of different layers [36]. This both decreases the training time and the memory pressure of the model dependent on the hardware [36]. Reversible BatchNorm uses techniques designed to optimize the memory footprint of CNN models which make extensive use of RELU and BatchNorm operators [53]. They do this by deriving in-place expressions for these operators (the input feature map for these operators no longer needs to be stored). Although this work shows better performance than checkpointing, it is also speciﬁc to CNN models, so is not directly comparable to our work. Other recent works such as Substation [24] aims to improve training performance by reducing the total movement of data with operator fusion; this is also orthogonal to our memory footprint based approach. Other techniques are more focused on inference and model weights, both of which are not applicable in this context of Transformer-based modeltraining, where the activations are the memory bottleneck [20, 19, 10, 25, 2, 9, 31, 45, 64]. C General Memory Footprint Reduction Techniques We expand on the techniques we referred to in section 2 of checkpointing, ofﬂoading, and compression in this section. As previously iterated in section 2, (1) these techniques do not look closely at the speciﬁc structure of the BERT model, (2) At a per layer level, our techniques can provide better performance at a cheaper computational cost, and (3) these techniques are orthogonal to our work. C.1 Checkpointing Early work in this direction was able to reduce the memory cost of linear models to O(√n) for general linear models in the number of layers. Further work has expanded on this. Echo has innovative optimizations for speciﬁc models [73], Checkmate computes an optimal checkpointing schedule minimizing recomputation time with respect to a speciﬁc memory budget [28], and Dynamic Tensor Rematerialization uses online heuristics to minimize recomputation time while training [33]. Furthermore, there are works in this area that combine checkpointing with other techniques such as ofﬂoading detailed below. C.2 Ofﬂoad Initial work in this direction such as vDNN focused on speciﬁc schemes to ofﬂoad layer outputs depending on their computation cost [ 52]. There was also some consideration into pre-fetching feature maps in anticipation of their use in the backward pass. More work in that direction focuses on several different directions. For one, Superneurons [65] combines both checkpointing and ofﬂoading. In this work, they checkpoint computationally cheap layers, avoiding transfer overhead, while simultaneously ofﬂoading computationally expensive layers, avoiding recomputation overhead for those cases. Capuchin takes this further in considering tensor level accesses, as well as considering the runtime fetching and computation time in making decisions on which strategy to apply [ 48]. Additionally, ZeRO-Inﬁnity combines ofﬂoading of feature maps with ofﬂoading of model states and other optimizations [50]. 17C.3 Compression Works like Gist [26] and ActNN [ 6] both include forms of lossy compression. Gist speciﬁcally targets CNN based models with a variety of different lossy and lossless optimizations, taking an approach similar to our own in examining the model structure closely [26]. However, as this is a work focused on CNNs, the techniques described do not directly apply to the BERT model we are aiming to optimize for. ActNN on the other hand is a more general technique, using a quantization strategy designed to reduce the number of bits needed for feature map storage, while preserving certain theoretical guarantees regarding model conversion [ 6]. It additionally shows good performance relative to techniques such as Dynamic Tensor Rematerialization [33] and Capuchin [48]. This work does not consider the BERT model however [6]. D Backward pass of In-place LayerNorm In the following, we show how to compute the gradients of LayerNorm layer using the output. D.1 Notations We use x, ˆx, y, µand σ2 to represent the input, intermediate normalized input, output, and mean and variance of the input, respectively. The parameters of LayerNorm function, scaling factor and bias, are denoted by γand β, respectively. Lrepresents the loss. For simplicity but without the loss of generality, we assume the size of input is (N,M ), where the second dimension represents all the dimensions that are needed to be normalized. The meanings, deﬁnitions and sizes of variables are listed in Table 3. Meaning Deﬁnition Size input x= {xij,i = 1,...,N, j = 1,...,M } (N,M) norm-input ˆx= {ˆxij,i = 1,...,N, j = 1,...,M } (N,M) output y= {yij,i = 1,...,N, j = 1,...,M } (N,M) mean µ= {µi,i = 1,...,N } (N) variance σ2 = {σ2 i ,i = 1,...,N } (N) weight γ = {γj,j = 1,...,M } (M) bias β = {βj,j = 1,...,M } (M) Table 3: The meanings, deﬁnitions and sizes of variables used in LayerNorm layer. D.2 Forward Pass In the forward pass, input is ﬁrstly normalized along the second dimension, and then scaled and shifted accordingly. ˆxij = xij −µi√ σ2 i + ϵ yij = γj ˆxij + βj where µi = 1 M ∑M j=1 xij and σ2 i = 1 M ∑M j=1(xij −µi)2. ϵis added for numerical stability. D.3 Backward Pass Our goal is to use output to compute the gradients with minimum overhead. Intuitively however, the input is needed to compute the gradients of LayerNorm, which means we need to compute backwards to get input. We ﬁnd that we can use the intermediate normalized input to get what we want. The gradient derivations are listed as follows, along the lines of the BatchNorm derivation in [53]. ∂yij ∂γj = ˆxij, ∂yij ∂βj = 1, ∂yij ∂ˆxij = γj, 18∂L ∂γj = N∑ i=1 ∂L ∂yij ∂yij ∂γj = N∑ i=1 ∂L ∂yij ˆxij, ∂L ∂βj = N∑ i=1 ∂L ∂yij ∂yij ∂βj = N∑ i=1 ∂L ∂yij , ∂L ∂ˆxij = ∂L ∂yij ∂yij ∂ˆxij = ∂L ∂yij γj, Here we can the gradients with regard to γ, β and ˆx. We still need to derive the gradient to input further. ∂ˆxij ∂σ2 i = − ˆxij 2(σ2 i + ϵ), ∂ˆxij ∂µ2 i = − 1√ σ2 i + ϵ , ∂L ∂σ2 i = N∑ p=1 M∑ q=1 ∂L ∂ˆxpq ∂ˆxpq ∂σ2 i = M∑ q=1 ∂L ∂ˆxiq ∂ˆxiq ∂σ2 i (p= i) = M∑ j=1 ∂L ∂ˆxij ∂ˆxij ∂σ2 i (letq = j) = M∑ j=1 ∂L ∂yij γj · ( − ˆxij 2(σ2 i + ϵ) ) , ∂L ∂µi = N∑ p=1 M∑ q=1 ∂L ∂ˆxpq ∂ˆxpq ∂µi = M∑ q=1 ∂L ∂ˆxiq ∂ˆxiq ∂µi (p= i) = M∑ j=1 ∂L ∂ˆxij ∂ˆxij ∂µi (letq = j) = M∑ j=1 ∂L ∂yij γj · ( − 1√ σ2 i + ϵ ) , ∂σ2 i ∂xij = 2(xij −µi) M , ∂µi ∂xij = 1 M, ∂ˆxij ∂xij = 1√ σ2 i + ϵ , Combining all the intermediate results above, we have ∂L ∂xij = N∑ p=1 M∑ q=1 ( ∂L ∂ˆxpq ∂ˆxpq ∂xij ) + N∑ p=1 ( ∂L ∂σ2p ∂σ2 p ∂xij + ∂L ∂µp ∂µp ∂xij ) = ∂L ∂ˆxij ∂ˆxij ∂xij + ∂L ∂σ2 i ∂σ2 i ∂xij + ∂L ∂µi ∂µi ∂xij = [ ∂L ∂yij γj − ( m∑ j=1 ∂L ∂yij γj ·ˆxij ) ·ˆxij m − ( m∑ j=1 ∂L ∂yij γj ) · 1 m ] · 1√ σ2 i + ϵ where intermediate normalized input ˆxcan be computed as ˆxij = (yij −βj)/γj. Therefore, by extra stashing weight (scaling factor) γ, bias βand variance of the input γ2, we can get the gradients using output without recovering input. 19E Implementation Details E.1 In-Place GELU As we show in Section 3, it is possible to compute the inverse of the GELU function by knowing what side of the minimum the input originated from. This is shown in Equation 1 where GELU* is a function that returns both the GELU output and mis the mask bit that denotes which side of the minimum the input originates from. GELU*−1(GELU(x),m) =x (1) Moreover, we observe that we do not need to compute the inverse and then compute the derivative with respect to the input in a two step process as a naïve approach would suggest. Instead, we can precompute this composition (see Equation (2)): dGELU dx (y) = GELU′◦GELU*−1(y,m) (2) in order to compute the derivative with respect to the inputdirectly using the output value. A plot of this relation is shown in Figure 10a. (a) GELU derivative from (2). The section correspond- ing to x >−0.75179 is in blue, x ≤−0.75179 – in orange. (b) Our approximation of function (2) with a piece-wise polynomial approximation. Different sections of the approximation are shown in different colours. A key observation about the GELU function is that it is transcendental, and hence there is no simple solution for the inverse of the GELU function in terms of elementary functions [58]. We therefore approximate sections of Equation (2) with a piece-wise polynomial with a degree up to 13.5 A plot of this approximation is shown in Figure 10b. E.2 In-Place LayerNorm As stated in Section 3, we aim to reuse the output of the LayerNorm [4], which must be stored for the backpropagation of the successive fully-connected layers, while discarding the input. Although prior work has demonstrated the usability of In-Place Activated BatchNorm in the context of CNN networks [53], we note that this approach is not applicable in the Transformer case, which employs LayerNorm instead [63]. By employing an alternative derivation for the gradient of LayerNorm which stashes alternative parameters, we can compute the gradients with negligible performance overhead while achieving ideal memory footprint reduction for this operator. Following a similar approach as for In-Place GELU, we implement this operator as a Python PyTorch module, allowing it to be easily substituted in place of the existing LayerNorm layer in an implementation of the BERT model [41, 69]. See F for additional implementation details and the full derivation. 5Additional details of this implementation are covered in Appendix F. 20E.3 Sub-Layer Dropout Recomputation Our basic implementation of Sub-Layer Dropout Recomputation follows the example of our other optimizations. Note that the way Dropout is implemented requires a randomly generated mask, where a portion of the inputs are set to zero according to a percentage p, which is also needed in the backward pass [46, 7]. We simply stash this mask, and discard the output. Then, in the backward pass, we recompute the output as shown in Figure 4. Storing a boolean mask of size N will take N ×1 bytes, boolean tensors in PyTorch use 1 byte per value, while 32-bit ﬂoating points will use N×4 bytes [46]. Therefore, the total memory saved by discarding the output will be 4/5 of the total pre-optimization dropout output memory cost. In contrast to prior works which modify the framework and may not expose this optimization [48, 32, 27], we develop a PyTorch module which can be added in to reduce the memory pressure of the critical attention section of the Transformer-based models [63] with minimal overhead. Appendix F provides more in-depth comparison between prior work and our implementation. E.4 Other Engineering Optimizations We note that PyTorch uses a memory-inefﬁcient implementation of the softmax function which retains both the input and output of the function for the backward pass [46]. Instead, only the output is necessary. This optimization has also previously been implemented as part of some models in the Huggingface library [18]. We use this optimization as well in our implementation of the attention mechanism. E.5 Elementwise Extension We illustrate the difference between our general elementwise strategy in Figure 11. Figure 11: Comparison of our in-place general elementwise strategy with the baseline. Dotted border rectangles enclose functions that can be executed in a single kernel. Red bordered activations and gradients are needed for the computations of the elementwise and successive layer’s backward pass. F Additional Implementation Details This section contains additional implementation details of our technique. F.1 In-place GELU We implement this optimization in PyTorch. In this case, we write the forward pass to return the GELU of the input function, as well as a boolean mask indicating whether the input is greater than or equal to the point at which the minimum value occurs, x≈−0.75179. The backward pass is slightly more complicated. We take as inputs to the backward pass the incoming gradient, the saved mask, as 21well as the saved output values. The corresponding approximating polynomial shown in Figure 10b is determined and then computed. We use approximating polynomials of up to degree 13 in this case. We implement the forward and backward pass using CUDA [ 37] which is wrapped in C++ [ 23]. Both of these are wrapped in a Python [ 62] PyTorch layer to be substituted easily for an existing implementation. While proﬁling our implementation we found that the memory latency of loading these inputs, as well as storing the output to be the bottleneck. We were thereby able to implement polynomials of degree 13, since the computation is hidden by the memory access latency, although better approximations may be possible. F.2 In-place LayerNorm In-place Layernorm is implemented as a custom PyTorch module [46]. This is done in 3 stages as per PyTorch’s custom module implementation. To do this, we write a custom CUDA [37] implementation based on PyTorch’s own implementation of the LayerNorm layer [ 46]. We write a forward and backward layer wrapper in C++ [23], which is then again wrapped as a Python [62] PyTorch module, allowing it to be easily substituted in place of the existing LayerNorm layer in an implementation of the BERT model such as the NVIDIA DeepLearningExamples BERT implementation or the Huggingface implementation [41, 69]. F.3 Sub-Layer Dropout Recomputation We note that although the state-of-the-art recomputation/checkpointing papers [28, 33] consider such an abstract idea in theory, their practical implementations [27, 32] never treat the sub-layer granularity provided in the frameworks as the lowest granularity of those techniques’ applicability. We concede that in the case of TensorFlow, the dropout layer has additional underlying granularity as a result of its implementation [1], and hence this would not be applicable in that case – and Checkmate [ 28] would consider this level of granularity. However, this is not the case in PyTorch’s checkpointing implementation [16]. The tensor-level granularity of optimization is noted in Capuchin [48] as well. However, Capuchin is a technique that requires runtime level proﬁling, and modiﬁcations to the framework. It may or may not ofﬂoad, recompute, or otherwise store any part of the dropout layer speciﬁed. Our method uses in-built PyTorch operators at a C++ level to rewrite the attention mechanism, which is again wrapped as a Python PyTorch module to be substituted. G Experimental Setup Table 4 shows a more detailed view of our experimental platform. GPU # GPUsGPU Mem. (GB)CUDA VersionGPU DriverPytorch Version# (v)CPUsSys. Mem. (GiB) 2080 Ti 4 11 11.2 460.27.04 1.9.0 64 126 V100 4 16 11.0 450.142.00 1.9.0 32 244 A100 1 40 11.2 460.32.03 1.9.0 64 250 Table 4: A short summary of our test setups, including CUDA [37], PyTorch [46], and driver [38] versions. H Memory Footprint Reduction Ablation Study We calculate the memory footprint reduction contributed by each optimization across different sequence lengths relative to the total memory footprint of each encoder layer, given aH to Aratio of 64 [ 63, 12]. This is shown in Figure 12 for the selected conﬁgurations. From the ﬁgure, it’s clear that In-Place GELU and LayerNorm provide the bulk of the memory footprint reduction in the 22short sequence length regime, while the other two optimizations provide an improvement in the long sequence length case. Note that this is due to the fact that the latter’s memory footprint reduction is O(S2), while the former’s memory footprint reduction goes asO(SH) where Sis the sequence length. This allows Tempo to stay robust to model parameters, and provide consistent performance across sequence lengths. 0 10 20 30 40 50 60 128 512 2048 % of Encoder Layer Activation Memory Sequence Length In-Place GELU In-Place LayerNorm Dropout Recomp. Softmax Eng. Opt. Figure 12: Per layer comparison of Tempo memory footprint reduction across different sequence lengths. I Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [N/A] This is a system level work which does not have any direct negative impact other than that of the underlying model. We discuss the positive impact in Section 1. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] See Appendix/ 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [Yes] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] For accuracy experiments, otherwise all others are system level experiments which should not change based on random seed and are expensive to run. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [No] (c) Did you include any new assets either in the supplemental material or as a URL? [No] 23(d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 24",
      "meta_data": {
        "arxiv_id": "2210.10246v2",
        "authors": [
          "Muralidhar Andoorveedu",
          "Zhanda Zhu",
          "Bojian Zheng",
          "Gennady Pekhimenko"
        ],
        "published_date": "2022-10-19T01:59:37Z",
        "pdf_url": "https://arxiv.org/pdf/2210.10246v2.pdf",
        "github_url": "https://github.com/UofT-EcoSystem/Tempo"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes Tempo, a novel approach to efficiently manage accelerator memory for training Transformer-based models, addressing the limitation of batch size due to activation memory footprint. Tempo introduces drop-in replacements for GELU, LayerNorm, and Attention layers, significantly reducing memory usage and boosting training throughput. It demonstrates up to 2x higher batch sizes and 16% higher training throughput for BERTLARGE pre-training and 19% and 26% speedups for GPT2 and RoBERTa respectively over state-of-the-art baselines. This work is the first to specifically optimize memory footprint for Transformer-based layers that translates to an actual increase in throughput.",
        "methodology": "Tempo integrates three core techniques: (i) In-place GELU, which uses an alternative backward pass derivation to discard the input activation and retain only the output and an 8-bit mask indicating the input's position relative to GELU's minimum, allowing for input recovery via a piecewise polynomial approximation of the inverse derivative; (ii) In-place LayerNorm, which derives the gradient as a function of its output, stashing only the intermediate mean and variance instead of the input feature map; and (iii) Sub-Layer Dropout Recomputation, where only the Boolean mask is stored during the forward pass of the Dropout layer, and the output is recomputed in the backward pass. An additional engineering optimization involves using a memory-efficient softmax implementation that only retains the output for the backward pass.",
        "experimental_setup": "The main test setup comprised 4 NVIDIA RTX 2080 Ti GPUs (11 GB each) over PCIe v3. An AWS p3.8xlarge instance with 4 NVIDIA Tesla V100 GPUs (16 GB each) using NVLink was also utilized, and ablation studies were conducted on an NVIDIA A100 GPU (40 GB). The work was evaluated on BERT pre-training (BERTLARGE configuration) using the NVIDIA DeepLearningExamples library and English Wikipedia dataset, with sequence lengths of 128 and 512. Fine-tuning experiments used BERTBASE on the MRPC paraphrasing task with the Huggingface library. Ablation studies included RoBERTa (Fairseq library) and GPT2 (Huggingface GPT2 model) with the WikiText Dataset. Metrics included total memory footprint (maximum batch size and memory usage at fixed batch size), throughput (sequences/s), loss curves, and accuracy (10 trials, 10 epochs for fine-tuning). Baselines were the NVIDIA BERTLARGE model without optimizations and the same model with default PyTorch checkpointing.",
        "limitations": "In-place GELU, while memory-efficient, requires slightly more memory than full recomputation from X, though this is balanced by lower overhead. Checkpointing, despite offering significant memory reduction, introduces high computational overhead (up to 30% performance degradation). The GELU inverse derivative relies on a piece-wise polynomial approximation due to the transcendental nature of GELU. Furthermore, PyTorch's boolean masks use 8-bits per value, which, while offering memory savings, could potentially be 1-bit with added manual overhead. The sub-layer granularity of Dropout recomputation might not be universally applicable across all deep learning frameworks, specifically noted for TensorFlow's implementation.",
        "future_research_directions": "Future research includes extending the In-place GELU concept to general elementwise layers by developing generic strategies for input recovery, function approximation, and kernel fusion. Another direction is Auto-Tempo, exploring the application of Tempo as an automatic compiler pass through either a fast profiling method to apply Tempo to all applicable layers or a fine-grained, search-based method to apply it to a subset of layers. The authors also hope Tempo will be integrated with other existing memory footprint reduction methods to further enhance training efficiency of Transformer-based models."
      }
    },
    {
      "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
      "abstract": "Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate'' to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.",
      "full_text": "Accelerating Transformer Pre-training with 2:4 Sparsity Yuezhou Hu1 Kang Zhao Weiyu Huang 1 Jianfei Chen 1 Jun Zhu 1 Abstract Training large transformers is slow, but recent innovations on GPU architecture give us an ad- vantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a “flip rate” to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through es- timator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model’s quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two tech- niques to practically accelerate training: to calcu- late transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar con- vergence to dense training algorithms on several transformer pre-training tasks, while actual ac- celeration can be observed on different shapes of transformer block apparently. Our toolkit is avail- able at https://github.com/huyz2023/ 2by4-pretrain. 1. Introduction Pre-training large-scale transformers is hard, for its intensive computation and time-consuming process (Anthony et al., 2020). To accelerate training, sparsity-based methods have recently emerged as a promising solution, and one of the hardware-friendly sparse patterns is 2:4 sparsity. In a 2:4 sparse matrix, every four consecutive elements contain two 1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University. Correspondence to: Jianfei Chen <jianfeic@tsinghua.edu.cn>. Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). zeros. Within a tensor core, a 2:4 sparse matrix multiplica- tion (2:4-spMM) could be 2x faster than its dense equivalent on NVIDIA Ampere architecture GPUs. Some works use 2:4 sparsity for accelerating training (Hubara et al., 2021; Lu et al., 2023; McDanel et al., 2022; Chmiel et al., 2023). However, they mainly target on con- volutional neural networks (CNNs) (Hubara et al., 2021; McDanel et al., 2022), whose architecture, optimizer and training procedure are different from transformers. Whether these 2:4 sparse training methods are capable for transform- ers remains under-explored. In practice, we find two bar- riers: 1) Low accuracy. The hyperparameters in some accuracy preserving techniques for transformers vary sig- nificantly from that for CNNs, which is ineffective if trans- planted directly. Remarkably, simply halving the inner di- mensionality of a feed-forward network can also reduce the same amount of computational cost, but provides bet- ter performance than most of proposed 2:4 sparse training methods. 2) Inefficiency. All previous works on 2:4 training stay on simulation, and do not provide actual acceleration results. Besides, they don’t focus on other key operations be- yond matrix multiplication that affect the practical time cost, such as overheads of pruning and activation functions. They usually lead to substantial mismatches between simulation and actual acceleration performance. In this work, we aim to propose an end-to-end acceleration method for pre-training transformers based on 2:4 sparsity. Here are our major contributions: • We propose three accuracy-preserving techniques (two for masked decay and one for dense fine-tune) for 2:4 training. First, we propose to apply the masked decay on gradients rather than on weight. Second, we show that the feasible masked decay factor on transformers may be very small (100x smaller than it has been reported on CNNs) and devise a method to quickly determine an available decay factor. Besides, our analysis demonstrates that employing a dense fine-tuning stage at the end of pre- training, rather than at the beginning, can enhance the quality of transformers. • We analyze practical factors affecting the 2:4 training speed of transformers, which is rarely considered by pre- vious works. We identify two speed bottlenecks: prun- ing overhead and gated activation functions’ overhead. 1 arXiv:2404.01847v3  [cs.LG]  27 Oct 2024Accelerating Transformer Pre-training with 2:4 Sparsity We proposed kernel-level accelerated methods to address each of these bottlenecks. • To the best of our knowledge, this is the first report on end-to-end acceleration on pre-training transformers (Fig- ure 7, Table 11). Experiments show that transformers pre-trained using our proposed sparse training scheme are comparable or even superior in accuracy to those trained with dense training methods (Table 5, 6). 2. Related Work Existing sparsity-based methods can be classified into two categories: accelerating inference and accelerating training. For training acceleration, they can be further grouped by whether 2:4 sparsity is involved. Sparsity for Inference Acceleration Early methods in- clude one-shot pruning (Han et al., 2015; 2016; Lee et al., 2018; Mishra et al., 2021). Later methods (Evci et al., 2021; Zhou et al., 2021; Lasby et al., 2023) suggest using dynamic sparse training (DST). Particularly, Zhou et al. (2021) pro- poses sparse-refined straight-through estimator (SR-STE) for 2:4 inference. Iterative magnitude-based pruning (IMP) methods (Chen et al., 2020; 2021; You et al., 2022), orig- inated from the winning lottery ticket theory (Frankle & Carbin, 2019; Frankle et al., 2020), can also be viewed as a DST approach. All these methods only speedup the forward pass. They are insufficient to accelerate training. 2:4 Semi-Structured Sparsity for Training Acceleration Accelerating training by 2:4 sparsity is hard, because both the forward and backward passes need to be accelerated. On some GPUs involving sparse tensor cores, 2:4-spMMs perform 2x faster than dense GEMMs (Mishra et al., 2021; BUSATO & POOL). In light of this, (Hubara et al., 2021) firstly proposes a transposable N:M mask to accelerate both output activations and input gradients computation in back- ward pass. Zhang et al. (2023) improve transposable mask to bi-directional mask (Bi-Mask) to further boost mask di- versity. To accelerate calculating weight gradient via 2:4- spMM, an unbiased minimum-variance estimator (MVUE) is introduced (Chmiel et al., 2023). In addition, Xu et al. (2022) also achieve fully sparse training of CNNs using spatial similarity. However, all these works do not report end-to-end training speedups on 2:4 sparse tensor cores, and they are built for CNNs. Practical 2:4 training acceleration on transformers has not been reported so far. Other Structured Sparsity for Training Acceleration Structured sparsity means channel-wise pruning to dense networks. For instance, training a large model and then compressing it to be thinner or shallower seems effective (Li et al., 2020; Zhou et al., 2020), given a fixed accuracy requirement. However, it’s not memory-efficient due to the larger model’s redundancy. In addition, low-rank adaption proves to be an effective method to reduce fine-tuning costs (Hu et al., 2023), but it can’t accelerate the pre-training. 3. Preliminary In this section, we first present the mathematical formula- tions of dense training and fully sparse training. Afterward, we revisit the related methods which are helpful to achieve fully sparse training with 2:4 sparsity, including SR-STE (Zhou et al., 2021), transposable N: M mask (Hubara et al., 2021), and MVUE (Chmiel et al., 2023). 3.1. Dense Training Problem Formulation Dense training solves an opti- mization problem minw L(w), where L is a loss function, w ∈ RD is the collection of dense weights of all layers, flat- tened to a vector. The loss is optimized by gradient descent optimization algorithms such as SGD, Adam (Kingma & Ba, 2017) and AdamW (Loshchilov & Hutter, 2019). GEMMs of a Linear Layer in Dense Training In each training step, a single linear layer performs three general matrix multiplications (GEMMs): Z = XW⊤, ∇X = ∇ZW, ∇W = ∇⊤ ZX, (1) where X, W and Z are input activations, weights, and out- put activations, with shape X, ∇X ∈ Rp×q, W, ∇W ∈ Rr×q, and Z, ∇Z ∈ Rp×r. Here, the three GEMMs com- putes output activations, input activation gradients, and weight gradients, respectively. Without loss of generality, we assume the input X to be a 2D matrix rather than a 3D tensor. In the feed-forward networks of a transformer, this can be done by simply flattening the input tensors’ first two axes, i.e., axes of batch size and sequence length. 3.2. Fully Sparse Training with 2:4 Sparsity GEMMs can be accelerated with structured sparsity. Partic- ularly, 2:4 sparsity (Mishra et al., 2021) is a semi-structured sparsity pattern supported on NVIDIA Ampere architec- tures. A 2:4 sparse matrix partitions its elements into groups of four numbers, where each group has exactly two zeros. Depending on the direction of partition, there are row-wise 2:4 sparse matrix and column-wise 2:4 sparse matrix; see Appendix A.1. With such sparsity, a GEMM C = AB can be accelerated by 2x with the 2:4-spMM kernel if either A is row-wise 2:4 sparse, or B is column-wise 2:4 sparse. To accelerate training, each GEMM in Equation (1) should have one 2:4 sparse operand. In general, weights and out- put activation gradients are selected to be pruned due to relatively lower pruning-induced loss (Chmiel et al., 2023). 2Accelerating Transformer Pre-training with 2:4 Sparsity That is, Z = XSwt(W⊤), (2) ∇X = ∇ZSw(W), (3) ∇W = Sz(∇⊤ Z)X. (4) In Equations (2) to (4), Swt, Sw, and Sz represent the prun- ing functions of W⊤, W, and ∇⊤ Z. They take dense matri- ces as input, and outputs 2:4 sparse matrices. By intuition, a pruning function picks out the 2 elements with the max magnitudes in the adjoining 4 elements and zero out the rest. With hardware support, computing Equations (2) to (4) can be theoretically 2x faster than Equation (1). This method use 2:4-spMMs for all matrix multiplications in forward and backward propagation, so we call it fully sparse training (FST). Note that Equation (4) contains a straight-through estimator (STE), which we will explain later. Transposable Masks Hubara et al. (2021) suggest that a weight matrix and its transpose can be simply pruned by multiplying binary masks, i.e., Swt(W⊤) =W⊤ ⊙ Mwt, S w(W) =W ⊙ Mw, where Mwt, Mw ∈ {0, 1}p×q are 2:4 sparse, and ⊙ is element-wise product. To utilize 2:4-spMM, the two binary masks should be mutually transposable: Mwt = M⊤ w, (5) which they call as transposable masks (same as our defina- tion in Section 5.1). In this manner, the backward pass share the same sparse weight matrix with the forward pass. The authors also propose a 2-approximation method for generat- ing such masks with claimed low computational complexity. Minimum-Variance Unbiased Estimator Chmiel et al. (2023) propose to calculate the 2:4 sparse masks of neural gradients by MVUE, i.e., Sz(∇⊤ Z) = MVUE(∇⊤ Z). (6) Compared to the commonly used minimum square error esti- mation, MVUE guarantees unbiasedness and minimizes the variance of the sparsified gradients, which is more favorable for promoting the convergence of training. 3.3. Optimization Strategies for Sparse Training The optimization of a sparse network is difficult as it has non- differentiable pruning functions. The optimization objective can be formulated as minw L(˜ w). The network makes prediction with a sparse weight vector ˜ w= m(w) ⊙ w, where the mask m(w) ∈ {0, 1}D is the concatenation of masks for each layer. If a layer is not sparsified, then the corresponding mask is an all-one matrix. Computing the gradient is tricky since the mask m is dynamically com- puted based on the dense weight w: by chain rule we have ∇wL(˜ w) =∂ ˜w ∂w ∇˜ wL(˜ w), where ∂ ˜w ∂w is a Jacobian matrix. However, ˜w is not differentiable with w since it includes a non-differentiable mask-computing-function m(·) in it. Thus, it takes some skills to estimate the gradients and up- date the parameters. STE As ˜w is an approximation of w, a straight-through estimator (STE, Bengio et al. (2013)) directly passes the gradient of ˜w to w: ∇wL(˜ w) ← ∇˜ wL(˜ w). (7) SR-STE There is a problem with STE: only a portion of the weights in a layer participate in the forward calculation, but all the weights receive gradients. This indicates that the gradients associated with masked weights1 might be inac- curate. To suppress those inaccurate gradients, Zhou et al. (2021) proposes sparse-refined straight-through estimator (SR-STE) which adds a decay term when updating: wt ← wt−1 − γ(∇wLt(˜ wt−1) +λW (m(wt−1)) ⊙ wt−1), (8) where γ stands for the learning rate, λW is the decay fac- tor, and m(wt−1) denotes the logical not operation of m(wt−1). This decay term alleviates the change of weight mask. With SR-STE, the optimization target becomes min w L(˜ w) +λW 2 ∥w ⊙ m(w)∥2 2. (9) 4. Accuracy Preserving Techniques While the methods reviewed in Section 3 can successfully perform FST on small-scale models such as ResNet and DenseNet, it is not clear whether they can be directly ap- plied to pre-train large transformers. It is challenging for FST to preserve the accuracy of dense training, since the weights and masks need to be learned jointly, which is a non- differentiable, combinatorial optimization problem. More- over, unlike inference acceleration methods, FST has no pre-trained dense model to start with. In this section, we pro- pose three practical techniques to improve the convergence of FST for transformers: transformer-specific masked decay, Fast decay factor determination and dense fine-tuning. 4.1. Flip Rate: Stability of Training Inspired by previous work (Zhou et al., 2021; You et al., 2022), we define a “flip rate” to measure how frequently the mask vector changes after one optimizer step. This metric could be used to monitor whether the network connection is stable during training. 3Accelerating Transformer Pre-training with 2:4 Sparsity Figure 1.Flip rates change throughout the training of differentλW on Transformer-base. Note that these models utilize an identical learning rate schedule. Table 1.Training results of different λW on Transformer-base. As λW increases from 0 to 2e-4, accuracy first rises and then drops, which means that λW should be neither too big nor too small to reach the optimal results. λW AVG EPOCH LOSS VAL LOSS TEST BLEU DENSE 4.558 3.978 26.15 0 (STE) 4.76 4.164 24.98 6E-7 4.684 4.079 25.68 6E-6 4.626 4.033 25.81 2E-6 4.64 4.041 25.94 2E-5 4.642 4.049 25.74 2E-4 4.662 4.06 25.62 Definition 4.1. Suppose wt is a D-dimensional weight vector at time t, and the flip rate rt is defined as the change in proportion of the mask vector after an optimizer step: rt = ∥m(wt) − m(wt−1)∥1/D ∈ [0, 1]. The larger rt is, the more unstable the network connections become. You et al. (2022) suggest that a sparse neural network acts differently in different training phases. In the early phase of training, it eagerly explores different connection modes, which means the masks vector change rapidly over time. Later, the masks gradually become stable, and the network turns itself to fine-tune weight values. In terms of flip rate, we hypothesize that A healthy training process comes with the flip rate rt rising at the beginning of training and then gradually fading to 0. We measure flip rate change for dense training, STE and SR-STE with different λW in Figure 1. For dense training, we compute the flip rate by pruning the dense weight in each iteration, despite the pruned weight is never used for training. In terms of flip rate, dense training is healthy: itsrt exactly increases first before declines. If a training process 1Unlike some relevant literature, we use “masked weights” and “pruned weights” to denote the weights that are set to 0. consistently has higher flip rate than dense training, which we call as “flip rate explosion”, it may suffer from a loss in final accuracy due to unstable training; see Table 1. In practice, STE suffers from a flip rate explosion, while SR- STE takes effect by “freezing” masks of weights: by adding a decay term, it decrease the number of flips. This inhibition effect is related to the decay factor of SR-STE: the larger λW is, the stronger the inhibition of flips is, and the smaller flip rate goes. In this section, all methods we propose involve our ultimate principle: the peak of the curve should be sufficiently high to fully explore different connection modes, and the tail should be sufficiently low for the optimization process to converge. 4.2. Transformer-Specific Masked Decay Based on our insights on flip rate, we propose a method to suppress the frequent change of masks during FST for transformers, which we call masked decay. Unlike Equation (8) which imposes regularization directly on weights, we propose to add masked decay on gradients, i.e., gt ← ∇wLt(˜ wt−1) +λW (m(wt−1) ⊙ wt−1). (10) On SGD, applying decay on weights and on gradients are equivalent, but on popular optimizers like Adam and AdamW they aren’t. Specifically, Adam updates weights by wt ← wt−1 − γ(β1ut−1 + (1− β1)gt) (1 − βt 1)(√ˆvt + ϵ) (11) where u and v are the first and second order momentum of w. Compared to Equation (8), the masked decay regu- larization term in Equation (10) would be later normalized by √ˆvt + ϵ in Equation (11), before it is subtracted from weights. In this way, each dimension receives a different intensity of decay (“masked decay”). More specifically, weights with larger gradients get smaller decay intensity, and vice versa. In FST, we periodically prune weights by their magnitudes. STE may cause the network to fall into such “dilemma points”, where a portion of pruned weights and unpruned weights have nearly the same L1 norm. Thus, the network consistently oscillate between two possible masks m1 and m2, and is unlikely to jump out the dilemma itself. To illustrate this, we split each weight matrix by small 4 × 4 blocks. We count each block’s cumulative flip number and measure the ”L1 norm gap” by gi = ∥m1 ⊙ wi∥1 − ∥m2 ⊙ wi∥1, where wi is the i-th 4 × 4 weights, m1 ⊙ wi and m2 ⊙ wi have the first and second largest L1-norm among different pruning binary masks. The selected mask is most likely to oscillate between m1 and m2, especially when gi is small. In STE, there exists more 4 × 4 blocks 4Accelerating Transformer Pre-training with 2:4 Sparsity Figure 2.Scatter plots of cumulative flip number and L1 norm gap gi on every 4 × 4 block. All results are selected on Transformer- base, with epoch=20. (a) shows the result of dense model. (b)-(d) shows that of masked decaying on gradients, no decaying, and masked decaying on weights. Also, we do it on purpose to choose an extremely large λW for SR-STE. Figure 3.Applying masked decay on weights takes no effect to inhibit flip rate on BERT-base (compared to applying directly on gradient). Table 2.Optimal λW for multiple models. MODEL OPTIMAL λW RESNET18 (Z HOU ET AL ., 2021) 2 E-4 BERT-BASE 6E-6 TRANSFORMER -BASE 1E-6 DEIT-TINY 2E-3 GPT-2 124M 6 E-5 350M 2 E-4 774M 2 E-4 1558M 6 E-5 with high flip num and low ”L1 norm gap”; see Figure 2. This results in overall flip rate explosion of STE. On these occasions, we argue that an evenly masked de- cay applied on weights is insufficient to save the training from such “traps”. The weights don’t differentiate them- selves after an update, so masks may oscillate back. By normalizing the weight gradients with √ˆvt + ϵ, our masked decay amplifies the regularization strength for the dimen- sion with smaller gradient, pushing it towards zero. Then, the regularized dimension can no longer compete with other dimensions. So we effectively break the tie and push the training process out of the trap, towards a “healthier” state. The comparison results between our masked decay defined in Equation (10) and the conventional counterpart in Equa- tion (8) are shown in Figure 3. Results show that applying masked decay on weights takes no effect to inhibit flip rate explosion of STE, while applying on gradients works fine. 4.3. Fast Decay Factor Determination The determination of the decay factor λW in Equation (10) is non-trivial: if λW is excessively large, then the “peak” of the flip rate curve is not high enough; ifλW is too small, the “tail” of the curve is not low enough. Both do not provide a healthy training process. Besides, we find that λW values for CNNs and other small-scale networks differ significantly from those for transformers, while on transformers, optimal λW can span up to three orders of magnitude (Table 2). As pre-training large transformers is costly, grid searching for λW with the final accuracy is impractical, so it is vital to determine a feasible λW as quickly as possible. To quickly determine λW , here we propose a test-based method: 1) Grid search on the warm-up stage of training. For each λW value in a candidate set, sample a corresponding flip rate of the sparse network from a small number of training steps. Note that sampling in early training stage is enough to obtain a representative flip rate specific to a sparse network. 2) Comparison with the dense counterparts. Suppose rt0 to be the standard flip rate on the dense network at time t0 and r ′ t0 to be the sparse network’s flip rate. Their ratio is µ = r ′ t0/rt0 . We suggest that a feasibleλW should have µ ∈ [0.60, 0.95] and the sparse network may suffer from an accuracy drop if µ ≥ 1. 4.4. Dense Fine-Tuning To better improve accuracy, we suggest using a “dense fine- tuning” procedure at the end of training. Formally, we select a switch point ts. FST is performed while t ≤ ts, and dense training is switched to if t > ts. Why Choose Dense Fine-Tuning Instead of Dense Pre- training? While previous work (Han et al., 2017) suggest to switch between sparse and dense training stages, some recent works like STEP (Lu et al., 2023) utilize dense pre- training rather than dense fine-tuning, which means a dense network is initially trained for a period of time before being switched to a sparse one. However, we argue that dense pre- training is meaningless in our FST process. As described in 5Accelerating Transformer Pre-training with 2:4 Sparsity Figure 4.Dense fine-tuning versus dense pre-training on BERT- base Section 4.1, the peak of the flip rate curve should be suffi- ciently high to explore connection modes, so what matters most to the flip rate is the magnitudes of weights, which are the key to determine if connections are built or demol- ished. In this regard, both FST and dense pre-training are capable of delivering proper gradient magnitudes, so dense pre-training is a waste. The precise gradients are generally more necessary in the later stages of training, where the flip rate of the dense network comes to its tail. Figure 4 visual- izes the loss curve of pre-training BERT-base, where dense pre-train obtains nearly the same result as the naive SR-STE method. From this, we propose the following insight: If dense pre-training of tα steps provides slight improve- ment of accuracy, then moving the tα dense steps to the end gives far more improvement than dense pre-training. As for the specific position of the switch point in training, STEP (Lu et al., 2023) suggests that the dense pre-training occupy 10% to 50% of the total steps. Likewise, we deter- mine that our dense fine-tuning takes up the last 1/6 of total steps for balance training efficiency and accuracy. 5. Training Acceleration Techniques For transformers, the forward pass of FST involves prun- ing weights in FFNs with transposable 2:4 masks and then performing normal forward propagation. During backward propagation in FST, the gradients of input activations and weight gradients in FFNs are derived by Equation (3) and (4), respectively. Note that we also utilize MVUE to prune gradients of output activations, i.e., Equation (6). Compared to dense training, our FST replaces all the GEMMs in FFNs with 2:4-spMMs that theoretically perform 2x faster than their dense counterparts on GPUs within sparse tensor cores. In addition to speeding up the most time-consuming GEMMs in FFNs, there are three major operations that also have non-negligible impacts on training speed: 1) Pruning. In FST, pruning includes two steps: finding a mask that satisfies the 2:4 sparse patterns and then enforc- ing the mask to the corresponding dense matrices. In our case, we find that the time cost of finding transposable masks is time-consuming. 2) Activation functions. In transformers, SwiGLU and GEGLU (Shazeer, 2020) are popular. These two acti- vation functions involve a gate mechanism to regulate activations. This mechanism easily induces the GPU L2 cache misses, thus decreasing the computing speed. 3) Updating optimizer states. The excessive update fre- quency can introduce additional time overheads. Below, we show our methods to accelerate these operations, the main workflow of which is shown in Appendix B. 5.1. Fast Computation of Transposable Masks Problem Formulation We aim to find such a mask matrix M ∈ {0, 1}r×q for every W ∈ Rr×q in the FFN layer that 1) each adjoining 4 × 4 block contains 8 non-zero positions; each row and column in the block occupies 2 non-zero elements exactly; 2) maxM ∥M ⊙ W∥1. Then M would be our targeting transposable mask. As described in Equation (5), both a transposable mask itself and its transposition conform to the format of 2:4 sparsity. Previous 2-approximation algorithm (Hubara et al., 2021) consists of two steps: sort elements, and pick elements out of the array. They claim that the procedure has less computational complexity. However, in practice, the sorting and picking process contains too many jumps in its control flow, and may be fatal to modern GPU architecture. To make full use of the GPUs’ parallel computation capability (SIMD and SIMT), we convert the transposable mask-search process into a convolution operation which traverse all the masks to obtain the optimal one in three steps: 1) Create a convolutional kernel in the shape of 4 × 4 × nt, where nt denotes the number of transposable masks. In the case of 2:4 sparsity, mask diversity nt = 90. These mask blocks for 2:4 sparsity can be selected by exhaus- tively inspecting all potential masks offline. 2) Calculate the index matrix via Algorithm 1. The index matrix denotes which 4 × 4 mask in the convolutional kernel is the optimal mask that retains most of the weight norms after being applied to weights. Algorithm 1 transposable mask search Input: mask pattern m′, weight matrix W 1. W = abs(W) 2. out = conv2d(W, m′, stride= 4, padding= 0) 3. index = argmax(out, dim= 2) return index 3) Replace all the elements in the index matrix by the cor- responding 4 × 4 block, which is the desired mask. 6Accelerating Transformer Pre-training with 2:4 Sparsity Figure 5.Transposable mask search Figure 6.left: adapted method; right: intuitive method Table 3.Throughput of two transposable search kernels on RTX3090 (TB/s). INPUT METHOD 2-APPROX OURS FP16 FP32 FP16 FP32 3072 × 768 18.5 36.4 69.2 104.7 4096 × 1024 22.5 38.4 91.9 131.5 5120 × 1280 22.6 44.4 91 128.2 1024 × 1600 22.8 44.8 95 134.5 8192 × 2048 23 45.1 99.4 142.9 16384 × 4096 23.2 45.4 100.1 144.8 30768 × 8192 23.2 45.5 100.9 145.1 Table 4.Throughput of two GEGLU implementations on RTX3090 with fp16 column-major input tensors (TB/s). INPUT METHOD INTUITIVE OURS 32 × 512 × 768 18.4 55.5 32 × 512 × 1024 19.9 55.7 32 × 512 × 1280 18.2 55.9 32 × 512 × 1600 18.4 55.9 32 × 512 × 2048 19.5 56 32 × 512 × 4096 11.8 56.1 32 × 512 × 8192 12.1 56.2 Notably, step (1) is executed offline. Step (2) and (3) are fre- quently performed during FST. The workflow of our method is shown in Figure 5. Compared to the 2-approximation al- gorithm, our method is up to about 5 times faster (Table 3). 5.2. Acceleration of Gated Activation Functions Activation functions with gated mechanisms are widely used in transformers such as GLM (Du et al., 2022) and LLaMA (Touvron et al., 2023). Typical gated activation functions involve SwiGLU and GEGLU. The bottleneck of such activation functions is that the gate operations easily incur GPU L2 cache miss. Take GEGLU as an example: GEGLU(X, U, V, b, c) = GELU(XU⊤ +b)⊙(XV⊤ + c), where X ∈ Rp×q, U, V ∈ Rr×q, b, c ∈ Rr. In prac- tice, this function is composed of three steps: 1) Concatenate U and V into a new weight matrix W ∈ R2r×q, and b, c into a new bias vector d ∈ R2r. 2) Directly calculate Z = XW⊤ + d ∈ Rp×2r as a com- pressed matrix. 3) Split the Z in the second dimension intoZ1, Z2 ∈ Rp×r. Calculate GELU(Z1) ⊙ Z2. Different from dense model, where output activations are row-major matrices, in FST, the output activations are column-major; see Appendix A.2. This property results in the third step being extremely time-consuming if conven- tionally Z is accessed along the row dimension. To illustrate, Figure 6 shows that in a column-major matrix Z, accessing along the column accords with array layout. Thus, adjacent elements loaded into the GPU cache can be probably hit. By contrast, accessing along the row does not fully utilize the efficiency of GPU cache. In light of this, we carefully imple- ment a GEGLU kernel where elements are accessed along the column dimension. In this way, GEGLU is performed 5 times faster than the naive counterpart; see Table 4. 5.3. Other Implementation Details Reducing Updating Frequency We find that a 2:4 mask doesn’t change a lot after one optimization step, and it is not necessary to update a mask frequently. For the sake of efficiency, we update the transposable masks of weights every l optimizer steps. We usually take l = 40in practice. Utilities For 2:4-spMMs, we use CUTLASS (Thakkar et al., 2023). Other GPU kernels are implemented in Triton, including transposable mask search kernel, pruning kernel, MVUE kernel, GEGLU kernel, and masked decay kernel. 6. Experiments In this section, we validate the proposed training speedup methods on several transformers, including BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), Transformer- 2Results reported in the original paper; see https: //github.com/facebookresearch/deit/blob/ main/README_deit.md. 3DeiT-base dense model using the original recipe. 7Accelerating Transformer Pre-training with 2:4 Sparsity Table 5.GLUE scores of different 2:4 training methods with BERT. METHODLOSS AVG SCORECOLA MNLI MNLIEXTRAMRPC QNLI QQP RTE SST-2 STS-B DENSE 2.066979.8±0.4 45.3±1.1 82.6±0.2 83.4±0.1 78.8±1.7/86.1±1 89 .3±0.2 90.3±0.1/87.1±0 55.8±0.9 91±0.5 83 .7±1/83.7±1HALF 2.128077.9±0.4 37.2±1.3 82.4±0.1 83±0.3 75 .1±1.4/84.2±0.7 88 .8±0.3 89.9±0.1/86.6±0.1 51.2±2.4 92.1±0.5 82.1±0.5/82.3±0.4STEP 2.1179 77.7±0.1 40.4±1.4 82.2±0.1 82.8±0.1 74.5±0.7/83.5±0.4 88 .3±0.4 90.2±0.1/87±0.1 50.8±2.1 92.3±0.3 79.7±1.2/80.7±0.6BI-MASK2.117677.7±0.3 38.3±0.7 82.3±0.1 83±0.1 74 .3±0.7/83±0.6 88 .3±0.3 90.2±0.1/86.9±0.1 53.1±1.4 90.9±0.3 80.9±0.7/81.7±0.4OURS 2.0968 79.6±0.6 44.4±1.9 82.6±0.2 83±0.1 80.9±0.7/87.4±0.4 88.4±0.3 90.3±0.1/87±0.1 54.3±1 91.2±0.4 82.9±2.1/83±1.7 Table 6.GLUE scores with different model sizes on GPT-2 models. PARAMSMETHODVAL LOSSAVGSCORECOLA MNLI MRPC QNLI QQP RTE SST-2 STS-B WNLI 124M DENSE 2.907 73.9±1.1 44.6±0.9 82±0.1 78.3±1.3/84.8±1 88 .4±0.2 90±0 86 .5±0/61.3±1.5 91 .9±0.2 77.3±3.2/77.9±2.9 24.3±7.1OURS 2.952 74.3±0.5 44.8±1.3 81.5±0.2 77.5±1.8/84.2±1.3 87.8±0.1 89.5±0.1 85.9±0.1/66±1 90.6±0.4 80±0.8/80.3±0.5 23.9±6.4 350M DENSE 2.618 76.3±0.1 54.3±0.4 85.1±0.1 80.7±1/86.6±0.7 90 .7±0.1 91±0.1 87.8±0.1/64.9±1.7 93.5±0.4 81.7±1.2/82.2±0.8 17.6±3.2OURS 2.688 77.1±0.2 51.8±1.8 84.3±0.1 80.6±1.3/86.5±0.8 90.4±0.2 90.7±0.1 87.5±0.1/66.7±1.3 93.3±0.4 83.4±1.1/83.5±1.1 26.4±4 774M DENSE 2.493 76.2±0.4 57.5±2 86.1±0.1 80.3±1.3/86.4±0.9 91.4±0.2 91.1±0.1 88±0.1/67.7±2.6 94 .6±0.4 77.3±3.3/78.4±2.9 15.1±2.3OURS 2.564 77.1±0.4 55.9±0.9 85.6±0.2 81.2±0.6/87±0.4 91.4±0.1 91±0.1 87.8±0.1/71.5±0.7 94.2±0.4 81.8±1.3/82.3±1.2 15.8±1.2 1558MDENSE 2.399 76.5±0.5 55.3±2 87 ±0.1 79 ±1/85.3±0.8 91 .8±0.3 91.3±0.1 88.3±0.1/73.3±2 95 .9±0.3 78.5±2.4/79.2±2.5 13±1.3OURS 2.489 77.1±0.5 56.4±3 86.6±0.1 80±0.4/86.1±0.3 91.9±0.1 91.4±0.1 88.4±0.1/75±1.8 95.2±0.4 80.6±1.1/81.1±1.3 12.7±1.1 Table 7.SQuAD scores on GPT-2 models. PARAMS METHOD EM F1 124M DENSE 67.6 78.8 OURS 67.5 78 .5 350M DENSE 73.2 83.6 OURS 71.9 82 .4 774M DENSE 74.3 84.9 OURS 74.3 84 .6 Table 8.Experimental results for DeiT. SIZE METHOD ACC@1 A CC@5 DEIT-TINY ORIGINAL 2 72.2 91.1 DENSE 3 72.9 91.6 OURS 70.4 90 .1 DEIT-SMALL ORIGINAL 79.9 90.5 DENSE 79.9 94.5 BI-MASK 77.6 - OURS 79.2 94.8 DEIT-BASE ORIGINAL 81.8 95.6 DENSE 81.0 95.0 OURS 81.3 95 .4 Table 9.Experimental results for Transformer-base. METHOD AVG EPOCH LOSS TEST BLEU VAL BLEU VAL LOSS DENSE 4.558 26.15 26.56 3.982 HALF 4.659 26.12 26.36 4.041 STEP 4.692 25.27 25.85 4.082 OURS 4.649 26 .48 26 .78 3 .977 base for machine translation (Vaswani et al., 2023), and DeiT (Touvron et al., 2021b). For BERT, we use Cramming (Geiping & Goldstein, 2022) to pre-train a 16-layer BERT model with the sequence length of 512 on the C4 dataset (Raffel et al., 2019). For GPT-2, we use nanoGPT (Karpathy, 2023) to pre-train GPT-2 124M, 355M, 774M, and 1.5B on OpenWebText (Gokaslan & Cohen, 2019). Both BERT and GPT-2 models are estimated on GLUE (Wang et al., 2018). For DeiT (Touvron et al., 2021a), we pre-train DeiT-tiny on ImageNet-1K dataset (Deng et al., 2009). Besides, we use fairseq (Ott et al., 2019) to train Transformer-base on the WMT 14 En-De dataset (Bojar et al., 2014) and measure the BLEU (Papineni et al., 2002) score of the trained model. Of note, we use n to denote the length of sequences, d to denote the input and output dimensions of each trans- former block, dff to denote the inner dimensions of the FFNs in each transformer block, h to denote the number of heads, and N to denote the micro-batch size on each device. The pre-training and evaluation scripts are pub- licly available at https://github.com/thu-ml/ 2by4-pretrain-acc-examples . 6.1. Accuracy Results To investigate the effect of different 2:4 sparse training meth- ods, we pre-train a sparse BERT-base model on the C4 dataset using two sparse training methods: STEP (Lu et al., 2023) and Bi-Mask (Zhang et al., 2023). Besides, we also pre-train a dense BERT-base and a ‘Half’ BERT-base for comparison. Of note, ‘Half’ denotes a smaller yet still dense BERT-base model. To create Half model, we simply reduce the dff of each FFN layer in the original BERT-base by half while maintaining the original value of d. Theoretically, this adjustment halves the floating operations (FLOPs) of the original FFN layer as well. Except for the FFN layers, the shapes of the rest layers remain unaltered. All the pre-trained models are measured on GLUE bench- mark (WNLI excluded). Surprisingly, Table 5 shows that despite having identical FLOPs, the 2:4-sparse BERT-base trained with STEP and Bi-Mask shows inferior average scores compared to the Half model. The Half model attains 8Accelerating Transformer Pre-training with 2:4 Sparsity Table 10.Experimental results of masked decay, MVUE, and dense fine-tuning (FT) with BERT-Base. For decay term, we use both techniques in Sections 4.2 and 4.3. MASKED DECAY MVUE D ENSE FT L OSS AVG SCORE % % % 2.1553 77.6 ± 0.2 ! % % 2.1096 79.2 ± 0.2 ! ! % 2.1172 78.4 ± 0.3 ! % ! 2.0896 79.4 ± 0.2 ! ! ! 2.0968 79 .6 ±0.6 Table 11.Actual pre-train speed up on the whole network. PARAMETERS BATCH SIZE SPEEDUP 124M 16 1.18 350M 8 1.2 774M 4 1.21 Figure 7.Result of acceleration ratio S of different batch sizes and embedding Sizes. (a) shows the acceleration of a FFN layer. (b)-(d) shows the acceleration of a transformer block when n = 2048, 1024, 512. an average score of 77.9 on GLUE tests, while STEP and Bi-Mask only reach 77.7 due to the weaknesses in MRPC, QNLI, and STSB. By comparison, BERT-base trained in our proposed training method achieves 79.6 on GLUE, which significantly outperforms other sparse training methods and is comparable with the dense baseline, i.e., 79.8. Besides, we pre-train GPT-2 models with proposed meth- ods. Table 6 and 7 shows that our method for model sizes of 124M, 350M, 775M and 1558M achieves lossless scores compared with dense baselines. Similarly, DeiT and Transformer-base trained with our method also reach com- parable results to dense training; see Table 8 and 9. For GPT-2 and BERT, the training loss curves are sketched in Appendix C. Ablation Study We aim to investigate the effect of masked decay, MVUE and dense fine-tuning introduced in Section 4.2, 3.2, and 4.4. The 16-layer BERT-base is used for ablation study. Results in Table 10 show that: 1) The dense fine-tuning procedure helps to improve accuracy on GLUE by 2 points at most ; 2) MVUE leads to insignifi- cant, controllable accuracy loss; 3) By combining all these techniques together, 2:4 sparse training for transformers achieves comparable accuracy results as dense training. 6.2. Speedup Results The training acceleration techniques proposed in Section 5 are evaluated using GPT-2 models and RTX3090 GPUs. FP16 mixed precision training is used on all models. The practical speedups of a single FFN layer, a single trans- former block, and the entire network, compared to their re- spective dense counterparts, are reported. All the measured datum contain both forward and backward propagation. Feed-forward Network Layers For a single FFN layer, we fix n = 2048and change d. Results in Figure 7 show that a FFN layer can be accelerated up to 1.7x faster than its corresponding dense layer. Transformer Block We measure the acceleration ratio of a transformer block when n = 512, 1024, 2048. Results in Figure 7 show that in most cases, a transformer block can be accelerated to 1.3x faster via 2:4 sparsity. To illustrate this, a detailed profile result is given in Appendix D. End-to-end Acceleration Finally, we test the practical speedups of training GPT-2 models. Results in Table 11 show that our training method conducts up to 1.2x faster than the dense training on a single RTX3090. 7. Conclusions In this study, we are the first to propose accelerating the pre-training of transformers by 2:4 sparsity. We analyze the limitations of previous 2:4 training methods, including the impropriety in choosing positions and determining values of the masked decay factor, speed bottleneck incurred by computing transposable masks and gated activation func- tions. We propose a series of techniques to tackle them. Our training method is validated on DeiT, BERT, Transformer- base and GPT-2 models. In particular, we have attained 1.2x end-to-end training acceleration for the GPT-2 774M model without losing its accuracy. 9Accelerating Transformer Pre-training with 2:4 Sparsity Acknowledgements We would like to thank Ziteng Wang, Bingrui Li and Haocheng Xi for valuable discussions and help on the training large transformers. This work was supported by the National Key Research and Development Pro- gram of China (No. 2021ZD0110502), NSFC Projects (Nos. 62376131, 62061136001, 62106123, 62076147, U19A2081, 61972224), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J.Z is also supported by the XPlorer Prize. Impact Statement Our proposed efficient algorithm can be used to accelerate pre-training large-scale transformers like GLM (Du et al., 2022), LLaMA (Touvron et al., 2023), etc. Recently, large transformers have exhibited remarkable efficacy in various fields such as natural language processing, computer vision, and speech recognition. However, the pre-training stage of large transformers is computationally intensive and time- consuming. For instance, pre-training a GPT-4 can span several months, even using a supercomputer equipped with thousands of GPUs. Thus, acceleration approaches are nec- essary. Our fully sparse training approach of transformers can potentially accelerate the FFN layers of a model by the- oretical 2x faster, without loss of accuracy. Thus, it can be potentially used to save energy and reduce carbon footprint. But this work can also be used to accelerate baleful software, like software that generates malicious contents, which may have a negative impact on human society. References Anthony, L. F. W., Kanding, B., and Selvan, R. Carbon- tracker: Tracking and predicting the carbon footprint of training deep learning models, 2020. Bengio, Y ., L´eonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint- Amand, H., Soricut, R., Specia, L., and Tamchyna, A. Findings of the 2014 workshop on statistical machine translation. In WMT@ACL, 2014. URL https://api. semanticscholar.org/CorpusID:15535376. BUSATO, F. and POOL, J. Exploiting nvidia ampere struc- tured sparsity with cusparselt [online]. 2020 [visited on 2021-10-10]. Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y ., Wang, Z., and Carbin, M. The lottery ticket hypothesis for pre- trained bert networks, 2020. Chen, X., Cheng, Y ., Wang, S., Gan, Z., Wang, Z., and Liu, J. Earlybert: Efficient bert training via early-bird lottery tickets, 2021. Chmiel, B., Hubara, I., Banner, R., and Soudry, D. Min- imum variance unbiased n:m sparsity for the neural gradients. In The Eleventh International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=vuD2xEtxZcj. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vi- sion and Pattern Recognition, pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding, 2019. Du, Z., Qian, Y ., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. Glm: General language model pretraining with autoregressive blank infilling, 2022. Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners, 2021. Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks, 2019. Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. Stabilizing the lottery ticket hypothesis, 2020. Geiping, J. and Goldstein, T. Cramming: Training a lan- guage model on a single gpu in one day, 2022. Gokaslan, A. and Cohen, V . Openwebtext cor- pus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Han, S., Pool, J., Tran, J., and Dally, W. J. Learning both weights and connections for efficient neural networks, 2015. Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2016. Han, S., Pool, J., Narang, S., Mao, H., Gong, E., Tang, S., Elsen, E., Vajda, P., Paluri, M., Tran, J., Catanzaro, B., and Dally, W. J. Dsd: Dense-sparse-dense training for deep neural networks, 2017. Hu, Z., Lan, Y ., Wang, L., Xu, W., Lim, E.-P., Lee, R. K.-W., Bing, L., and Poria, S. Llm-adapters: An adapter fam- ily for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. 10Accelerating Transformer Pre-training with 2:4 Sparsity Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, S., and Soudry, D. Accelerated sparse neural training: A provable and efficient method to find n:m transposable masks, 2021. Karpathy, A. nanogpt. https://github.com/ karpathy/nanoGPT/, 2023. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization, 2017. Lasby, M., Golubeva, A., Evci, U., Nica, M., and Ioannou, Y . Dynamic sparse training with structured sparsity, 2023. Lee, N., Ajanthan, T., and Torr, P. H. Snip: Single-shot network pruning based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., and Gonzalez, J. Train big, then compress: Rethinking model size for efficient training and inference of trans- formers. In International Conference on machine learn- ing, pp. 5958–5968. PMLR, 2020. Loshchilov, I. and Hutter, F. Decoupled weight decay regu- larization, 2019. Lu, Y ., Agrawal, S., Subramanian, S., Rybakov, O., Sa, C. D., and Yazdanbakhsh, A. Step: Learning n:m struc- tured sparsity masks from scratch with precondition, 2023. McDanel, B., Dinh, H., and Magallanes, J. Accelerating dnn training with structured data gradient pruning, 2022. Mishra, A., Latorre, J. A., Pool, J., Stosic, D., Stosic, D., Venkatesh, G., Yu, C., and Micikevicius, P. Accelerating sparse deep neural networks, 2021. Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL- HLT 2019: Demonstrations, 2019. Papineni, K., Roukos, S., Ward, T., and Zhu, W. J. Bleu: a method for automatic evaluation of machine translation. 10 2002. doi: 10.3115/1073083.1073135. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsu- pervised multitask learners. 2019. URL https: //api.semanticscholar.org/CorpusID: 160025533. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019. Shazeer, N. Glu variants improve transformer, 2020. Thakkar, V ., Ramani, P., Cecka, C., Shivam, A., Lu, H., Yan, E., Kosaian, J., Hoemmen, M., Wu, H., Kerr, A., Nicely, M., Merrill, D., Blasig, D., Qiao, F., Majcher, P., Springer, P., Hohnerbach, M., Wang, J., and Gupta, M. CUTLASS, January 2023. URL https://github. com/NVIDIA/cutlass. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image trans- formers & amp; distillation through attention. In Interna- tional Conference on Machine Learning, volume 139, pp. 10347–10357, July 2021a. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J ´egou, H. Training data-efficient image trans- formers & distillation through attention, 2021b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan- guage models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and anal- ysis platform for natural language understanding. In BlackboxNLP@EMNLP, 2018. URL https://api. semanticscholar.org/CorpusID:5034059. Xu, W., He, X., Cheng, K., Wang, P., and Cheng, J. Towards fully sparse training: Information restoration with spatial similarity. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 2929–2937, 2022. You, H., Li, C., Xu, P., Fu, Y ., Wang, Y ., Chen, X., Baraniuk, R. G., Wang, Z., and Lin, Y . Drawing early-bird tickets: Towards more efficient training of deep networks, 2022. Zhang, Y ., Luo, Y ., Lin, M., Zhong, Y ., Xie, J., Chao, F., and Ji, R. Bi-directional masks for efficient n:m sparse training, 2023. Zhou, A., Ma, Y ., Zhu, J., Liu, J., Zhang, Z., Yuan, K., Sun, W., and Li, H. Learning n:m fine-grained structured sparse neural networks from scratch, 2021. Zhou, D., Ye, M., Chen, C., Meng, T., Tan, M., Song, X., Le, Q., Liu, Q., and Schuurmans, D. Go wide, then narrow: Efficient training of deep thin networks. In In- ternational Conference on Machine Learning, pp. 11546– 11555. PMLR, 2020. 11Accelerating Transformer Pre-training with 2:4 Sparsity A. 2:4-spMM A.1. 2:4 Sparsity Examples of row-wise, column-wise and transposable 2:4 sparse matrix are shown in Figure 8. Note that transposable 2:4 sparsity aligns with both row-wise and column-wise 2:4 sparsity. Figure 8.Row-wise 2:4, column-wise and transposable 2:4 sparse matrix. A.2. Array Layout The array layout of different types of matrix multiplications are listed in Table 12, which explains why output activations and activation gradients are column-major matrices in FST. Table 12.Array layout of MN. Here S denotes that the matrix is in row-wise 2:4 sparsity, R denotes row-major dense matrix, and C denotes column-major dense matrix. M N S S ⊤ R C S % % R R S⊤ % % % % R % C R R C % C R R B. Workflow The main workflow of a single linear layer in FST process is depicted in Figure 9. Figure 9.2:4 sparse training iteration for a layer on a single batch. 12Accelerating Transformer Pre-training with 2:4 Sparsity C. Training Loss Curve For BERT-base and GPT-2, we depict training loss curve in Figure 10. Figure 10.Left: train loss of GPT-2; right: train loss of BERT. D. Profiling result To explain how we reach 1.3x block speedup, we profile our code and break down the time costs as shown in the table below; see Table 13. Table 13.Time costs of each part of our network and the dense model in one iteration per layer. m denotes the accumulation steps over micro batches. Our method is evaluated on GPT-2, with batch size 16, sequence length 1024, embedding dimension 1024 and heads number 16. DENSE (MS/EXEC ) SPARSE (MS/EXEC ) ACCELERATION RATIO S FREQUENCY (EXEC /ITER ) FFN LINEAR FWD GEMM 12173.8 7305.78 1.666324472 - BWD GEMM 23295 14080.82 1.654378083 - MVUE+ PRUNE 0 171.4 - - TOTAL 23295 14252.22 1.634482207 - TOTAL 35468.8 21558 1.645273216 - OTHERS 4 FWD 167 118.17 - - BWD 65.5 20.03 - - TOTAL 232.5 138.2 - - TOTAL FWD 12340.8 7423.95 1.662295678 - BWD 23360.5 14272.25 1.636777663 - TOTAL 35701.3 21696.2 1.645509352 - OTHERS FWD 6874.3 7090.55 - - BWD 13920.7 14117.45 - - TOTAL 20795 21208 - - TOTAL FWD 19215.1 14514.5 1.323855455 - BWD 37281.2 28389.7 1.313194574 - TOTAL 56496.3 42904.2 1.316801152 - MASKED DECAY 0 45.2 - 1 m PRUNE WEIGHTS 0 320.3 - 1 m TRANSPOSABLE MASK SEARCH 0 634.8 - 1 40m 4All functions in FFN except linear layers, i.e., activation function and dropout. 13",
      "meta_data": {
        "arxiv_id": "2404.01847v3",
        "authors": [
          "Yuezhou Hu",
          "Kang Zhao",
          "Weiyu Huang",
          "Jianfei Chen",
          "Jun Zhu"
        ],
        "published_date": "2024-04-02T11:12:42Z",
        "venue": "Proceedings of the 41st International Conference on Machine Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
        "pdf_url": "https://arxiv.org/pdf/2404.01847v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "This research addresses the challenge of accelerating large transformer pre-training by leveraging the 2:4 sparse matrix multiplication capabilities of NVIDIA Ampere GPUs. The main contributions include proposing three accuracy-preserving techniques for 2:4 sparse training: applying masked decay on gradients, developing a fast method to determine the optimal decay factor in the warm-up stage, and employing dense fine-tuning at the end of pre-training. Additionally, two practical acceleration techniques are introduced: using convolution to quickly compute transposable 2:4 masks, and optimizing gated activation functions to reduce GPU L2 cache misses. The paper presents the first reported end-to-end acceleration for pre-training transformers with 2:4 sparsity, demonstrating comparable or superior accuracy to dense training and achieving up to 1.2x actual acceleration for models like GPT-2 774M.",
        "methodology": "The core methodology involves applying 2:4 semi-structured sparsity, which allows for 2x faster matrix multiplications on NVIDIA Ampere GPUs, to the Feed-Forward Networks (FFNs) of transformers. To preserve accuracy, the authors define a 'flip rate' metric to monitor mask stability and propose a masked decay regularization applied specifically to gradients (rather than weights) to stabilize training, especially for Adam/AdamW optimizers. They devise a rapid warm-up stage test to determine a suitable decay factor (λW). A dense fine-tuning phase is introduced at the end of pre-training to enhance model quality, rather than at the beginning. For practical acceleration, the process of finding transposable 2:4 masks is reframed as a convolution operation, significantly speeding up mask generation. Gated activation functions like GEGLU are optimized by re-implementing their kernels to access column-major output activations efficiently, minimizing GPU L2 cache misses. Additionally, the frequency of mask updates is reduced. The implementation leverages CUTLASS for 2:4-spMMs and custom Triton kernels for other operations, including the Minimum-Variance Unbiased Estimator (MVUE) for gradient pruning.",
        "experimental_setup": "The proposed 2:4 sparse training algorithm was validated across several transformer models: BERT (16-layer), GPT-2 (124M, 355M, 774M, 1.5B parameters), Transformer-base for machine translation, and DeiT-tiny. BERT models were pre-trained on the C4 dataset, GPT-2 models on OpenWebText, DeiT-tiny on ImageNet-1K, and Transformer-base on the WMT 14 En-De dataset. Evaluation metrics included GLUE scores (for BERT and GPT-2), SQuAD scores (for GPT-2), ACC@1 and ACC@5 (for DeiT), and BLEU score (for Transformer-base). Comparisons were made against dense training baselines, a 'Half' dense BERT model (with reduced FFN dimensionality but similar FLOPs), STEP, and Bi-Mask methods. An ablation study on BERT-base investigated the individual contributions of masked decay, MVUE, and dense fine-tuning. All experiments were conducted using FP16 mixed precision training on RTX3090 GPUs. Speedups were measured for single FFN layers, transformer blocks, and end-to-end network training.",
        "limitations": "The acceleration benefits are specifically tied to NVIDIA Ampere architecture GPUs, which support the fine-grained 2:4 sparse matrix multiplication. While the paper demonstrates significant improvements over existing sparse training methods for transformers, a 'Half' dense model (with simply reduced dimensionality) sometimes offers comparable or better accuracy than other sparse methods (STEP, Bi-Mask), highlighting a challenge in showing definitive superiority of sparsity over simple model reduction for similar FLOPs. The end-to-end acceleration of 1.2x, while notable, is less than the theoretical 2x speedup offered by 2:4-spMM for FFN layers, indicating that other parts of the transformer architecture or training overheads still limit overall throughput. The impact statement also acknowledges that this technology, while beneficial for accelerating research, could potentially be misused to accelerate 'baleful software' like that which generates malicious content.",
        "future_research_directions": "Future research could explore extending this 2:4 sparsity acceleration to other components of the transformer architecture beyond FFNs, such as the attention mechanism, to achieve even greater overall speedups. Investigating the applicability and optimization of these techniques on different hardware architectures or with other structured sparsity patterns could also be valuable. Further work could focus on developing more advanced or adaptive methods for automatically determining optimal hyperparameters like the masked decay factor. Additionally, the ethical implications and potential negative societal impacts mentioned in the paper's impact statement warrant further research into safeguards or responsible deployment strategies for accelerated AI models."
      }
    }
  ],
  "new_method": {
    "method": "{\n    \"Open Problems\": \"Tempo removes most large activations but still keeps all binary Dropout / Attention masks as 1-byte (uint8) tensors.  For long-sequence Transformers these masks alone can occupy hundreds of MB and become the new memory bottleneck.  The key open problem is: how can we store those strictly-binary tensors with 1-bit precision while keeping back-prop fast and fully compatible with PyTorch autograd?\",\n    \"Methods\": \"PackedMask Dropout (PM-Drop)\\n1. Replace every Dropout (and future Attention, Head & MLP binary gates) by a custom autograd function that bit-packs the Boolean mask.\\n   • Forward: generate the standard Boolean mask M (torch.bool), reshape to (N, .., 8) and compress 8 bits into a single uint8 using bitwise   operations (torch.bitwise_or & left-shift).  Store the packed mask P and dropout scale 1⁄(1-p).\\n   • Backward: on first use, unpack P back to the Boolean mask with bitwise_and & right-shift.  Multiply the unpacked mask with upstream gradient as usual.\\n2. Because only the mask tensor is changed, the objective function and model weights stay untouched—training dynamics are identical.\\nTheoretical motivation: masks carry exactly 1 bit of information; storing them in 8 bits is an 8× waste.  Since mask generation and application are element-wise, the extra (un)packing cost is O(#elements) and completely hidden by GPU memory-bandwidth savings for practical batch sizes.\",\n    \"Experimental Setup\": \"Model: Hugging-face TinyBERT (4-layer) fine-tuned on the MRPC subset of GLUE (to keep the run <5 min on a single GPU).\\nBaseline: standard Dropout.\\nProposed: replace every torch.nn.Dropout by PM-Drop (one-liner import).\\nMetrics:\\n• GPU peak memory (torch.cuda.max_memory_allocated).\\n• Training throughput (sequences/s).\\n• Validation accuracy/F1 to confirm identical learning behaviour.\\nExpectation: ~5-10 % lower overall memory (≈8× lower mask memory) with <1 % throughput change.\",\n    \"Experimental Code\": \"import torch, torch.nn as nn\\n\\nclass PackedDropoutFn(torch.autograd.Function):\\n    @staticmethod\\n    def forward(ctx, x, p, training):\\n        if not training or p == 0.0:\\n            ctx.save_for_backward(None)\\n            return x\\n        keep_prob = 1 - p\\n        mask = torch.rand_like(x) < keep_prob  # bool\\n        # pack every 8 bits into one byte\\n        flat = mask.flatten()\\n        pad = (-flat.numel()) % 8  # pad to multiple of 8\\n        if pad:\\n            flat = torch.cat([flat, flat.new_zeros(pad)])\\n        flat = flat.view(-1, 8)\\n        byte = sum((flat[:, i].byte() << i) for i in range(8))\\n        packed = byte.contiguous()\\n        ctx.save_for_backward(packed)\\n        ctx.shape = x.shape\\n        ctx.p = p\\n        return x * mask.div(keep_prob)\\n\\n    @staticmethod\\n    def backward(ctx, grad_out):\\n        packed, = ctx.saved_tensors\\n        if packed is None:\\n            return grad_out, None, None\\n        # unpack\\n        byte = packed.view(-1, 1)\\n        bits = (byte >> torch.arange(8, device=byte.device)) & 1\\n        mask = bits.flatten()[:grad_out.numel()].view(ctx.shape).bool()\\n        grad_in = grad_out * mask.div(1 - ctx.p)\\n        return grad_in, None, None\\n\\nclass PackedDropout(nn.Module):\\n    def __init__(self, p=0.1):\\n        super().__init__(); self.p = p\\n    def forward(self, x):\\n        return PackedDropoutFn.apply(x, self.p, self.training)\\n\\n# --- quick sanity test ---\\nif __name__ == \\\"__main__\\\":\\n    x = torch.randn(2, 3, 4, device='cuda')\\n    pd = PackedDropout(0.2).cuda()\\n    y = pd(x).sum()\\n    y.backward()\\n    print('peak mem:', torch.cuda.max_memory_allocated()/1e6, 'MB')\",\n    \"Expected Result\": \"1. Memory: Mask storage reduced by ≈8×.  For TinyBERT with sequence length 512 and batch 64 this cuts total GPU memory by ~6 %.  For BERT-Large the saving rises to 400–600 MB enabling +10-15 % larger batch size.\\n2. Speed: (Un)packing adds ≤0.3 ms per step; throughput change within ±1 %.\\n3. Accuracy/F1 on MRPC remains statistically identical (same stochastic graph).\",\n    \"Expected Conclusion\": \"PM-Drop fixes an overlooked inefficiency in state-of-the-art memory-optimized training: binary masks are stored with 8× too many bits.  Bit-packing is a one-line, framework-friendly change that frees hundreds of MB on typical Transformer workloads, converts directly into larger batch/sequence sizes, and composes seamlessly with existing techniques such as Tempo or activation checkpointing.  The idea is simple, yet delivers measurable acceleration without touching model quality or requiring special hardware.\"\n}"
  },
  "idea_info_history": [
    {
      "idea": {
        "open_problems": "Tempo removes most large activations but still keeps all binary Dropout / Attention masks as 1-byte (uint8) tensors.  For long-sequence Transformers these masks alone can occupy hundreds of MB and become the new memory bottleneck.  The key open problem is: how can we store those strictly-binary tensors with 1-bit precision while keeping back-prop fast and fully compatible with PyTorch autograd?",
        "methods": "PackedMask Dropout (PM-Drop)\n1. Replace every Dropout (and future Attention, Head & MLP binary gates) by a custom autograd function that bit-packs the Boolean mask.\n   • Forward: generate the standard Boolean mask M (torch.bool), reshape to (N, .., 8) and compress 8 bits into a single uint8 using bitwise   operations (torch.bitwise_or & left-shift).  Store the packed mask P and dropout scale 1⁄(1-p).\n   • Backward: on first use, unpack P back to the Boolean mask with bitwise_and & right-shift.  Multiply the unpacked mask with upstream gradient as usual.\n2. Because only the mask tensor is changed, the objective function and model weights stay untouched—training dynamics are identical.\nTheoretical motivation: masks carry exactly 1 bit of information; storing them in 8 bits is an 8× waste.  Since mask generation and application are element-wise, the extra (un)packing cost is O(#elements) and completely hidden by GPU memory-bandwidth savings for practical batch sizes.",
        "experimental_setup": "Model: Hugging-face TinyBERT (4-layer) fine-tuned on the MRPC subset of GLUE (to keep the run <5 min on a single GPU).\nBaseline: standard Dropout.\nProposed: replace every torch.nn.Dropout by PM-Drop (one-liner import).\nMetrics:\n• GPU peak memory (torch.cuda.max_memory_allocated).\n• Training throughput (sequences/s).\n• Validation accuracy/F1 to confirm identical learning behaviour.\nExpectation: ~5-10 % lower overall memory (≈8× lower mask memory) with <1 % throughput change.",
        "experimental_code": "import torch, torch.nn as nn\n\nclass PackedDropoutFn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, p, training):\n        if not training or p == 0.0:\n            ctx.save_for_backward(None)\n            return x\n        keep_prob = 1 - p\n        mask = torch.rand_like(x) < keep_prob  # bool\n        # pack every 8 bits into one byte\n        flat = mask.flatten()\n        pad = (-flat.numel()) % 8  # pad to multiple of 8\n        if pad:\n            flat = torch.cat([flat, flat.new_zeros(pad)])\n        flat = flat.view(-1, 8)\n        byte = sum((flat[:, i].byte() << i) for i in range(8))\n        packed = byte.contiguous()\n        ctx.save_for_backward(packed)\n        ctx.shape = x.shape\n        ctx.p = p\n        return x * mask.div(keep_prob)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        packed, = ctx.saved_tensors\n        if packed is None:\n            return grad_out, None, None\n        # unpack\n        byte = packed.view(-1, 1)\n        bits = (byte >> torch.arange(8, device=byte.device)) & 1\n        mask = bits.flatten()[:grad_out.numel()].view(ctx.shape).bool()\n        grad_in = grad_out * mask.div(1 - ctx.p)\n        return grad_in, None, None\n\nclass PackedDropout(nn.Module):\n    def __init__(self, p=0.1):\n        super().__init__(); self.p = p\n    def forward(self, x):\n        return PackedDropoutFn.apply(x, self.p, self.training)\n\n# --- quick sanity test ---\nif __name__ == \"__main__\":\n    x = torch.randn(2, 3, 4, device='cuda')\n    pd = PackedDropout(0.2).cuda()\n    y = pd(x).sum()\n    y.backward()\n    print('peak mem:', torch.cuda.max_memory_allocated()/1e6, 'MB')",
        "expected_result": "1. Memory: Mask storage reduced by ≈8×.  For TinyBERT with sequence length 512 and batch 64 this cuts total GPU memory by ~6 %.  For BERT-Large the saving rises to 400–600 MB enabling +10-15 % larger batch size.\n2. Speed: (Un)packing adds ≤0.3 ms per step; throughput change within ±1 %.\n3. Accuracy/F1 on MRPC remains statistically identical (same stochastic graph).",
        "expected_conclusion": "PM-Drop fixes an overlooked inefficiency in state-of-the-art memory-optimized training: binary masks are stored with 8× too many bits.  Bit-packing is a one-line, framework-friendly change that frees hundreds of MB on typical Transformer workloads, converts directly into larger batch/sequence sizes, and composes seamlessly with existing techniques such as Tempo or activation checkpointing.  The idea is simple, yet delivers measurable acceleration without touching model quality or requiring special hardware."
      },
      "evaluate": {
        "novelty_reason": "None of the cited papers actually implement a 1-bit representation for Dropout / Attention masks during training.  Tempo keeps the masks in the default 8-bit PyTorch bool format and merely recomputes the activations gated by those masks; the authors even list 1-bit storage only as a possible future improvement.  The new method turns this unimplemented suggestion into a concrete, end-to-end solution by (1) supplying an explicit bit-packing / unpacking algorithm that is differentiable and PyTorch-autograd–compatible, (2) packaging it as a drop-in nn.Module so every existing model can adopt it with a one-line change, and (3) showing that the extra kernel work is fully hidden by the memory-bandwidth savings.  While the idea of bit-packing itself is conceptually simple and has precedents in inference‐time compression, applying it to the training graph—with correct gradient flow, mask scaling, and no change to stochastic behaviour—has not been demonstrated in the literature surveyed.  Therefore the work offers a genuinely new but incremental contribution.",
        "novelty_score": 7,
        "significance_reason": "Mask tensors are small per element but dominate memory once sequence length and batch size grow; for BERT-Large they can reach 0.4–0.6 GB, which is on par with the gains reported by Tempo for several other layers.  Recovering this wasted 7/8 of mask memory translates directly into 10–15 % larger batches or longer sequences, benefits that stack with checkpointing, activation recomputation, and Tempo’s in-place layers.  The implementation imposes <1 % runtime overhead and requires no hardware changes, lowering the adoption barrier for both academia and industry.  However, the absolute memory saved (≈5-10 % total) is smaller than what larger-scale techniques (e.g., mixed precision, ZeRO, recomputation) already deliver, and the method does not improve FLOPs or model quality.  Hence it is practically useful but not transformative for the field.",
        "significance_score": 6
      }
    }
  ]
}