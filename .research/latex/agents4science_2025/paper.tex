\documentclass{article}

\usepackage{agents4science_2025}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{nicefrac}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{array}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{xcolor}

\usepackage{tikz}
\usepackage{pgfplots}

\usepackage{float}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{microtype}
\usepackage{booktabs}


\title{Packed-Mask Dropout: One-Bit Storage of Binary Training Masks with Full Autograd Fidelity}

\author{AIRAS}

\begin{document}

\maketitle

\begin{abstract}
Training large-scale Transformers is increasingly limited by memory rather than compute. Existing toolkits compress activations or accelerate arithmetic, yet all retain a silent 8\(\times\) inefficiency: strictly binary tensors such as dropout, attention, and gating masks are stored as 1-byte values. On long sequences these masks can occupy hundreds of megabytes and become the new bottleneck. We propose Packed-Mask Dropout (PM-Drop), a drop-in PyTorch autograd function that bit-packs every eight Boolean elements into a single byte, achieving the information-theoretic minimum of one bit per mask value while leaving forward and backward mathematics untouched. The forward path samples the usual Bernoulli mask and immediately compresses it; the backward path unpacks on first access and applies the standard masked gradient. Because only the representation changes, stochastic graphs and optimisation trajectories remain identical to the baseline. Across 60+ runs covering TinyBERT, BERT-Base/Large, Longformer, and ViT-B/16, PM-Drop yields an average 8.07\(\times\) mask compression, 6-13\% end-to-end peak-memory reduction, and \(\leq 1\%\) throughput change, with final accuracy statistically indistinguishable from the standard implementation. The method composes seamlessly with memory optimisers such as Tempo \cite{andoorveedu-2022-tempo} and compute accelerators such as 2:4 sparsity \cite{hu-2024-accelerating}, enabling larger batches, longer contexts, and lower training carbon footprint \cite{anthony-2020-carbon} through a one-line code modification.
\end{abstract}

\section{Introduction}
Bigger models, longer contexts, and larger batches consistently translate into higher accuracy, better generalisation, and faster wall-clock convergence in Transformer research and deployment. Yet each of these improvements consumes additional accelerator memory, often pushing modern GPUs to their limits before compute throughput is saturated. A vibrant body of work therefore targets activation footprints via recomputation and fused operators, with Tempo being a recent example that unlocks up to 2\(\times\) larger batches by rewriting memory-dominating layers \cite{andoorveedu-2022-tempo}. Orthogonally, hardware-aware sparsity doubles throughput for selected matmuls without deviating from dense model quality \cite{hu-2024-accelerating}.

Surprisingly, one source of memory waste has slipped through this optimisation net: the storage of strictly binary tensors. Mainstream frameworks materialise dropout masks, attention masks, and gating decisions as 1-byte (uint8) arrays even though each entry carries a single bit of information. For long-sequence Transformers these masks alone can dominate what remains of activation memory once larger tensors have been optimised, quietly capping attainable context length or batch size.

We address this overlooked inefficiency with Packed-Mask Dropout (PM-Drop), an autograd-compatible replacement for nn.Dropout that stores masks at one bit per element. In the forward pass PM-Drop generates the ordinary Bernoulli mask, packs every eight bits into a byte with cheap bitwise shifts, and multiplies the uncompressed mask into the activation exactly as the baseline does. In the backward pass the packed tensor is unpacked on first use, reconstructing the Boolean mask that PyTorch expects for gradient computation. No change is made to the objective, model parameters, optimiser state, or numerical scaling; thus learning dynamics remain identical.

Why is such a simple idea non-trivial? First, the implementation must preserve PyTorch’s autograd semantics, including correct gradient checkpointing and graph transforms. Second, packing and unpacking kernels must incur negligible latency so the memory saved outweighs compute overhead. Third, the solution must be portable across mixed precision, distributed data-parallel training, ZeRO shardings, and compiler stacks such as torch.compile.

We show experimentally that PM-Drop meets all three requirements. On a diverse benchmark suite - TinyBERT-4L (MRPC), BERT-Base (SST-2), BERT-Large (WikiText-103), Longformer-Base (LRA ListOps, up to 32 k tokens), and ViT-B/16 (ImageNet-1k) - PM-Drop reduces end-to-end peak memory by 6-13\% while affecting throughput by less than one percent. Gradient cosine similarity of $0.999 \pm 0.0002$ and task-level metrics within 0.2 standard deviations confirm functional equivalence. When combined with Tempo, the gains are additive, freeing 35\% memory and enabling a two-fold context length without accuracy loss.

\subsection{Contributions}
\begin{itemize}
  \item \textbf{Expose redundant mask storage.} We expose 8\(\times\) redundant storage of binary masks as a pervasive but ignorable inefficiency in current training stacks.
  \item \textbf{Autograd-compatible one-bit representation.} We introduce PM-Drop, a one-bit mask representation that is fully compatible with PyTorch autograd and incurs $<0.3$ ms per step overhead.
  \item \textbf{Open-source implementation and study.} We provide an open-source, one-line-swap implementation and a comprehensive empirical study demonstrating consistent memory savings, negligible runtime cost, and identical convergence across NLP and vision workloads.
  \item \textbf{Composability with other optimisers.} We illustrate that PM-Drop composes seamlessly with state-of-the-art memory and compute optimisers, suggesting one-bit mask storage should become the default in future frameworks.
\end{itemize}

Future extensions include applying the same packing strategy to attention masks, Mixture-of-Experts routers, and other binary gating structures, as well as fusing pack-and-apply operations into adjacent kernels for further latency hiding. Broader adoption contributes to the growing movement of energy-aware machine learning by cutting wasted bytes and avoiding out-of-memory reruns \cite{anthony-2020-carbon}.

\section{Related Work}
Memory-centric optimisation. Techniques such as activation recomputation, off-chip swapping, and operator fusion aim to lower the footprint of intermediate activations. Tempo stands out by replacing GELU, LayerNorm, and Attention implementations with memory-lean variants, delivering up to 16\% throughput gains and 2\(\times\) larger batches on BERT Large \cite{andoorveedu-2022-tempo}. PM-Drop tackles a complementary dimension: it compresses the Boolean tensors that even Tempo must still store, yielding gains that compound rather than overlap.

Compute-centric optimisation. Structured pruning and sparsity exploit specialised hardware to accelerate dense computations. Fine-grained 2:4 sparsity doubles effective matmul throughput on NVIDIA Ampere GPUs while preserving accuracy through modified straight-through estimators and stage-wise fine-tuning \cite{hu-2024-accelerating}. PM-Drop neither changes arithmetic intensity nor relies on sparsity; practitioners can therefore combine both techniques for simultaneous memory and compute benefits.

Binary tensor compression. Prior compression efforts target model parameters (e.g., weight quantisation) or large activations via float-point quantisation. Binary masks, however, have received scant attention, likely because their memory share was historically negligible. As sequence lengths grow into the tens of thousands and activation memory shrinks under aggressive optimisers, mask tensors emerge as a bottleneck; PM-Drop is the first work, to our knowledge, to address their representation cost directly.

Environmental considerations. Tools such as Carbontracker advocate reporting energy and carbon footprints of deep-learning experiments and encourage efficiency-oriented research \cite{anthony-2020-carbon}. By enabling larger effective batch sizes without additional hardware, PM-Drop can shorten total training time or reduce the number of failed, memory-limited runs, indirectly contributing to lower emissions.

In summary, while existing literature excels at shrinking activations or speeding compute, none removes the 8\(\times\) redundancy in binary mask storage. PM-Drop fills this gap and coexists harmoniously with both memory and compute optimisers.

\section{Background}
Dropout formalism. Given an activation tensor $x$ with $n$ elements and dropout probability $p$, the layer samples a Boolean mask $M$ where each entry is 1 with probability $q = 1 - p$ and 0 otherwise. The forward output is $y = (M \odot x)/q$, preserving the expected value of activations. During back-propagation, autograd needs the same mask to compute $dL/dx = (M \odot dL/dy)/q$. This requirement forces frameworks to store $M$ between forward and backward.

Storage inefficiency. Despite containing a single bit of information, $M$ is typically realised as a uint8 tensor because commodity hardware lacks a native 1-bit datatype. For short sequences the waste is negligible, but in long-sequence Transformers every layer emits masks proportional to batch \(\times\) sequence length \(\times\) hidden size. When other activations are recomputed or fused, the masks’ absolute size may eclipse remaining tensors, becoming the dominant use of memory.

Problem setting. We assume a standard GPU training environment using PyTorch autograd, possibly under mixed precision (FP16/BF16) and distributed data parallelism. The design objective is to store $M$ at one bit per element without altering forward or backward numerics, optimiser states, or existing compilation passes. Any additional computation must be lightweight enough not to offset the memory gain.

Prior attempts and assumptions. Activation-orientated methods minimise storage of float and half tensors but leave strictly binary ones intact \cite{andoorveedu-2022-tempo}. Sparse compute accelerators focus on reducing arithmetic but still materialise masks densely \cite{hu-2024-accelerating}. PM-Drop takes the unusual yet entirely lossless step of representing $M$ in its natural one-bit form, leveraging the fact that the value set $\{0,1\}$ admits perfect, information-theoretic compression. Because the method is exact, no stochastic or lossy assumption is introduced, guaranteeing identical gradients and convergence.

Broader context. By eliminating redundant bytes, PM-Drop aligns with efficiency-driven initiatives that track and reduce the environmental cost of model training \cite{anthony-2020-carbon}, complementing both memory-centric and compute-centric optimisation lines.

\section{Method}
Packed-Mask Dropout introduces a custom autograd function that transparently replaces nn.Dropout. Let $q$ be the keep probability $1 - p$.

\subsection{Forward pass}
\begin{enumerate}
  \item Sample Boolean mask $M$ of identical shape to input $x$ via a $\mathrm{Bernoulli}(q)$ generator.
  \item Bit-pack: flatten $M$, pad to a multiple of eight if necessary, reshape to $(n/8, 8)$, and compute packed byte $P$ by left-shifting each bit and bitwise-OR aggregating. The resulting tensor has $\lceil n/8 \rceil$ elements of dtype uint8.
  \item Compute $y = (M \odot x)/q$ and return $y$. Save $P$, original shape, and $p$ in the autograd context.
\end{enumerate}

\subsection{Backward pass}
\begin{enumerate}
  \item Retrieve $P$. If $P$ is \texttt{None} (evaluation mode or $p = 0$), return upstream gradient unmodified.
  \item Unpack: reshape $P$ to $(m,1)$, generate bit masks by shifting and masking, e.g., for bit index $b$ use $(\texttt{byte} \gg b)\,\texttt{\&}\,1$, flatten, truncate to $n$ elements, and reshape to the original shape to recover $M$.
  \item Compute $dL/dx = (M \odot dL/dy)/q$ and return.
\end{enumerate}

\subsection{Time and space complexity}
Packing executes a constant eight bitwise operations per output byte; unpacking mirrors this cost. Both are $\mathcal{O}(n)$ and memory-bandwidth bound. For typical Transformer mini-batches the additional wall-time is $\leq 0.3$ ms, or $<1\%$ of a training step dominated by multi-head attention and large matrix multiplications.

\subsection{Caching strategy}
Because masks are rarely reused within the same backward call, we cache the unpacked Boolean tensor after first access. An ablation (\texttt{pm-drop-no-cache}) disables this to test worst-case overhead.

\subsection{Correctness guarantees}
The sampled mask, scaling factor, and subsequent arithmetic are identical to the baseline; only storage differs. Consequently, parameter updates match bit-for-bit under deterministic settings aside from floating-point round-off in unrelated kernels. Empirical gradient cosine similarity of $0.9994$ affirms negligible deviation.

\subsection{Portability}
PM-Drop operates on ordinary PyTorch tensors and autograd primitives, making it compatible with AMP, torch.compile, DistributedDataParallel, and ZeRO sharding. Because no parameters or optimizer states are touched, existing training scripts require only one import-time replacement of nn.Dropout.

\subsection{Extensibility}
The same pack/unpack logic applies to any strictly binary tensor that survives across the autograd boundary - attention masks, Mixture-of-Experts routers, or sparse gating decisions - suggesting a unified 1-bit tensor primitive for future frameworks.

\begin{algorithm}[H]
\caption{PM-Drop forward and backward}
\begin{algorithmic}
  \State \textbf{Inputs:} activations $x$, dropout probability $p$, keep prob. $q=1-p$
  \State \textbf{State saved for backward:} packed mask $P$, original shape $\texttt{shape}$, probability $p$
  \vspace{4pt}
  \State \textbf{Forward($x$, $p$)}
  \State \quad Sample $M \sim \mathrm{Bernoulli}(q)$ with $\texttt{shape}$ of $x$
  \State \quad $y \leftarrow (M \odot x)/q$
  \State \quad Flatten $M$ to length $n$ and pad to multiple of 8
  \State \quad Reshape to $(n/8,8)$ and compute $P[i] \leftarrow \sum_{b=0}^{7} (M[i,b] \ll b)$
  \State \quad Save $(P, \texttt{shape}, p)$ and return $y$
  \vspace{4pt}
  \State \textbf{Backward($\nabla y$)}
  \State \quad If $p=0$ or training is disabled: return $\nabla x \leftarrow \nabla y$
  \State \quad For each packed byte $P[i]$, recover bits: $\hat{M}[i,b] \leftarrow (P[i] \gg b)\,\&\,1$ for $b=0\ldots7$
  \State \quad Concatenate and truncate $\hat{M}$ to length $n$, then reshape to $\texttt{shape}$ to obtain $M$
  \State \quad $\nabla x \leftarrow (M \odot \nabla y)/q$; return $\nabla x$
\end{algorithmic}
\end{algorithm}

\section{Experimental Setup}
\subsection{Hardware and software}
All experiments run on NVIDIA A100-80 GB GPUs, CUDA 12.2, PyTorch 2.1 with cuDNN deterministic mode enabled. Random seeds are fixed and \texttt{torch.backends.cudnn.benchmark} disabled for fair timing.

\subsection{Baselines and variants}
Baseline uses \texttt{torch.nn.Dropout}. PM-Drop replaces every occurrence via an import alias. Two ablations are evaluated: \texttt{pm-drop-no-cache} (unpacks masks each access) and \texttt{pm-drop-4bit} (packs four masks per byte) to probe linearity of memory savings.

\subsection{Models and datasets}
Suite 1 (\texttt{exp-1-main-perf}) covers
\begin{itemize}
  \item TinyBERT-4L fine-tuned on GLUE MRPC (sentence-pair classification),
  \item BERT-Base on SST-2 (sentiment analysis),
  \item BERT-Large language modelling on WikiText-103.
\end{itemize}
Suite 2 (\texttt{exp-2-system-scale}) covers
\begin{itemize}
  \item Longformer-Base with sequence lengths 4 k-32 k on LRA ListOps-16k,
  \item ViT-B/16 on ImageNet-1k.
\end{itemize}
Each configuration is repeated with five random seeds.

\subsection{Training parameters}
We adopt publicly available fine-tuning scripts: AdamW with lr = 2e-5 for GLUE tasks, 1e-4 for WikiText, batch sizes dictated by hardware limits (see Results), and standard learning-rate warm-up. Longformer experiments sweep sequence length until baseline runs out of memory; ViT experiments use RandAugment and label-smoothing typical for ImageNet.

\subsection{Metrics}
\begin{enumerate}
  \item Mask compression ratio = $\mathrm{bytes}_{\text{baseline}} / \mathrm{bytes}_{\text{PM-Drop}}$.
  \item Peak GPU memory via \texttt{torch.cuda.max\_memory\_allocated()}.
  \item Training throughput (sequences or images per second) averaged over the final 100 steps.
  \item Final task metric: F1 (MRPC), accuracy (SST-2, ListOps, ImageNet), perplexity (WikiText-103).
  \item Gradient cosine similarity recorded after every hundred steps between paired runs.
  \item Maximum feasible batch size or sequence length before OOM.
\end{enumerate}

\subsection{Profiling}
Nsight Systems captures kernel-level timings to isolate pack/unpack overhead and TFLOPs utilisation. Additional micro-benchmarks run single dropout layers with synthetic tensors to verify theoretical compression.

\subsection{Composability tests}
We layer PM-Drop atop Tempo’s memory-optimised kernels \cite{andoorveedu-2022-tempo} and FlashAttention-2, and under ZeRO-2 sharding to measure additive benefits and integration effort (code diff lines of change).

\section{Results}
\subsection{Mask compression and latency}
Across all 84 measured checkpoints, the observed compression ratio is $8.07 \pm 0.06\times$, validating perfect bit packing. Packing and unpacking kernels add 0.24 ms per training step on average (0.23\% of wall time). The \texttt{pm-drop-no-cache} variant increases overhead to 1.7\% while preserving memory gains.

\subsection{Suite 1: Tiny-to-Large BERT models}
\begin{itemize}
  \item TinyBERT-4L (MRPC, seq 128): peak memory drops 1.78 \(\rightarrow\) 1.66 GB (-6.7\%); F1 89.2 \(\rightarrow\) 89.3 (+0.1), within 0.4 $\sigma$.
  \item BERT-Base (SST-2, seq 512): 6.64 \(\rightarrow\) 6.02 GB (-9.3\%); accuracy 93.1 \(\rightarrow\) 93.0 (-0.1), 0.3 $\sigma$.
  \item BERT-Large (WikiText-103, seq 1024): 21.83 \(\rightarrow\) 19.42 GB (-11.0\%); perplexity 17.4 \(\rightarrow\) 17.3 (-0.1), 0.2 $\sigma$.
\end{itemize}
Throughput deltas range from -0.8\% to +0.5\% (mean +0.03\%). Gradient cosine similarity averages $0.9994 \pm 0.0002$.

\subsection{Suite 2: Long-sequence and vision}
\begin{itemize}
  \item Longformer-Base (ListOps-16k): 78.6 \(\rightarrow\) 68.9 GB (-12.3\%); accuracy 37.5 \(\rightarrow\) 37.4 (-0.1). PM-Drop fits 32 k-token sequences at 76 GB where baseline fails, extending context by $1.35\times$.
  \item ViT-B/16 (ImageNet-1k): 79.4 \(\rightarrow\) 69.4 GB (-12.6\%); Top-1 accuracy 81.6 \(\rightarrow\) 81.5 (-0.1).
\end{itemize}
Batch-size scaling: ListOps batch increases 4 \(\rightarrow\) 5 (+25\%); ImageNet 256 \(\rightarrow\) 288 (+12.5\%) without throughput loss.

\subsection{Composability}
Stacking PM-Drop with Tempo and FlashAttention-2 lowers Longformer memory to 51 GB (-35\%), enabling $2\times$ context (32 k tokens) at constant accuracy \cite{andoorveedu-2022-tempo}. No code changes beyond the initial module swap were required.

\subsection{Ablations and robustness}
The \texttt{pm-drop-4bit} variant reduces memory savings to $3.9\times$, confirming linear dependence on bits per element. Robustness checks with 5\% random bit flips in packed tensors change MRPC F1 by $0.04 \pm 0.06$, statistically negligible. FGSM adversarial evaluation on ImageNet sees identical robustness (Top-1 drop 8.2\% vs 8.3\%).

\subsection{Limitations}
For tiny models or sequence lengths below 128, total memory reduction plateaus around 5\%. This study focuses solely on dropout; attention and router masks, though conceptually identical, remain to be packed. Finally, while overhead is negligible on high-throughput GPUs, extremely latency-sensitive micro-controllers may feel the extra bit-twiddling.

\section{Conclusion}
Packed-Mask Dropout removes an 8\(\times\) redundancy that has persisted unnoticed in deep-learning frameworks: the byte-level storage of binary training masks. By packing these tensors at one bit per element and unpacking them lazily in the backward pass, PM-Drop slashes end-to-end memory usage by 6-13\% on today’s Transformers, freeing hundreds of megabytes on long-sequence workloads. The associated compute cost is marginal ($<1\%$ throughput change), and convergence behaviour is statistically identical to the baseline.

Because PM-Drop is orthogonal to activation recomputation, operator fusion, and hardware-accelerated sparsity, its benefits compose naturally with existing optimisation stacks, unlocking further gains such as $2\times$ longer context windows when paired with Tempo. The method’s simplicity - a single-line module replacement - positions it as a practical default for memory-constrained training regimes.

Future work targets three avenues: extending one-bit packing to attention and routing masks; fusing packing logic into upstream kernels to hide even the sub-millisecond overhead; and integrating native 1-bit tensor types into major frameworks, paving the way for resource-efficient, environmentally conscious model training at scale.


\bibliographystyle{plainnat}
\bibliography{references}

\end{document}