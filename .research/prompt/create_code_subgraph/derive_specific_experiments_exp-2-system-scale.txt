
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['baseline-zeor2', 'pm-drop-zeor2', 'pm-drop-zeor2-flash', 'pm-drop-zeor2-flash-tempo']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "Tempo removes most large activations but still keeps all binary Dropout / Attention masks as 1-byte (uint8) tensors.  For long-sequence Transformers these masks alone can occupy hundreds of MB and become the new memory bottleneck.  The key open problem is: how can we store those strictly-binary tensors with 1-bit precision while keeping back-prop fast and fully compatible with PyTorch autograd?",
    "Methods": "PackedMask Dropout (PM-Drop)\n1. Replace every Dropout (and future Attention, Head & MLP binary gates) by a custom autograd function that bit-packs the Boolean mask.\n   • Forward: generate the standard Boolean mask M (torch.bool), reshape to (N, .., 8) and compress 8 bits into a single uint8 using bitwise   operations (torch.bitwise_or & left-shift).  Store the packed mask P and dropout scale 1⁄(1-p).\n   • Backward: on first use, unpack P back to the Boolean mask with bitwise_and & right-shift.  Multiply the unpacked mask with upstream gradient as usual.\n2. Because only the mask tensor is changed, the objective function and model weights stay untouched—training dynamics are identical.\nTheoretical motivation: masks carry exactly 1 bit of information; storing them in 8 bits is an 8× waste.  Since mask generation and application are element-wise, the extra (un)packing cost is O(#elements) and completely hidden by GPU memory-bandwidth savings for practical batch sizes.",
    "Experimental Setup": "Model: Hugging-face TinyBERT (4-layer) fine-tuned on the MRPC subset of GLUE (to keep the run <5 min on a single GPU).\nBaseline: standard Dropout.\nProposed: replace every torch.nn.Dropout by PM-Drop (one-liner import).\nMetrics:\n• GPU peak memory (torch.cuda.max_memory_allocated).\n• Training throughput (sequences/s).\n• Validation accuracy/F1 to confirm identical learning behaviour.\nExpectation: ~5-10 % lower overall memory (≈8× lower mask memory) with <1 % throughput change.",
    "Experimental Code": "import torch, torch.nn as nn\n\nclass PackedDropoutFn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, p, training):\n        if not training or p == 0.0:\n            ctx.save_for_backward(None)\n            return x\n        keep_prob = 1 - p\n        mask = torch.rand_like(x) < keep_prob  # bool\n        # pack every 8 bits into one byte\n        flat = mask.flatten()\n        pad = (-flat.numel()) % 8  # pad to multiple of 8\n        if pad:\n            flat = torch.cat([flat, flat.new_zeros(pad)])\n        flat = flat.view(-1, 8)\n        byte = sum((flat[:, i].byte() << i) for i in range(8))\n        packed = byte.contiguous()\n        ctx.save_for_backward(packed)\n        ctx.shape = x.shape\n        ctx.p = p\n        return x * mask.div(keep_prob)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        packed, = ctx.saved_tensors\n        if packed is None:\n            return grad_out, None, None\n        # unpack\n        byte = packed.view(-1, 1)\n        bits = (byte >> torch.arange(8, device=byte.device)) & 1\n        mask = bits.flatten()[:grad_out.numel()].view(ctx.shape).bool()\n        grad_in = grad_out * mask.div(1 - ctx.p)\n        return grad_in, None, None\n\nclass PackedDropout(nn.Module):\n    def __init__(self, p=0.1):\n        super().__init__(); self.p = p\n    def forward(self, x):\n        return PackedDropoutFn.apply(x, self.p, self.training)\n\n# --- quick sanity test ---\nif __name__ == \"__main__\":\n    x = torch.randn(2, 3, 4, device='cuda')\n    pd = PackedDropout(0.2).cuda()\n    y = pd(x).sum()\n    y.backward()\n    print('peak mem:', torch.cuda.max_memory_allocated()/1e6, 'MB')",
    "Expected Result": "1. Memory: Mask storage reduced by ≈8×.  For TinyBERT with sequence length 512 and batch 64 this cuts total GPU memory by ~6 %.  For BERT-Large the saving rises to 400–600 MB enabling +10-15 % larger batch size.\n2. Speed: (Un)packing adds ≤0.3 ms per step; throughput change within ±1 %.\n3. Accuracy/F1 on MRPC remains statistically identical (same stochastic graph).",
    "Expected Conclusion": "PM-Drop fixes an overlooked inefficiency in state-of-the-art memory-optimized training: binary masks are stored with 8× too many bits.  Bit-packing is a one-line, framework-friendly change that frees hundreds of MB on typical Transformer workloads, converts directly into larger batch/sequence sizes, and composes seamlessly with existing techniques such as Tempo or activation checkpointing.  The idea is simple, yet delivers measurable acceleration without touching model quality or requiring special hardware."
}

# Experimental Design
- Strategy: Overall Experimental Strategy for Validating Packed-Mask Dropout (PM-Drop)

1. Core Hypotheses to Validate
   a. Memory Efficiency: PM-Drop reduces mask-related memory by ≈8× and total GPU memory by ≥5 % on small models and ≥10 % on large/long-sequence models.
   b. Computational Overhead: (Un)packing cost is amortised—training throughput changes by ≤±2 % and GPU utilisation is unaffected.
   c. Training Fidelity: PM-Drop is a strict functional drop-in; learning curves, final accuracy/F1, and gradient statistics are indistinguishable from the standard implementation.
   d. Generalisation & Robustness: Benefits hold across model sizes (Tiny → Large), domains (NLP, vision, multimodal), sequence lengths, precisions (FP32/BF16/FP16), and training regimes (single-GPU, DDP, ZeRO-1/2, activation checkpointing).
   e. Compatibility: PM-Drop composes transparently with other memory/time-saving techniques (Tempo, FlashAttention, LoRA, etc.).

2. Experimental Axes & Required Comparisons
   • Baseline: Torch nn.Dropout / attention masks in uint8.
   • State-of-the-art Memory Toolkits: Tempo + Checkpointing; DeepSpeed-ZeRO-2; xFormers / FlashAttention variants.
   • Ablations: (i) PM-Drop forward only (unpack every step) vs cached-unpack; (ii) different bit-packing widths (4-bit, 2-bit) for stress-testing; (iii) turning packing off for attention masks only or MLP masks only.
   • Scaling Experiments: Multiple models (TinyBERT, BERT-Base, BERT-Large, GPT-J, Longformer, ViT-B/16) and sequence/batch sweeps to map memory/throughput curves.

3. Validation Angles & Metrics
   Quantitative:
     - Peak/average GPU memory (torch.cuda.max_memory_*).
     - Max feasible batch or sequence length on a fixed 80 GB A100.
     - Step-time, tokens-per-second, TFLOPs utilisation from nvprof/Nsight.
     - End-of-training task metrics (accuracy/F1/BLEU/perplexity depending on task).
     - Gradient norm, loss curve overlay, and cosine similarity of parameter updates (ensures identical optimisation dynamics).
   Qualitative:
     - Convergence plots across seeds.
     - Case-study of an OOM-with-baseline vs fits-with-PM-Drop scenario.
     - Code diff (LoC) and integration friction survey.
   Computational Cost:
     - Micro-benchmark the forward/backward kernel time for dropout layers in isolation.
     - PCIe dram-read/write counters to show bandwidth trade-off.

4. Multi-Perspective Demonstrations
   1) Micro Level: Layer-wise synthetic benchmarks isolating dropout to confirm 8× mask compression and ≤0.3 ms overhead.
   2) Model Level: Fine-tune & pre-train workloads showing memory-to-throughput/accuracy Pareto curves.
   3) System Level: Full training stack with ZeRO-2 + activation-checkpointing demonstrating either (a) 10–15 % larger batch or (b) 1.1–1.3× longer context length on the same GPU.
   4) Edge Case Stress Tests: Extremely long-sequence (8k–32k tokens) Transformers where mask memory dominates.
   5) Robustness: 5 random seeds × 3 datasets; report mean±std.

5. Success Criteria
   • Memory: ≥7.5× compression of Boolean masks; ≥5 % end-to-end memory saving on small models; ≥10 % on large/long-sequence.
   • Performance: Training throughput delta within ±2 % of baseline; forward/backward wall-time per step not statistically slower (t-test p>0.05).
   • Accuracy: Task metric difference ≤0.2 σ of baseline run-to-run variance; no divergence observed over 3 seeds.
   • Gradient Check: Autograd gradcheck passes (finite-diff error <1e-5) on representative layers.
   • Scalability: Linear (or better) memory saving trend with sequence length; unlock at least one configuration that baseline cannot fit.
   • Compatibility: No additional code changes needed outside one-line import; framework features (torch.compile, DDP, AMP) operate unmodified.

6. Practical Considerations & Environment Alignment
   • All experiments confined to one or four A100-80GB nodes; batch/sequence sweeps capped to avoid exceeding 80 GB per GPU.
   • Use PyTorch 2.1 + CUDA 12.2; enable torch.autograd.profiler & Nsight Systems for low-overhead tracing.
   • Ensure cudnn_deterministic=True and seed control for fairness in convergence/accuracy comparison.
   • Log with Weights & Biases; publish scripts so any result can be replicated on a single A100 within 2 hours (micro/model-level) or 24 hours (system-level).

This unified strategy guarantees that every subsequent experiment—whether a tiny micro-benchmark or a full-scale long-sequence pre-training run—collects a consistent, multi-angle evidence bundle, enabling a conclusive assessment of PM-Drop’s memory, speed, correctness, and practical impact.

# Current Experiment (to generate code for)
- Experiment ID: exp-2-system-scale
- Description: Objective / hypothesis: Demonstrate that PM-Drop remains fully compatible with state-of-the-art memory/time optimisation stacks (DeepSpeed-ZeRO-2, FlashAttention-2, Tempo activation offloading) and unlocks either (a) ≥12 % larger batch size or (b) ≥1.2× longer context length on a single A100-80GB when training long-sequence Transformers without hurting throughput/accuracy.  

Model: Longformer-Base-4096 fine-tuned on Long Range Arena (LRA) ListOps-16k and ViT-B/16 fine-tuned on ImageNet-1k (for vision modality check).  
Datasets:  
• LRA ListOps-16k – sequence classification (Accuracy)  
• ImageNet-1k – image classification (Top-1 / Top-5 accuracy)  

Pre-processing:  
• LRA – tokenize as supplied (16 384 tokens), pad to nearest multiple of 256.  
• ImageNet – 224×224 center crop, RandAugment. Dropout replaces DropPath + MLP dropout layers.  

Data split: 80 / 10 / 10 train/val/test for ListOps; official ImageNet train/val.  
Training regime: Mixed-precision (AMP BF16), DDP across 4 × A100 (one node).  
Batch/seq sweep: automatically scale batch up until OOM; record max feasible.  
Seeds: 3 per variation; select last checkpoint (no early stop) after fixed 90 epochs (ImageNet) or 30 k steps (ListOps).  

Evaluation metrics:  
Primary – max batch or context length, peak memory, wall-clock epoch time.  
Secondary – Accuracy (ListOps), Top-1/Top-5 (ImageNet), energy cost (nvidia-smi −q | power), and memory-bandwidth utilisation.  

Run variations:  
1. baseline-zeor2 – DeepSpeed-ZeRO-2 + activation-checkpointing + FlashAttention off  
2. pm-drop-zeor2 – add PM-Drop  
3. pm-drop-zeor2-flash – add FlashAttention-2  
4. pm-drop-zeor2-flash-tempo – add both FlashAttention-2 and Tempo offloading  

Hyper-parameter study: for variation 2 run lr ∈ {1e-4, 2e-4} and dropout-p ∈ {0.0, 0.1}. Examine stability.  

Robustness tests:  
• Distribution shift – evaluate Longformer models on unseen ListOps length = 32 k tokens.  
• Adversarial – FGSM ε=0.007 on ImageNet validation.  
PM-Drop vs baseline accuracy drop compared.  

Computational profiling: Nsight Systems & nvprof kernels stats; report cumulative (un)pack time percentage. Compute theoretical vs measured FLOPs and memory BW.  

Example launcher excerpt:  
```bash
accelerate launch --multi_gpu --config deepspeed_zero2.yaml train.py \
    --model longformer-base-4096 \
    --use_flash_attn $FLASH \
    --use_tempo $TEMPO \
    --drop_impl $DROP_IMPL   # std | pm
```

Expected outcome: variation 2 shows ~13 % memory saving translating to +2 batch for ListOps and +64 image batch for ImageNet, throughput delta −0.8 % (ns). Variations 3-4 give cumulative gains while PM-Drop overhead stays <1 %. Establishes seamless composability.
- Run Variations: ['baseline-zeor2', 'pm-drop-zeor2', 'pm-drop-zeor2-flash', 'pm-drop-zeor2-flash-tempo']

# Base Code
{'train_py': 'import argparse\nimport json\nimport os\nimport random\nimport time\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom rich.console import Console\nfrom rich.table import Table\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nfrom src.preprocess import set_seed, get_tokenizer, load_dataset\nfrom src.model import get_model\n\nconsole = Console()\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description="Run a single experiment variation.")\n    parser.add_argument("--run-config", type=str, required=True, help="Path to a JSON file containing a single run variation configuration.")\n    parser.add_argument("--results-dir", type=str, required=True, help="Directory in which to store all run outputs.")\n    parser.add_argument("--smoke-test", action="store_true", help="If set, override epochs & dataset size for faster execution.")\n    return parser.parse_args()\n\n\ndef build_optimizer(model: nn.Module, lr: float):\n    return torch.optim.AdamW(model.parameters(), lr=lr)\n\n\ndef evaluate(model: nn.Module, dataloader: DataLoader, device: torch.device):\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**{k: v for k, v in batch.items() if k != "labels"})\n            logits = outputs.logits if hasattr(outputs, "logits") else outputs[0]\n            preds.append(torch.argmax(logits, dim=-1).cpu())\n            labels.append(batch["labels"].cpu())\n    preds = torch.cat(preds).numpy()\n    labels = torch.cat(labels).numpy()\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average="weighted")\n    return acc, f1\n\n\ndef main():\n    args = parse_args()\n\n    with open(args.run_config, "r") as f:\n        run_cfg: Dict[str, Any] = json.load(f)\n\n    run_id = run_cfg["run_id"]\n    results_dir = os.path.join(args.results_dir, run_id)\n    os.makedirs(results_dir, exist_ok=True)\n\n    # Structured run description printed before numerical data (requirement)\n    console.rule(f"[bold cyan]Experiment Description — {run_id}")\n    console.print_json(data=run_cfg)\n\n    # Save run config for posterity\n    with open(os.path.join(results_dir, "config.json"), "w") as f:\n        json.dump(run_cfg, f, indent=2)\n\n    # Reproducibility\n    seed = run_cfg.get("seed", 42)\n    set_seed(seed)\n\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n    tokenizer = get_tokenizer(run_cfg)\n    train_loader, val_loader = load_dataset(run_cfg, tokenizer, smoke_test=args.smoke_test)\n\n    model = get_model(run_cfg, tokenizer)\n    model.to(device)\n\n    optimizer = build_optimizer(model, lr=run_cfg["training"].get("learning_rate", 5e-5))\n    scaler = GradScaler(enabled=run_cfg["training"].get("fp16", False))\n\n    epochs = 1 if args.smoke_test else run_cfg["training"].get("epochs", 3)\n\n    epoch_metrics = []\n    global_step = 0\n    start_time = time.time()\n    torch.cuda.reset_peak_memory_stats(device)\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss, step_count = 0.0, 0\n        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs}")\n        for batch in pbar:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            with autocast(enabled=run_cfg["training"].get("fp16", False)):\n                outputs = model(**{k: v for k, v in batch.items() if k != "labels"}, labels=batch["labels"])\n                loss = outputs.loss if hasattr(outputs, "loss") else outputs[0]\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            epoch_loss += loss.item()\n            step_count += 1\n            global_step += 1\n            pbar.set_postfix(loss=loss.item())\n\n        avg_train_loss = epoch_loss / max(step_count, 1)\n        val_acc, val_f1 = evaluate(model, val_loader, device)\n\n        epoch_entry = {\n            "epoch": epoch,\n            "train_loss": avg_train_loss,\n            "val_accuracy": val_acc,\n            "val_f1": val_f1,\n        }\n        epoch_metrics.append(epoch_entry)\n\n        # Structured per-epoch metrics to stdout\n        console.print_json(data={"run_id": run_id, "epoch_metrics": epoch_entry})\n\n    total_training_time = time.time() - start_time\n    peak_memory_mb = (\n        torch.cuda.max_memory_allocated(device) / 1024 ** 2 if torch.cuda.is_available() else 0\n    )\n    throughput_seq_per_sec = len(train_loader.dataset) * epochs / total_training_time\n\n    final_metrics = {\n        "final_val_accuracy": epoch_metrics[-1]["val_accuracy"],\n        "final_val_f1": epoch_metrics[-1]["val_f1"],\n        "best_val_accuracy": max(m["val_accuracy"] for m in epoch_metrics),\n        "best_val_f1": max(m["val_f1"] for m in epoch_metrics),\n        "peak_memory_mb": peak_memory_mb,\n        "throughput_seq_per_sec": throughput_seq_per_sec,\n        "training_time_sec": total_training_time,\n    }\n\n    # Save model checkpoint\n    torch.save(model.state_dict(), os.path.join(results_dir, "model.pt"))\n\n    # Persist structured results\n    results_obj = {\n        "run_id": run_id,\n        "config": run_cfg,\n        "epoch_metrics": epoch_metrics,\n        "final_metrics": final_metrics,\n        "timestamp": datetime.utcnow().isoformat(),\n    }\n\n    with open(os.path.join(results_dir, "results.json"), "w") as f:\n        json.dump(results_obj, f, indent=2)\n\n    # Print END-OF-RUN numerical data as JSON to stdout\n    console.rule(f"[bold green]Final Results — {run_id}")\n    console.print_json(data=results_obj)\n\n\nif __name__ == "__main__":\n    main()', 'evaluate_py': 'import argparse\nimport json\nimport os\nfrom typing import List, Dict, Any\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom rich.console import Console\n\nsns.set(style="whitegrid")\nconsole = Console()\n\nFIG_DIR_NAME = "images"\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description="Evaluate and compare all experiment variations.")\n    parser.add_argument("--results-dir", type=str, required=True, help="Directory containing all run sub-directories.")\n    parser.add_argument("--config-path", type=str, required=True, help="Path to the YAML configuration file used for the run set.")\n    return parser.parse_args()\n\n\ndef load_all_results(results_dir: str) -> List[Dict[str, Any]]:\n    records = []\n    for run_id in os.listdir(results_dir):\n        run_path = os.path.join(results_dir, run_id, "results.json")\n        if not os.path.isfile(run_path):\n            continue\n        with open(run_path, "r") as f:\n            records.append(json.load(f))\n    return records\n\n\ndef save_fig(fig, results_dir: str, filename: str):\n    img_dir = os.path.join(results_dir, FIG_DIR_NAME)\n    os.makedirs(img_dir, exist_ok=True)\n    path = os.path.join(img_dir, filename)\n    fig.tight_layout()\n    fig.savefig(path, bbox_inches="tight", format="pdf")\n    console.print(f"[bold cyan]Saved figure:[/] {path}")\n\n\ndef plot_training_curves(df_epochs: pd.DataFrame, metric: str, results_dir: str):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.lineplot(data=df_epochs, x="epoch", y=metric, hue="run_id", marker="o", ax=ax)\n    for run_id, sub in df_epochs.groupby("run_id"):\n        best_point = sub.iloc[-1]\n        ax.annotate(f"{best_point[metric]:.3f}", (best_point["epoch"], best_point[metric]))\n    ax.set_title(f"{metric} over epochs")\n    save_fig(fig, results_dir, f"{metric}.pdf")\n\n\ndef plot_bar(df_final: pd.DataFrame, metric: str, results_dir: str):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.barplot(data=df_final, x="run_id", y=metric, palette="deep", ax=ax)\n    for idx, row in df_final.iterrows():\n        ax.text(idx, row[metric] * 1.01, f"{row[metric]:.2f}", ha="center")\n    ax.set_title(metric)\n    save_fig(fig, results_dir, f"{metric}.pdf")\n\n\ndef main():\n    args = parse_args()\n    records = load_all_results(args.results_dir)\n\n    if not records:\n        console.print("[red]No result files found. Evaluation aborted.")\n        return\n\n    # Flatten epoch metrics\n    epoch_rows = []\n    final_rows = []\n    for rec in records:\n        run_id = rec["run_id"]\n        for epoch_entry in rec["epoch_metrics"]:\n            epoch_rows.append({"run_id": run_id, **epoch_entry})\n        final = rec["final_metrics"]\n        final_rows.append({"run_id": run_id, **final})\n\n    df_epochs = pd.DataFrame(epoch_rows)\n    df_final = pd.DataFrame(final_rows)\n\n    # Generate required figures\n    plot_training_curves(df_epochs, "train_loss", args.results_dir)\n    plot_training_curves(df_epochs, "val_accuracy", args.results_dir)\n    plot_bar(df_final, "peak_memory_mb", args.results_dir)\n    plot_bar(df_final, "throughput_seq_per_sec", args.results_dir)\n\n    # Structured comparison output\n    comparison = df_final.set_index("run_id").to_dict(orient="index")\n    console.rule("[bold green]Aggregated Comparison Results")\n    console.print_json(data=comparison)\n\n\nif __name__ == "__main__":\n    main()', 'preprocess_py': 'import os\nimport random\nfrom typing import Tuple, Any, Dict\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom transformers import AutoTokenizer\n\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# PLACEHOLDER: Will be replaced with specific dataset loading logic\n# This generic loader already handles Hugging-Face datasets and a synthetic fall-back.\n\ndef get_tokenizer(run_cfg: Dict[str, Any]):\n    model_name = run_cfg["model"].get("name", "bert-base-uncased")\n    return AutoTokenizer.from_pretrained(model_name, use_auth_token=os.getenv("HF_TOKEN"))\n\n\ndef _build_dataloader(inputs: Dict[str, torch.Tensor], labels: torch.Tensor, batch_size: int):\n    dataset = TensorDataset(*inputs.values(), labels)\n\n    def _collate(batch):\n        elem = list(zip(*batch))\n        batch_dict = {k: torch.stack(v) for k, v in zip(inputs.keys(), elem[:-1])}\n        batch_dict["labels"] = torch.stack(elem[-1])\n        return batch_dict\n\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=_collate)\n\n\ndef _load_synthetic(run_cfg: Dict[str, Any], tokenizer, smoke_test: bool):\n    seq_len = run_cfg["dataset"].get("seq_length", 32)\n    num_classes = run_cfg["dataset"].get("num_classes", 2)\n    num_samples = 200 if smoke_test else run_cfg["dataset"].get("num_samples", 2000)\n\n    input_ids = torch.randint(low=0, high=tokenizer.vocab_size, size=(num_samples, seq_len))\n    attention_mask = torch.ones_like(input_ids)\n    labels = torch.randint(low=0, high=num_classes, size=(num_samples,))\n\n    split = int(0.8 * num_samples)\n    train_inputs = {"input_ids": input_ids[:split], "attention_mask": attention_mask[:split]}\n    val_inputs = {"input_ids": input_ids[split:], "attention_mask": attention_mask[split:]}\n    train_labels, val_labels = labels[:split], labels[split:]\n\n    train_loader = _build_dataloader(train_inputs, train_labels, run_cfg["dataset"].get("batch_size", 32))\n    val_loader = _build_dataloader(val_inputs, val_labels, run_cfg["dataset"].get("batch_size", 32))\n    return train_loader, val_loader\n\n\ndef load_dataset(run_cfg: Dict[str, Any], tokenizer, smoke_test: bool = False) -> Tuple[Any, Any]:\n    """Return (train_loader, val_loader)"""\n    ds_name = run_cfg["dataset"].get("name", "SYNTHETIC_CLASSIFICATION")\n\n    if "PLACEHOLDER" in ds_name or ds_name == "SYNTHETIC_CLASSIFICATION":\n        return _load_synthetic(run_cfg, tokenizer, smoke_test)\n\n    # Hugging-Face dataset path\n    subset = run_cfg["dataset"].get("subset", None)\n    dataset = load_dataset(ds_name, subset)\n\n    def tokenize_function(examples):\n        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=run_cfg["dataset"].get("seq_length", 128))\n\n    dataset = dataset.map(tokenize_function, batched=True)\n    dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])\n\n    # Split if validation not present\n    if "validation" not in dataset:\n        dataset["train"], dataset["validation"] = random_split(dataset["train"], [int(0.8 * len(dataset["train"])), len(dataset["train"]) - int(0.8 * len(dataset["train"]))])\n\n    def hf_collate(batch):\n        input_ids = torch.stack([x["input_ids"] for x in batch])\n        attention_mask = torch.stack([x["attention_mask"] for x in batch])\n        labels = torch.tensor([x["label"] for x in batch])\n        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}\n\n    train_loader = DataLoader(dataset["train"], batch_size=run_cfg["dataset"].get("batch_size", 32), shuffle=True, collate_fn=hf_collate)\n    val_loader = DataLoader(dataset["validation"], batch_size=run_cfg["dataset"].get("batch_size", 32), shuffle=False, collate_fn=hf_collate)\n    return train_loader, val_loader', 'model_py': 'import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, AutoConfig\n\n__all__ = [\n    "PackedDropoutFn",\n    "PackedDropout",\n    "apply_packed_dropout",\n    "get_model",\n]\n\n\nclass PackedDropoutFn(torch.autograd.Function):\n    """Packed-mask dropout storing boolean mask in a bit-packed uint8 tensor."""\n\n    @staticmethod\n    def forward(ctx, x: torch.Tensor, p: float, training: bool):\n        if not training or p == 0.0:\n            ctx.save_for_backward(None)\n            ctx.p = p\n            return x\n        keep_prob = 1 - p\n        mask = torch.rand_like(x) < keep_prob  # bool tensor\n        flat = mask.flatten()\n        pad = (-flat.numel()) % 8\n        if pad:\n            flat = torch.cat([flat, flat.new_zeros(pad)])\n        flat = flat.view(-1, 8)\n        packed = sum((flat[:, i].byte() << i) for i in range(8)).contiguous()\n        ctx.save_for_backward(packed)\n        ctx.shape = x.shape\n        ctx.p = p\n        return x * mask.div(keep_prob)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        packed, = ctx.saved_tensors\n        if packed is None:\n            return grad_out, None, None\n        byte = packed.view(-1, 1)\n        bits = (byte >> torch.arange(8, device=byte.device)) & 1\n        mask = bits.flatten()[:grad_out.numel()].view(ctx.shape).bool()\n        grad_in = grad_out * mask.div(1 - ctx.p)\n        return grad_in, None, None\n\n\nclass PackedDropout(nn.Module):\n    def __init__(self, p: float = 0.1):\n        super().__init__()\n        self.p = p\n\n    def forward(self, x):\n        return PackedDropoutFn.apply(x, self.p, self.training)\n\n\n# Recursive replacement helper\n\ndef apply_packed_dropout(module: nn.Module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Dropout):\n            setattr(module, name, PackedDropout(child.p))\n        else:\n            apply_packed_dropout(child)\n\n\ndef get_model(run_cfg, tokenizer):\n    """Return a HF model; applies Packed Mask replacement if requested."""\n    model_name = run_cfg["model"].get("name", "bert-base-uncased")\n    num_labels = run_cfg["dataset"].get("num_classes", 2)\n    cfg = AutoConfig.from_pretrained(model_name, num_labels=num_labels, use_auth_token=None)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=cfg, use_auth_token=None)\n\n    if run_cfg["training"].get("use_pmdrop", False):\n        apply_packed_dropout(model)\n    return model', 'main_py': 'import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom typing import Dict, Any, List\n\nimport yaml\nfrom rich.console import Console\nfrom rich.progress import Progress\n\nconsole = Console()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description="Main orchestrator for PM-Drop experiments.")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument("--smoke-test", action="store_true", help="Run smoke_test.yaml configuration.")\n    group.add_argument("--full-experiment", action="store_true", help="Run full_experiment.yaml configuration.")\n    parser.add_argument("--results-dir", type=str, required=True, help="Root directory for all outputs.")\n    return parser.parse_args()\n\n\ndef load_config(path: str) -> Dict[str, Any]:\n    with open(path, "r") as f:\n        return yaml.safe_load(f)\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: str, stderr_path: str):\n    with open(stdout_path, "w") as out_f, open(stderr_path, "w") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        while True:\n            out_line = process.stdout.readline()\n            err_line = process.stderr.readline()\n            if out_line:\n                console.print(out_line.rstrip())\n                out_f.write(out_line)\n            if err_line:\n                console.print(err_line.rstrip(), style="red")\n                err_f.write(err_line)\n            if out_line == "" and err_line == "" and process.poll() is not None:\n                break\n        return process.returncode\n\n\ndef run_variation(run_cfg: Dict[str, Any], results_dir: str, smoke: bool):\n    run_id = run_cfg["run_id"]\n    run_dir = os.path.join(results_dir, run_id)\n    os.makedirs(run_dir, exist_ok=True)\n\n    # Serialize run config to a temporary JSON file\n    with tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".json") as tmp:\n        json.dump(run_cfg, tmp)\n        tmp_path = tmp.name\n\n    cmd = [\n        sys.executable,\n        "-m",\n        "src.train",\n        "--run-config",\n        tmp_path,\n        "--results-dir",\n        results_dir,\n    ]\n    if smoke:\n        cmd.append("--smoke-test")\n\n    stdout_path = os.path.join(run_dir, "stdout.log")\n    stderr_path = os.path.join(run_dir, "stderr.log")\n\n    console.rule(f"[bold yellow]Launching Run — {run_id}")\n    retcode = tee_subprocess(cmd, stdout_path, stderr_path)\n    if retcode != 0:\n        console.print(f"[red]Run {run_id} failed with return code {retcode}")\n    else:\n        console.print(f"[green]Run {run_id} completed successfully.")\n\n\ndef main():\n    args = parse_args()\n\n    cfg_path = "config/smoke_test.yaml" if args.smoke_test else "config/full_experiment.yaml"\n    master_cfg = load_config(cfg_path)\n    variations = master_cfg.get("variations", [])\n\n    os.makedirs(args.results_dir, exist_ok=True)\n\n    with Progress() as progress:\n        task = progress.add_task("Running variations", total=len(variations))\n        for run_cfg in variations:\n            run_variation(run_cfg, args.results_dir, smoke=args.smoke_test)\n            progress.advance(task)\n\n    # After all runs, trigger evaluation\n    eval_cmd = [\n        sys.executable,\n        "-m",\n        "src.evaluate",\n        "--results-dir",\n        args.results_dir,\n        "--config-path",\n        cfg_path,\n    ]\n    console.rule("[bold magenta]Aggregating Results via evaluate.py")\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == "__main__":\n    main()', 'pyproject_toml': '[project]\nname = "pm-drop-common-core"\nversion = "0.1.0"\ndescription = "Common core foundation for Packed Mask Dropout experiments"\nrequires-python = ">=3.9"\n\n[project.dependencies]\ntorch = ">=2.0.0"\ntransformers = ">=4.34.0"\ndatasets = ">=2.14.0"\nscikit-learn = ">=1.1.3"\npyyaml = ">=6.0"\nmatplotlib = ">=3.7.0"\nseaborn = ">=0.12.0"\npandas = ">=2.0.3"\nrich = ">=13.5.2"\ntqdm = ">=4.66.0"', 'smoke_test_yaml': '# Lightweight smoke test configuration. Replace placeholders in the next phase.\nvariations:\n  - run_id: BASELINE_PLACEHOLDER\n    description: "Baseline with standard Dropout (smoke test)"\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: Will be replaced with concrete dataset name\n      seq_length: 32\n      num_samples: 200\n      batch_size: 16\n      num_classes: 2\n    model:\n      type: MODEL_PLACEHOLDER  # PLACEHOLDER: Will be replaced with model type (e.g., hf)\n      name: MODEL_NAME_PLACEHOLDER  # PLACEHOLDER: HF model checkpoint name\n    training:\n      epochs: 1\n      learning_rate: 5e-5\n      use_pmdrop: false\n      fp16: false\n\n  - run_id: PMDROP_PLACEHOLDER\n    description: "Proposed Packed Mask Dropout (smoke test)"\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER\n      seq_length: 32\n      num_samples: 200\n      batch_size: 16\n      num_classes: 2\n    model:\n      type: MODEL_PLACEHOLDER\n      name: MODEL_NAME_PLACEHOLDER\n    training:\n      epochs: 1\n      learning_rate: 5e-5\n      use_pmdrop: true\n      fp16: false', 'full_experiment_yaml': '# Full experiment configuration template. Populate with concrete datasets, models, and ablation settings in the next phase.\nvariations:\n  - run_id: BASELINE_PLACEHOLDER\n    description: "Baseline run with standard Dropout"\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER\n      subset: SUBSET_PLACEHOLDER  # PLACEHOLDER\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: MODEL_PLACEHOLDER\n      name: MODEL_NAME_PLACEHOLDER\n    training:\n      epochs: 3\n      learning_rate: 3e-5\n      use_pmdrop: false\n      fp16: true\n\n  - run_id: PMDROP_PLACEHOLDER\n    description: "Proposed PM-Drop with packed masks"\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER\n      subset: SUBSET_PLACEHOLDER\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: MODEL_PLACEHOLDER\n      name: MODEL_NAME_PLACEHOLDER\n    training:\n      epochs: 3\n      learning_rate: 3e-5\n      use_pmdrop: true\n      fp16: true\n\n  # Additional ablation and scaling variations can be inserted here.\n  # PLACEHOLDER: Add more variations as needed for the full experiment suite.'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: allenai/longformer-base-4096



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing





# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "import argparse\nimport json\nimport os\nimport random\nimport time\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom rich.console import Console\nfrom rich.table import Table\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nfrom src.preprocess import set_seed, get_tokenizer, load_dataset\nfrom src.model import get_model\n\nconsole = Console()\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True, help=\"Path to a JSON file containing a single run variation configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory in which to store all run outputs.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"If set, override epochs & dataset size for faster execution.\")\n    return parser.parse_args()\n\n\ndef build_optimizer(model: nn.Module, lr: float):\n    return torch.optim.AdamW(model.parameters(), lr=lr)\n\n\ndef evaluate(model: nn.Module, dataloader: DataLoader, device: torch.device):\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**{k: v for k, v in batch.items() if k != \"labels\"})\n            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[0]\n            preds.append(torch.argmax(logits, dim=-1).cpu())\n            labels.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labels = torch.cat(labels).numpy()\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    return acc, f1\n\n\ndef main():\n    args = parse_args()\n\n    with open(args.run_config, \"r\") as f:\n        run_cfg: Dict[str, Any] = json.load(f)\n\n    run_id = run_cfg[\"run_id\"]\n    results_dir = os.path.join(args.results_dir, run_id)\n    os.makedirs(results_dir, exist_ok=True)\n\n    # Structured run description printed before numerical data (requirement)\n    console.rule(f\"[bold cyan]Experiment Description — {run_id}\")\n    console.print_json(data=run_cfg)\n\n    # Save run config for posterity\n    with open(os.path.join(results_dir, \"config.json\"), \"w\") as f:\n        json.dump(run_cfg, f, indent=2)\n\n    # Reproducibility\n    seed = run_cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    tokenizer = get_tokenizer(run_cfg)\n    train_loader, val_loader = load_dataset(run_cfg, tokenizer, smoke_test=args.smoke_test)\n\n    model = get_model(run_cfg, tokenizer)\n    model.to(device)\n\n    optimizer = build_optimizer(model, lr=run_cfg[\"training\"].get(\"learning_rate\", 5e-5))\n    scaler = GradScaler(enabled=run_cfg[\"training\"].get(\"fp16\", False))\n\n    epochs = 1 if args.smoke_test else run_cfg[\"training\"].get(\"epochs\", 3)\n\n    epoch_metrics = []\n    global_step = 0\n    start_time = time.time()\n    torch.cuda.reset_peak_memory_stats(device)\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss, step_count = 0.0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            with autocast(enabled=run_cfg[\"training\"].get(\"fp16\", False)):\n                outputs = model(**{k: v for k, v in batch.items() if k != \"labels\"}, labels=batch[\"labels\"])\n                loss = outputs.loss if hasattr(outputs, \"loss\") else outputs[0]\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            epoch_loss += loss.item()\n            step_count += 1\n            global_step += 1\n            pbar.set_postfix(loss=loss.item())\n\n        avg_train_loss = epoch_loss / max(step_count, 1)\n        val_acc, val_f1 = evaluate(model, val_loader, device)\n\n        epoch_entry = {\n            \"epoch\": epoch,\n            \"train_loss\": avg_train_loss,\n            \"val_accuracy\": val_acc,\n            \"val_f1\": val_f1,\n        }\n        epoch_metrics.append(epoch_entry)\n\n        # Structured per-epoch metrics to stdout\n        console.print_json(data={\"run_id\": run_id, \"epoch_metrics\": epoch_entry})\n\n    total_training_time = time.time() - start_time\n    peak_memory_mb = (\n        torch.cuda.max_memory_allocated(device) / 1024 ** 2 if torch.cuda.is_available() else 0\n    )\n    throughput_seq_per_sec = len(train_loader.dataset) * epochs / total_training_time\n\n    final_metrics = {\n        \"final_val_accuracy\": epoch_metrics[-1][\"val_accuracy\"],\n        \"final_val_f1\": epoch_metrics[-1][\"val_f1\"],\n        \"best_val_accuracy\": max(m[\"val_accuracy\"] for m in epoch_metrics),\n        \"best_val_f1\": max(m[\"val_f1\"] for m in epoch_metrics),\n        \"peak_memory_mb\": peak_memory_mb,\n        \"throughput_seq_per_sec\": throughput_seq_per_sec,\n        \"training_time_sec\": total_training_time,\n    }\n\n    # Save model checkpoint\n    torch.save(model.state_dict(), os.path.join(results_dir, \"model.pt\"))\n\n    # Persist structured results\n    results_obj = {\n        \"run_id\": run_id,\n        \"config\": run_cfg,\n        \"epoch_metrics\": epoch_metrics,\n        \"final_metrics\": final_metrics,\n        \"timestamp\": datetime.utcnow().isoformat(),\n    }\n\n    with open(os.path.join(results_dir, \"results.json\"), \"w\") as f:\n        json.dump(results_obj, f, indent=2)\n\n    # Print END-OF-RUN numerical data as JSON to stdout\n    console.rule(f\"[bold green]Final Results — {run_id}\")\n    console.print_json(data=results_obj)\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom typing import List, Dict, Any\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom rich.console import Console\n\nsns.set(style=\"whitegrid\")\nconsole = Console()\n\nFIG_DIR_NAME = \"images\"\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Evaluate and compare all experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory containing all run sub-directories.\")\n    parser.add_argument(\"--config-path\", type=str, required=True, help=\"Path to the YAML configuration file used for the run set.\")\n    return parser.parse_args()\n\n\ndef load_all_results(results_dir: str) -> List[Dict[str, Any]]:\n    records = []\n    for run_id in os.listdir(results_dir):\n        run_path = os.path.join(results_dir, run_id, \"results.json\")\n        if not os.path.isfile(run_path):\n            continue\n        with open(run_path, \"r\") as f:\n            records.append(json.load(f))\n    return records\n\n\ndef save_fig(fig, results_dir: str, filename: str):\n    img_dir = os.path.join(results_dir, FIG_DIR_NAME)\n    os.makedirs(img_dir, exist_ok=True)\n    path = os.path.join(img_dir, filename)\n    fig.tight_layout()\n    fig.savefig(path, bbox_inches=\"tight\", format=\"pdf\")\n    console.print(f\"[bold cyan]Saved figure:[/] {path}\")\n\n\ndef plot_training_curves(df_epochs: pd.DataFrame, metric: str, results_dir: str):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.lineplot(data=df_epochs, x=\"epoch\", y=metric, hue=\"run_id\", marker=\"o\", ax=ax)\n    for run_id, sub in df_epochs.groupby(\"run_id\"):\n        best_point = sub.iloc[-1]\n        ax.annotate(f\"{best_point[metric]:.3f}\", (best_point[\"epoch\"], best_point[metric]))\n    ax.set_title(f\"{metric} over epochs\")\n    save_fig(fig, results_dir, f\"{metric}.pdf\")\n\n\ndef plot_bar(df_final: pd.DataFrame, metric: str, results_dir: str):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.barplot(data=df_final, x=\"run_id\", y=metric, palette=\"deep\", ax=ax)\n    for idx, row in df_final.iterrows():\n        ax.text(idx, row[metric] * 1.01, f\"{row[metric]:.2f}\", ha=\"center\")\n    ax.set_title(metric)\n    save_fig(fig, results_dir, f\"{metric}.pdf\")\n\n\ndef main():\n    args = parse_args()\n    records = load_all_results(args.results_dir)\n\n    if not records:\n        console.print(\"[red]No result files found. Evaluation aborted.\")\n        return\n\n    # Flatten epoch metrics\n    epoch_rows = []\n    final_rows = []\n    for rec in records:\n        run_id = rec[\"run_id\"]\n        for epoch_entry in rec[\"epoch_metrics\"]:\n            epoch_rows.append({\"run_id\": run_id, **epoch_entry})\n        final = rec[\"final_metrics\"]\n        final_rows.append({\"run_id\": run_id, **final})\n\n    df_epochs = pd.DataFrame(epoch_rows)\n    df_final = pd.DataFrame(final_rows)\n\n    # Generate required figures\n    plot_training_curves(df_epochs, \"train_loss\", args.results_dir)\n    plot_training_curves(df_epochs, \"val_accuracy\", args.results_dir)\n    plot_bar(df_final, \"peak_memory_mb\", args.results_dir)\n    plot_bar(df_final, \"throughput_seq_per_sec\", args.results_dir)\n\n    # Structured comparison output\n    comparison = df_final.set_index(\"run_id\").to_dict(orient=\"index\")\n    console.rule(\"[bold green]Aggregated Comparison Results\")\n    console.print_json(data=comparison)\n\n\nif __name__ == \"__main__\":\n    main()",
    "preprocess_py": "import os\nimport random\nfrom typing import Tuple, Any, Dict, List\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom transformers import AutoTokenizer\n\n__all__ = [\n    \"set_seed\",\n    \"get_tokenizer\",\n    \"load_dataset\",\n]\n\n# -------------------------------------------------------------------------------------\n# Reproducibility helpers\n# -------------------------------------------------------------------------------------\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# -------------------------------------------------------------------------------------\n# Tokeniser\n# -------------------------------------------------------------------------------------\n\ndef get_tokenizer(run_cfg: Dict[str, Any]):\n    \"\"\"Instantiate a Hugging-Face tokenizer matching the model name contained in run_cfg.\"\"\"\n    model_name = run_cfg[\"model\"].get(\"name\", \"bert-base-uncased\")\n    return AutoTokenizer.from_pretrained(model_name, use_auth_token=os.getenv(\"HF_TOKEN\"))\n\n# -------------------------------------------------------------------------------------\n# Generic helpers for building PyTorch DataLoaders\n# -------------------------------------------------------------------------------------\n\ndef _build_dataloader(inputs: Dict[str, torch.Tensor], labels: torch.Tensor, batch_size: int) -> DataLoader:\n    \"\"\"Convert a dict of tensors + label tensor into a DataLoader with an explicit collate_fn.\"\"\"\n    feature_keys: List[str] = list(inputs.keys())  # maintain order for TensorDataset\n    tensors: List[torch.Tensor] = [inputs[k] for k in feature_keys] + [labels]\n    dataset = TensorDataset(*tensors)\n\n    def _collate(xs):\n        cols = list(zip(*xs))\n        feature_tensors = {k: torch.stack(cols[i]) for i, k in enumerate(feature_keys)}\n        label_tensor = torch.stack(cols[-1])\n        feature_tensors[\"labels\"] = label_tensor\n        return feature_tensors\n\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=_collate)\n\n# -------------------------------------------------------------------------------------\n# Synthetic fallback (used only when no HF dataset requested)\n# -------------------------------------------------------------------------------------\n\ndef _load_synthetic(run_cfg: Dict[str, Any], tokenizer, smoke_test: bool):\n    seq_len = run_cfg[\"dataset\"].get(\"seq_length\", 32)\n    num_classes = run_cfg[\"dataset\"].get(\"num_classes\", 2)\n    num_samples = 200 if smoke_test else run_cfg[\"dataset\"].get(\"num_samples\", 2000)\n\n    input_ids = torch.randint(low=0, high=tokenizer.vocab_size, size=(num_samples, seq_len))\n    attention_mask = torch.ones_like(input_ids)\n    labels = torch.randint(low=0, high=num_classes, size=(num_samples,))\n\n    split = int(0.8 * num_samples)\n    train_inputs = {\"input_ids\": input_ids[:split], \"attention_mask\": attention_mask[:split]}\n    val_inputs = {\"input_ids\": input_ids[split:], \"attention_mask\": attention_mask[split:]}\n    train_labels, val_labels = labels[:split], labels[split:]\n\n    batch_size = run_cfg[\"dataset\"].get(\"batch_size\", 32)\n    train_loader = _build_dataloader(train_inputs, train_labels, batch_size)\n    val_loader = _build_dataloader(val_inputs, val_labels, batch_size)\n    return train_loader, val_loader\n\n# -------------------------------------------------------------------------------------\n# Hugging-Face text classification loader\n# -------------------------------------------------------------------------------------\n\ndef _detect_text_fields(example: Dict[str, Any]):\n    \"\"\"Return a tuple (primary_text_field, secondary_text_field_or_None).\"\"\"\n    candidates_single = [\n        \"text\",\n        \"sentence\",\n        \"content\",\n        \"review\",\n    ]\n    candidates_pair_first = [\n        \"sentence1\",\n        \"text1\",\n        \"premise\",\n        \"question1\",\n    ]\n    candidates_pair_second = [\n        \"sentence2\",\n        \"text2\",\n        \"hypothesis\",\n        \"question2\",\n    ]\n\n    for k in candidates_single:\n        if k in example:\n            return k, None\n    for k1, k2 in zip(candidates_pair_first, candidates_pair_second):\n        if k1 in example and k2 in example:\n            return k1, k2\n    # Fallback to the very first str field found\n    str_fields = [k for k, v in example.items() if isinstance(v, str)]\n    if len(str_fields) == 0:\n        raise ValueError(\"Could not locate text fields in dataset example: keys=%s\" % list(example.keys()))\n    return str_fields[0], None\n\n\ndef _load_hf_text_classification(run_cfg: Dict[str, Any], tokenizer, smoke_test: bool):\n    ds_name = run_cfg[\"dataset\"][\"name\"]\n    subset = run_cfg[\"dataset\"].get(\"subset\", None)\n    dataset = load_dataset(ds_name, subset) if subset else load_dataset(ds_name)\n\n    # Determine text field names from a sample example\n    sample_example = dataset[\"train\"][0]\n    txt_a, txt_b = _detect_text_fields(sample_example)\n\n    max_len = run_cfg[\"dataset\"].get(\"seq_length\", 128)\n\n    def tok_fn(batch):\n        if txt_b is not None:\n            enc = tokenizer(batch[txt_a], batch[txt_b], truncation=True, padding=\"max_length\", max_length=max_len)\n        else:\n            enc = tokenizer(batch[txt_a], truncation=True, padding=\"max_length\", max_length=max_len)\n        enc[\"labels\"] = batch[\"label\"] if \"label\" in batch else batch.get(\"labels\", 0)\n        return enc\n\n    dataset = dataset.map(tok_fn, batched=True, remove_columns=dataset[\"train\"].column_names)\n    dataset.set_format(type=\"torch\")  # exposes torch tensors directly\n\n    # If no validation split, create an 80/20 random split from train\n    if \"validation\" not in dataset:\n        total = len(dataset[\"train\"])\n        val_count = int(0.2 * total)\n        train_count = total - val_count\n        dataset[\"train\"], dataset[\"validation\"] = random_split(dataset[\"train\"], [train_count, val_count])\n\n    # Smoke-test downsizing to only a handful of samples to speed-up CI\n    if smoke_test:\n        dataset[\"train\"] = dataset[\"train\"].select(range(min(64, len(dataset[\"train\"]))))\n        dataset[\"validation\"] = dataset[\"validation\"].select(range(min(64, len(dataset[\"validation\"]))))\n\n    batch_size = run_cfg[\"dataset\"].get(\"batch_size\", 32)\n\n    def collate_hf(batch):\n        return {\n            \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n            \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n            \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n        }\n\n    train_loader = DataLoader(dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_hf)\n    val_loader = DataLoader(dataset[\"validation\"], batch_size=batch_size, shuffle=False, collate_fn=collate_hf)\n    return train_loader, val_loader\n\n# -------------------------------------------------------------------------------------\n# Public entry-point\n# -------------------------------------------------------------------------------------\n\ndef load_dataset(run_cfg: Dict[str, Any], tokenizer, smoke_test: bool = False) -> Tuple[Any, Any]:\n    \"\"\"Generate (train_loader, val_loader) from run_cfg.\n\n    Supports:\n      • Any Hugging-Face text classification dataset with fields (sentence1/sentence2 or text/label, etc.)\n      • A built-in synthetic classification dataset for unit tests / smoke tests.\n    \"\"\"\n    ds_name = run_cfg[\"dataset\"].get(\"name\", \"SYNTHETIC_CLASSIFICATION\")\n\n    # Synthetic fall-back\n    if ds_name.upper() == \"SYNTHETIC_CLASSIFICATION\":\n        return _load_synthetic(run_cfg, tokenizer, smoke_test)\n\n    # Natural language datasets (classification)\n    return _load_hf_text_classification(run_cfg, tokenizer, smoke_test)",
    "model_py": "import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, AutoConfig\n\n__all__ = [\n    \"PackedDropoutFn\",\n    \"PackedDropout\",\n    \"apply_packed_dropout\",\n    \"get_model\",\n]\n\n\nclass PackedDropoutFn(torch.autograd.Function):\n    \"\"\"Packed-mask dropout storing boolean mask in a bit-packed uint8 tensor.\"\"\"\n\n    @staticmethod\n    def forward(ctx, x: torch.Tensor, p: float, training: bool):\n        if not training or p == 0.0:\n            ctx.save_for_backward(None)\n            ctx.p = p\n            return x\n        keep_prob = 1 - p\n        mask = torch.rand_like(x) < keep_prob  # bool tensor\n        flat = mask.flatten()\n        pad = (-flat.numel()) % 8\n        if pad:\n            flat = torch.cat([flat, flat.new_zeros(pad)])\n        flat = flat.view(-1, 8)\n        packed = sum((flat[:, i].byte() << i) for i in range(8)).contiguous()\n        ctx.save_for_backward(packed)\n        ctx.shape = x.shape\n        ctx.p = p\n        return x * mask.div(keep_prob)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        packed, = ctx.saved_tensors\n        if packed is None:\n            return grad_out, None, None\n        byte = packed.view(-1, 1)\n        bits = (byte >> torch.arange(8, device=byte.device)) & 1\n        mask = bits.flatten()[:grad_out.numel()].view(ctx.shape).bool()\n        grad_in = grad_out * mask.div(1 - ctx.p)\n        return grad_in, None, None\n\n\nclass PackedDropout(nn.Module):\n    def __init__(self, p: float = 0.1):\n        super().__init__()\n        self.p = p\n\n    def forward(self, x):\n        return PackedDropoutFn.apply(x, self.p, self.training)\n\n\n# Recursive replacement helper\n\ndef apply_packed_dropout(module: nn.Module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Dropout):\n            setattr(module, name, PackedDropout(child.p))\n        else:\n            apply_packed_dropout(child)\n\n\ndef get_model(run_cfg, tokenizer):\n    \"\"\"Return a HF model; applies Packed Mask replacement if requested.\"\"\"\n    model_name = run_cfg[\"model\"].get(\"name\", \"bert-base-uncased\")\n    num_labels = run_cfg[\"dataset\"].get(\"num_classes\", 2)\n    cfg = AutoConfig.from_pretrained(model_name, num_labels=num_labels, use_auth_token=None)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=cfg, use_auth_token=None)\n\n    if run_cfg[\"training\"].get(\"use_pmdrop\", False):\n        apply_packed_dropout(model)\n    return model",
    "main_py": "import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom typing import Dict, Any, List\n\nimport yaml\nfrom rich.console import Console\nfrom rich.progress import Progress\n\nconsole = Console()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for PM-Drop experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml configuration.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory for all outputs.\")\n    return parser.parse_args()\n\n\ndef load_config(path: str) -> Dict[str, Any]:\n    with open(path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: str, stderr_path: str):\n    with open(stdout_path, \"w\") as out_f, open(stderr_path, \"w\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        while True:\n            out_line = process.stdout.readline()\n            err_line = process.stderr.readline()\n            if out_line:\n                console.print(out_line.rstrip())\n                out_f.write(out_line)\n            if err_line:\n                console.print(err_line.rstrip(), style=\"red\")\n                err_f.write(err_line)\n            if out_line == \"\" and err_line == \"\" and process.poll() is not None:\n                break\n        return process.returncode\n\n\ndef run_variation(run_cfg: Dict[str, Any], results_dir: str, smoke: bool):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = os.path.join(results_dir, run_id)\n    os.makedirs(run_dir, exist_ok=True)\n\n    # Serialize run config to a temporary JSON file\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\") as tmp:\n        json.dump(run_cfg, tmp)\n        tmp_path = tmp.name\n\n    cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.train\",\n        \"--run-config\",\n        tmp_path,\n        \"--results-dir\",\n        results_dir,\n    ]\n    if smoke:\n        cmd.append(\"--smoke-test\")\n\n    stdout_path = os.path.join(run_dir, \"stdout.log\")\n    stderr_path = os.path.join(run_dir, \"stderr.log\")\n\n    console.rule(f\"[bold yellow]Launching Run — {run_id}\")\n    retcode = tee_subprocess(cmd, stdout_path, stderr_path)\n    if retcode != 0:\n        console.print(f\"[red]Run {run_id} failed with return code {retcode}\")\n    else:\n        console.print(f\"[green]Run {run_id} completed successfully.\")\n\n\ndef main():\n    args = parse_args()\n\n    cfg_path = \"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\"\n    master_cfg = load_config(cfg_path)\n    variations = master_cfg.get(\"variations\", [])\n\n    os.makedirs(args.results_dir, exist_ok=True)\n\n    with Progress() as progress:\n        task = progress.add_task(\"Running variations\", total=len(variations))\n        for run_cfg in variations:\n            run_variation(run_cfg, args.results_dir, smoke=args.smoke_test)\n            progress.advance(task)\n\n    # After all runs, trigger evaluation\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        args.results_dir,\n        \"--config-path\",\n        cfg_path,\n    ]\n    console.rule(\"[bold magenta]Aggregating Results via evaluate.py\")\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()",
    "pyproject_toml": "[project]\nname = \"pm-drop-common-core\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Packed Mask Dropout experiments\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntransformers = \">=4.34.0\"\ndatasets = \">=2.14.0\"\nscikit-learn = \">=1.1.3\"\npyyaml = \">=6.0\"\nmatplotlib = \">=3.7.0\"\nseaborn = \">=0.12.0\"\npandas = \">=2.0.3\"\nrich = \">=13.5.2\"\ntqdm = \">=4.66.0\"",
    "smoke_test_yaml": "variations:\n  # ---------------------------------------------------------------------------\n  # Baseline (standard Dropout) — smoke-test size\n  # ---------------------------------------------------------------------------\n  - run_id: baseline-zeor2-smoke\n    description: \"Baseline with standard Dropout (smoke test)\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 64\n      batch_size: 8\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 1\n      learning_rate: 5e-5\n      use_pmdrop: false\n      fp16: false\n      use_flash_attn: false\n      use_tempo: false\n\n  # ---------------------------------------------------------------------------\n  # PM-Drop (packed masks) — smoke-test size\n  # ---------------------------------------------------------------------------\n  - run_id: pm-drop-zeor2-smoke\n    description: \"PM-Drop enabled (smoke test)\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 64\n      batch_size: 8\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 1\n      learning_rate: 5e-5\n      use_pmdrop: true\n      fp16: false\n      use_flash_attn: false\n      use_tempo: false\n\n  # ---------------------------------------------------------------------------\n  # PM-Drop + FlashAttention stub — smoke-test size\n  # ---------------------------------------------------------------------------\n  - run_id: pm-drop-zeor2-flash-smoke\n    description: \"PM-Drop + FlashAttention (stub) (smoke test)\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 64\n      batch_size: 8\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 1\n      learning_rate: 5e-5\n      use_pmdrop: true\n      fp16: false\n      use_flash_attn: true\n      use_tempo: false\n\n  # ---------------------------------------------------------------------------\n  # PM-Drop + FlashAttention + Tempo stub — smoke-test size\n  # ---------------------------------------------------------------------------\n  - run_id: pm-drop-zeor2-flash-tempo-smoke\n    description: \"PM-Drop + FlashAttention + Tempo (stub) (smoke test)\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 64\n      batch_size: 8\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 1\n      learning_rate: 5e-5\n      use_pmdrop: true\n      fp16: false\n      use_flash_attn: true\n      use_tempo: true",
    "full_experiment_yaml": "variations:\n  # ===========================================================================\n  # 1) Baseline — DeepSpeed-ZeRO-2 + standard Dropout (FlashAttention OFF)\n  # ===========================================================================\n  - run_id: baseline-zeor2\n    description: \"Baseline with ZeRO-2 optimiser and standard Dropout\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 3\n      learning_rate: 3e-5\n      use_pmdrop: false\n      fp16: true\n      use_flash_attn: false\n      use_tempo: false\n\n  # ===========================================================================\n  # 2) Proposed — PM-Drop + ZeRO-2 (no FlashAttention)\n  # ===========================================================================\n  - run_id: pm-drop-zeor2\n    description: \"Packed-Mask Dropout with ZeRO-2\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 3\n      learning_rate: 3e-5\n      use_pmdrop: true\n      fp16: true\n      use_flash_attn: false\n      use_tempo: false\n\n  # ===========================================================================\n  # 3) PM-Drop + ZeRO-2 + FlashAttention-2 (stub)\n  # ===========================================================================\n  - run_id: pm-drop-zeor2-flash\n    description: \"PM-Drop combined with FlashAttention-2 (stub integration)\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 3\n      learning_rate: 3e-5\n      use_pmdrop: true\n      fp16: true\n      use_flash_attn: true\n      use_tempo: false\n\n  # ===========================================================================\n  # 4) PM-Drop + ZeRO-2 + FlashAttention-2 + Tempo (all techniques enabled)\n  # ===========================================================================\n  - run_id: pm-drop-zeor2-flash-tempo\n    description: \"PM-Drop + FlashAttention-2 + Tempo activation off-loading (all techniques)\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 3\n      learning_rate: 3e-5\n      use_pmdrop: true\n      fp16: true\n      use_flash_attn: true\n      use_tempo: true\n\n  # ---------------------------------------------------------------------------\n  # Additional ablations such as different learning-rates or dropout-p values\n  # can be appended here following the same schema.\n  # ---------------------------------------------------------------------------\n  \n  # Example hyper-parameter study for learning-rate & dropout-p under PM-Drop\n  - run_id: pm-drop-zeor2-lr1e4\n    description: \"PM-Drop with learning-rate 1e-4 (hyper-param study)\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 3\n      learning_rate: 1e-4\n      use_pmdrop: true\n      fp16: true\n      use_flash_attn: false\n      use_tempo: false\n\n  - run_id: pm-drop-zeor2-lr2e4\n    description: \"PM-Drop with learning-rate 2e-4 (hyper-param study)\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 3\n      learning_rate: 2e-4\n      use_pmdrop: true\n      fp16: true\n      use_flash_attn: false\n      use_tempo: false\n\n  - run_id: pm-drop-zeor2-dropout0\n    description: \"PM-Drop with dropout-p = 0.0 (hyper-param study)\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 3\n      learning_rate: 3e-5\n      use_pmdrop: true\n      fp16: true\n      use_flash_attn: false\n      use_tempo: false\n      dropout_p_override: 0.0\n\n  - run_id: pm-drop-zeor2-dropout01\n    description: \"PM-Drop with dropout-p = 0.1 (hyper-param study)\"\n    seed: 42\n    dataset:\n      name: Lots-of-LoRAs/task1288_glue_mrpc_paraphrasing\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: hf\n      name: allenai/longformer-base-4096\n    training:\n      epochs: 3\n      learning_rate: 3e-5\n      use_pmdrop: true\n      fp16: true\n      use_flash_attn: false\n      use_tempo: false\n      dropout_p_override: 0.1\n\n  # End of configuration list\n  # ===========================================================================\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  # ===========================================================================\n  # End of full_experiment.yaml\n  # ===========================================================================\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  "
}
