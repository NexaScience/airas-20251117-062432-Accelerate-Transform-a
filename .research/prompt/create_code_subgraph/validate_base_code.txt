
Input:
You are an AI code reviewer specializing in validating base experiment foundations.

Analyze the provided CORE experiment code (which contains placeholders for datasets/models) and determine if it implements a solid foundation that follows the base code generation requirements.

# Instructions

## Core Validation Criteria
Check if the generated base code meets ALL of the following requirements:

1. **Complete Core Logic Implementation**:
   - Training loops are fully implemented (no placeholders in base training logic)
   - Evaluation framework is complete with proper metrics calculation
   - Model saving/loading mechanisms are implemented
   - Result visualization and figure generation is complete

2. **Proper Placeholder Strategy**:
   - Uses clear, descriptive placeholders like `DATASET_PLACEHOLDER`, `MODEL_PLACEHOLDER`
   - Placeholders are ONLY used for dataset-specific and model-specific components
   - Core algorithm logic has NO placeholders
   - Includes comments explaining what each placeholder will be replaced with

3. **8-File Structure Compliance**:
   - Contains EXACTLY these 8 required files:
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/smoke_test.yaml`
     * `config/full_experiment.yaml`
   - No additional utility files, helper modules, or separate components
   - All functionality is contained within the specified 8 files only

4. **Command Line Interface & Module Structure**:
   - main.py properly supports `--smoke-test` and `--full-experiment` flags with `--results-dir <path>` argument
   - main.py reads configuration YAML files and launches train.py for each run variation sequentially
   - main.py executes run variations one at a time in sequential order
   - main.py redirects each subprocess stdout/stderr to `{results_dir}/{run_id}/stdout.log` and `stderr.log` while forwarding to main stdout/stderr
   - train.py outputs JSON-formatted metrics with `run_id` field using `print(json.dumps({...}))`
   - evaluate.py outputs JSON-formatted comparison results to stdout
   - Configuration YAML structure is ready to accept run variations (specific values will be added in derive_specific step)
   - Import statements are compatible with `uv run python -m src.main` execution

5. **Publication-Ready Infrastructure**:
   - Figure generation with proper formatting (PDF output to `{results_dir}/images/` directory, legends, annotations)
   - Consistent result formatting and comparison logic
   - Proper experimental description output

6. **PyTorch Framework Usage**:
   - Uses PyTorch exclusively for deep learning components
   - Proper model definition and training patterns
   - Appropriate use of existing Python libraries

7. **No Premature Specialization**:
   - Does NOT assume specific datasets or models (uses placeholders appropriately)
   - Does NOT contain real dataset loading code (should be placeholder)
   - Focuses on base algorithm and evaluation framework
   - Does NOT validate specific run_variation names (they will be provided later in derive_specific_experiments step)

## Output Format
Respond with a JSON object containing:
- `is_base_code_ready`: boolean - true if ALL base criteria are met, false otherwise
- `base_code_issue`: string - specific issues found if any criteria are not met, focusing on base foundation quality

# Current Research Method
{
    "Open Problems": "Tempo removes most large activations but still keeps all binary Dropout / Attention masks as 1-byte (uint8) tensors.  For long-sequence Transformers these masks alone can occupy hundreds of MB and become the new memory bottleneck.  The key open problem is: how can we store those strictly-binary tensors with 1-bit precision while keeping back-prop fast and fully compatible with PyTorch autograd?",
    "Methods": "PackedMask Dropout (PM-Drop)\n1. Replace every Dropout (and future Attention, Head & MLP binary gates) by a custom autograd function that bit-packs the Boolean mask.\n   • Forward: generate the standard Boolean mask M (torch.bool), reshape to (N, .., 8) and compress 8 bits into a single uint8 using bitwise   operations (torch.bitwise_or & left-shift).  Store the packed mask P and dropout scale 1⁄(1-p).\n   • Backward: on first use, unpack P back to the Boolean mask with bitwise_and & right-shift.  Multiply the unpacked mask with upstream gradient as usual.\n2. Because only the mask tensor is changed, the objective function and model weights stay untouched—training dynamics are identical.\nTheoretical motivation: masks carry exactly 1 bit of information; storing them in 8 bits is an 8× waste.  Since mask generation and application are element-wise, the extra (un)packing cost is O(#elements) and completely hidden by GPU memory-bandwidth savings for practical batch sizes.",
    "Experimental Setup": "Model: Hugging-face TinyBERT (4-layer) fine-tuned on the MRPC subset of GLUE (to keep the run <5 min on a single GPU).\nBaseline: standard Dropout.\nProposed: replace every torch.nn.Dropout by PM-Drop (one-liner import).\nMetrics:\n• GPU peak memory (torch.cuda.max_memory_allocated).\n• Training throughput (sequences/s).\n• Validation accuracy/F1 to confirm identical learning behaviour.\nExpectation: ~5-10 % lower overall memory (≈8× lower mask memory) with <1 % throughput change.",
    "Experimental Code": "import torch, torch.nn as nn\n\nclass PackedDropoutFn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, p, training):\n        if not training or p == 0.0:\n            ctx.save_for_backward(None)\n            return x\n        keep_prob = 1 - p\n        mask = torch.rand_like(x) < keep_prob  # bool\n        # pack every 8 bits into one byte\n        flat = mask.flatten()\n        pad = (-flat.numel()) % 8  # pad to multiple of 8\n        if pad:\n            flat = torch.cat([flat, flat.new_zeros(pad)])\n        flat = flat.view(-1, 8)\n        byte = sum((flat[:, i].byte() << i) for i in range(8))\n        packed = byte.contiguous()\n        ctx.save_for_backward(packed)\n        ctx.shape = x.shape\n        ctx.p = p\n        return x * mask.div(keep_prob)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        packed, = ctx.saved_tensors\n        if packed is None:\n            return grad_out, None, None\n        # unpack\n        byte = packed.view(-1, 1)\n        bits = (byte >> torch.arange(8, device=byte.device)) & 1\n        mask = bits.flatten()[:grad_out.numel()].view(ctx.shape).bool()\n        grad_in = grad_out * mask.div(1 - ctx.p)\n        return grad_in, None, None\n\nclass PackedDropout(nn.Module):\n    def __init__(self, p=0.1):\n        super().__init__(); self.p = p\n    def forward(self, x):\n        return PackedDropoutFn.apply(x, self.p, self.training)\n\n# --- quick sanity test ---\nif __name__ == \"__main__\":\n    x = torch.randn(2, 3, 4, device='cuda')\n    pd = PackedDropout(0.2).cuda()\n    y = pd(x).sum()\n    y.backward()\n    print('peak mem:', torch.cuda.max_memory_allocated()/1e6, 'MB')",
    "Expected Result": "1. Memory: Mask storage reduced by ≈8×.  For TinyBERT with sequence length 512 and batch 64 this cuts total GPU memory by ~6 %.  For BERT-Large the saving rises to 400–600 MB enabling +10-15 % larger batch size.\n2. Speed: (Un)packing adds ≤0.3 ms per step; throughput change within ±1 %.\n3. Accuracy/F1 on MRPC remains statistically identical (same stochastic graph).",
    "Expected Conclusion": "PM-Drop fixes an overlooked inefficiency in state-of-the-art memory-optimized training: binary masks are stored with 8× too many bits.  Bit-packing is a one-line, framework-friendly change that frees hundreds of MB on typical Transformer workloads, converts directly into larger batch/sequence sizes, and composes seamlessly with existing techniques such as Tempo or activation checkpointing.  The idea is simple, yet delivers measurable acceleration without touching model quality or requiring special hardware."
}

# Experimental Design
## Experiment Strategy
Overall Experimental Strategy for Validating Packed-Mask Dropout (PM-Drop)

1. Core Hypotheses to Validate
   a. Memory Efficiency: PM-Drop reduces mask-related memory by ≈8× and total GPU memory by ≥5 % on small models and ≥10 % on large/long-sequence models.
   b. Computational Overhead: (Un)packing cost is amortised—training throughput changes by ≤±2 % and GPU utilisation is unaffected.
   c. Training Fidelity: PM-Drop is a strict functional drop-in; learning curves, final accuracy/F1, and gradient statistics are indistinguishable from the standard implementation.
   d. Generalisation & Robustness: Benefits hold across model sizes (Tiny → Large), domains (NLP, vision, multimodal), sequence lengths, precisions (FP32/BF16/FP16), and training regimes (single-GPU, DDP, ZeRO-1/2, activation checkpointing).
   e. Compatibility: PM-Drop composes transparently with other memory/time-saving techniques (Tempo, FlashAttention, LoRA, etc.).

2. Experimental Axes & Required Comparisons
   • Baseline: Torch nn.Dropout / attention masks in uint8.
   • State-of-the-art Memory Toolkits: Tempo + Checkpointing; DeepSpeed-ZeRO-2; xFormers / FlashAttention variants.
   • Ablations: (i) PM-Drop forward only (unpack every step) vs cached-unpack; (ii) different bit-packing widths (4-bit, 2-bit) for stress-testing; (iii) turning packing off for attention masks only or MLP masks only.
   • Scaling Experiments: Multiple models (TinyBERT, BERT-Base, BERT-Large, GPT-J, Longformer, ViT-B/16) and sequence/batch sweeps to map memory/throughput curves.

3. Validation Angles & Metrics
   Quantitative:
     - Peak/average GPU memory (torch.cuda.max_memory_*).
     - Max feasible batch or sequence length on a fixed 80 GB A100.
     - Step-time, tokens-per-second, TFLOPs utilisation from nvprof/Nsight.
     - End-of-training task metrics (accuracy/F1/BLEU/perplexity depending on task).
     - Gradient norm, loss curve overlay, and cosine similarity of parameter updates (ensures identical optimisation dynamics).
   Qualitative:
     - Convergence plots across seeds.
     - Case-study of an OOM-with-baseline vs fits-with-PM-Drop scenario.
     - Code diff (LoC) and integration friction survey.
   Computational Cost:
     - Micro-benchmark the forward/backward kernel time for dropout layers in isolation.
     - PCIe dram-read/write counters to show bandwidth trade-off.

4. Multi-Perspective Demonstrations
   1) Micro Level: Layer-wise synthetic benchmarks isolating dropout to confirm 8× mask compression and ≤0.3 ms overhead.
   2) Model Level: Fine-tune & pre-train workloads showing memory-to-throughput/accuracy Pareto curves.
   3) System Level: Full training stack with ZeRO-2 + activation-checkpointing demonstrating either (a) 10–15 % larger batch or (b) 1.1–1.3× longer context length on the same GPU.
   4) Edge Case Stress Tests: Extremely long-sequence (8k–32k tokens) Transformers where mask memory dominates.
   5) Robustness: 5 random seeds × 3 datasets; report mean±std.

5. Success Criteria
   • Memory: ≥7.5× compression of Boolean masks; ≥5 % end-to-end memory saving on small models; ≥10 % on large/long-sequence.
   • Performance: Training throughput delta within ±2 % of baseline; forward/backward wall-time per step not statistically slower (t-test p>0.05).
   • Accuracy: Task metric difference ≤0.2 σ of baseline run-to-run variance; no divergence observed over 3 seeds.
   • Gradient Check: Autograd gradcheck passes (finite-diff error <1e-5) on representative layers.
   • Scalability: Linear (or better) memory saving trend with sequence length; unlock at least one configuration that baseline cannot fit.
   • Compatibility: No additional code changes needed outside one-line import; framework features (torch.compile, DDP, AMP) operate unmodified.

6. Practical Considerations & Environment Alignment
   • All experiments confined to one or four A100-80GB nodes; batch/sequence sweeps capped to avoid exceeding 80 GB per GPU.
   • Use PyTorch 2.1 + CUDA 12.2; enable torch.autograd.profiler & Nsight Systems for low-overhead tracing.
   • Ensure cudnn_deterministic=True and seed control for fairness in convergence/accuracy comparison.
   • Log with Weights & Biases; publish scripts so any result can be replicated on a single A100 within 2 hours (micro/model-level) or 24 hours (system-level).

This unified strategy guarantees that every subsequent experiment—whether a tiny micro-benchmark or a full-scale long-sequence pre-training run—collects a consistent, multi-angle evidence bundle, enabling a conclusive assessment of PM-Drop’s memory, speed, correctness, and practical impact.

# Generated Base Code Files
{"evaluate_py": "import argparse\nimport json\nimport os\nfrom typing import List, Dict, Any\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom rich.console import Console\n\nsns.set(style=\"whitegrid\")\nconsole = Console()\n\nFIG_DIR_NAME = \"images\"\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Evaluate and compare all experiment variations.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory containing all run sub-directories.\")\n    parser.add_argument(\"--config-path\", type=str, required=True, help=\"Path to the YAML configuration file used for the run set.\")\n    return parser.parse_args()\n\n\ndef load_all_results(results_dir: str) -\u003e List[Dict[str, Any]]:\n    records = []\n    for run_id in os.listdir(results_dir):\n        run_path = os.path.join(results_dir, run_id, \"results.json\")\n        if not os.path.isfile(run_path):\n            continue\n        with open(run_path, \"r\") as f:\n            records.append(json.load(f))\n    return records\n\n\ndef save_fig(fig, results_dir: str, filename: str):\n    img_dir = os.path.join(results_dir, FIG_DIR_NAME)\n    os.makedirs(img_dir, exist_ok=True)\n    path = os.path.join(img_dir, filename)\n    fig.tight_layout()\n    fig.savefig(path, bbox_inches=\"tight\", format=\"pdf\")\n    console.print(f\"[bold cyan]Saved figure:[/] {path}\")\n\n\ndef plot_training_curves(df_epochs: pd.DataFrame, metric: str, results_dir: str):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.lineplot(data=df_epochs, x=\"epoch\", y=metric, hue=\"run_id\", marker=\"o\", ax=ax)\n    for run_id, sub in df_epochs.groupby(\"run_id\"):\n        best_point = sub.iloc[-1]\n        ax.annotate(f\"{best_point[metric]:.3f}\", (best_point[\"epoch\"], best_point[metric]))\n    ax.set_title(f\"{metric} over epochs\")\n    save_fig(fig, results_dir, f\"{metric}.pdf\")\n\n\ndef plot_bar(df_final: pd.DataFrame, metric: str, results_dir: str):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    sns.barplot(data=df_final, x=\"run_id\", y=metric, palette=\"deep\", ax=ax)\n    for idx, row in df_final.iterrows():\n        ax.text(idx, row[metric] * 1.01, f\"{row[metric]:.2f}\", ha=\"center\")\n    ax.set_title(metric)\n    save_fig(fig, results_dir, f\"{metric}.pdf\")\n\n\ndef main():\n    args = parse_args()\n    records = load_all_results(args.results_dir)\n\n    if not records:\n        console.print(\"[red]No result files found. Evaluation aborted.\")\n        return\n\n    # Flatten epoch metrics\n    epoch_rows = []\n    final_rows = []\n    for rec in records:\n        run_id = rec[\"run_id\"]\n        for epoch_entry in rec[\"epoch_metrics\"]:\n            epoch_rows.append({\"run_id\": run_id, **epoch_entry})\n        final = rec[\"final_metrics\"]\n        final_rows.append({\"run_id\": run_id, **final})\n\n    df_epochs = pd.DataFrame(epoch_rows)\n    df_final = pd.DataFrame(final_rows)\n\n    # Generate required figures\n    plot_training_curves(df_epochs, \"train_loss\", args.results_dir)\n    plot_training_curves(df_epochs, \"val_accuracy\", args.results_dir)\n    plot_bar(df_final, \"peak_memory_mb\", args.results_dir)\n    plot_bar(df_final, \"throughput_seq_per_sec\", args.results_dir)\n\n    # Structured comparison output\n    comparison = df_final.set_index(\"run_id\").to_dict(orient=\"index\")\n    console.rule(\"[bold green]Aggregated Comparison Results\")\n    console.print_json(data=comparison)\n\n\nif __name__ == \"__main__\":\n    main()", "full_experiment_yaml": "# Full experiment configuration template. Populate with concrete datasets, models, and ablation settings in the next phase.\nvariations:\n  - run_id: BASELINE_PLACEHOLDER\n    description: \"Baseline run with standard Dropout\"\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER\n      subset: SUBSET_PLACEHOLDER  # PLACEHOLDER\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: MODEL_PLACEHOLDER\n      name: MODEL_NAME_PLACEHOLDER\n    training:\n      epochs: 3\n      learning_rate: 3e-5\n      use_pmdrop: false\n      fp16: true\n\n  - run_id: PMDROP_PLACEHOLDER\n    description: \"Proposed PM-Drop with packed masks\"\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER\n      subset: SUBSET_PLACEHOLDER\n      seq_length: 128\n      batch_size: 32\n      num_classes: 2\n    model:\n      type: MODEL_PLACEHOLDER\n      name: MODEL_NAME_PLACEHOLDER\n    training:\n      epochs: 3\n      learning_rate: 3e-5\n      use_pmdrop: true\n      fp16: true\n\n  # Additional ablation and scaling variations can be inserted here.\n  # PLACEHOLDER: Add more variations as needed for the full experiment suite.", "main_py": "import argparse\nimport json\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom typing import Dict, Any, List\n\nimport yaml\nfrom rich.console import Console\nfrom rich.progress import Progress\n\nconsole = Console()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Main orchestrator for PM-Drop experiments.\")\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml configuration.\")\n    group.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Root directory for all outputs.\")\n    return parser.parse_args()\n\n\ndef load_config(path: str) -\u003e Dict[str, Any]:\n    with open(path, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef tee_subprocess(cmd: List[str], stdout_path: str, stderr_path: str):\n    with open(stdout_path, \"w\") as out_f, open(stderr_path, \"w\") as err_f:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        while True:\n            out_line = process.stdout.readline()\n            err_line = process.stderr.readline()\n            if out_line:\n                console.print(out_line.rstrip())\n                out_f.write(out_line)\n            if err_line:\n                console.print(err_line.rstrip(), style=\"red\")\n                err_f.write(err_line)\n            if out_line == \"\" and err_line == \"\" and process.poll() is not None:\n                break\n        return process.returncode\n\n\ndef run_variation(run_cfg: Dict[str, Any], results_dir: str, smoke: bool):\n    run_id = run_cfg[\"run_id\"]\n    run_dir = os.path.join(results_dir, run_id)\n    os.makedirs(run_dir, exist_ok=True)\n\n    # Serialize run config to a temporary JSON file\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\") as tmp:\n        json.dump(run_cfg, tmp)\n        tmp_path = tmp.name\n\n    cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.train\",\n        \"--run-config\",\n        tmp_path,\n        \"--results-dir\",\n        results_dir,\n    ]\n    if smoke:\n        cmd.append(\"--smoke-test\")\n\n    stdout_path = os.path.join(run_dir, \"stdout.log\")\n    stderr_path = os.path.join(run_dir, \"stderr.log\")\n\n    console.rule(f\"[bold yellow]Launching Run \u2014 {run_id}\")\n    retcode = tee_subprocess(cmd, stdout_path, stderr_path)\n    if retcode != 0:\n        console.print(f\"[red]Run {run_id} failed with return code {retcode}\")\n    else:\n        console.print(f\"[green]Run {run_id} completed successfully.\")\n\n\ndef main():\n    args = parse_args()\n\n    cfg_path = \"config/smoke_test.yaml\" if args.smoke_test else \"config/full_experiment.yaml\"\n    master_cfg = load_config(cfg_path)\n    variations = master_cfg.get(\"variations\", [])\n\n    os.makedirs(args.results_dir, exist_ok=True)\n\n    with Progress() as progress:\n        task = progress.add_task(\"Running variations\", total=len(variations))\n        for run_cfg in variations:\n            run_variation(run_cfg, args.results_dir, smoke=args.smoke_test)\n            progress.advance(task)\n\n    # After all runs, trigger evaluation\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        args.results_dir,\n        \"--config-path\",\n        cfg_path,\n    ]\n    console.rule(\"[bold magenta]Aggregating Results via evaluate.py\")\n    subprocess.run(eval_cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, AutoConfig\n\n__all__ = [\n    \"PackedDropoutFn\",\n    \"PackedDropout\",\n    \"apply_packed_dropout\",\n    \"get_model\",\n]\n\n\nclass PackedDropoutFn(torch.autograd.Function):\n    \"\"\"Packed-mask dropout storing boolean mask in a bit-packed uint8 tensor.\"\"\"\n\n    @staticmethod\n    def forward(ctx, x: torch.Tensor, p: float, training: bool):\n        if not training or p == 0.0:\n            ctx.save_for_backward(None)\n            ctx.p = p\n            return x\n        keep_prob = 1 - p\n        mask = torch.rand_like(x) \u003c keep_prob  # bool tensor\n        flat = mask.flatten()\n        pad = (-flat.numel()) % 8\n        if pad:\n            flat = torch.cat([flat, flat.new_zeros(pad)])\n        flat = flat.view(-1, 8)\n        packed = sum((flat[:, i].byte() \u003c\u003c i) for i in range(8)).contiguous()\n        ctx.save_for_backward(packed)\n        ctx.shape = x.shape\n        ctx.p = p\n        return x * mask.div(keep_prob)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        packed, = ctx.saved_tensors\n        if packed is None:\n            return grad_out, None, None\n        byte = packed.view(-1, 1)\n        bits = (byte \u003e\u003e torch.arange(8, device=byte.device)) \u0026 1\n        mask = bits.flatten()[:grad_out.numel()].view(ctx.shape).bool()\n        grad_in = grad_out * mask.div(1 - ctx.p)\n        return grad_in, None, None\n\n\nclass PackedDropout(nn.Module):\n    def __init__(self, p: float = 0.1):\n        super().__init__()\n        self.p = p\n\n    def forward(self, x):\n        return PackedDropoutFn.apply(x, self.p, self.training)\n\n\n# Recursive replacement helper\n\ndef apply_packed_dropout(module: nn.Module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Dropout):\n            setattr(module, name, PackedDropout(child.p))\n        else:\n            apply_packed_dropout(child)\n\n\ndef get_model(run_cfg, tokenizer):\n    \"\"\"Return a HF model; applies Packed Mask replacement if requested.\"\"\"\n    model_name = run_cfg[\"model\"].get(\"name\", \"bert-base-uncased\")\n    num_labels = run_cfg[\"dataset\"].get(\"num_classes\", 2)\n    cfg = AutoConfig.from_pretrained(model_name, num_labels=num_labels, use_auth_token=None)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, config=cfg, use_auth_token=None)\n\n    if run_cfg[\"training\"].get(\"use_pmdrop\", False):\n        apply_packed_dropout(model)\n    return model", "preprocess_py": "import os\nimport random\nfrom typing import Tuple, Any, Dict\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom transformers import AutoTokenizer\n\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\n# PLACEHOLDER: Will be replaced with specific dataset loading logic\n# This generic loader already handles Hugging-Face datasets and a synthetic fall-back.\n\ndef get_tokenizer(run_cfg: Dict[str, Any]):\n    model_name = run_cfg[\"model\"].get(\"name\", \"bert-base-uncased\")\n    return AutoTokenizer.from_pretrained(model_name, use_auth_token=os.getenv(\"HF_TOKEN\"))\n\n\ndef _build_dataloader(inputs: Dict[str, torch.Tensor], labels: torch.Tensor, batch_size: int):\n    dataset = TensorDataset(*inputs.values(), labels)\n\n    def _collate(batch):\n        elem = list(zip(*batch))\n        batch_dict = {k: torch.stack(v) for k, v in zip(inputs.keys(), elem[:-1])}\n        batch_dict[\"labels\"] = torch.stack(elem[-1])\n        return batch_dict\n\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=_collate)\n\n\ndef _load_synthetic(run_cfg: Dict[str, Any], tokenizer, smoke_test: bool):\n    seq_len = run_cfg[\"dataset\"].get(\"seq_length\", 32)\n    num_classes = run_cfg[\"dataset\"].get(\"num_classes\", 2)\n    num_samples = 200 if smoke_test else run_cfg[\"dataset\"].get(\"num_samples\", 2000)\n\n    input_ids = torch.randint(low=0, high=tokenizer.vocab_size, size=(num_samples, seq_len))\n    attention_mask = torch.ones_like(input_ids)\n    labels = torch.randint(low=0, high=num_classes, size=(num_samples,))\n\n    split = int(0.8 * num_samples)\n    train_inputs = {\"input_ids\": input_ids[:split], \"attention_mask\": attention_mask[:split]}\n    val_inputs = {\"input_ids\": input_ids[split:], \"attention_mask\": attention_mask[split:]}\n    train_labels, val_labels = labels[:split], labels[split:]\n\n    train_loader = _build_dataloader(train_inputs, train_labels, run_cfg[\"dataset\"].get(\"batch_size\", 32))\n    val_loader = _build_dataloader(val_inputs, val_labels, run_cfg[\"dataset\"].get(\"batch_size\", 32))\n    return train_loader, val_loader\n\n\ndef load_dataset(run_cfg: Dict[str, Any], tokenizer, smoke_test: bool = False) -\u003e Tuple[Any, Any]:\n    \"\"\"Return (train_loader, val_loader)\"\"\"\n    ds_name = run_cfg[\"dataset\"].get(\"name\", \"SYNTHETIC_CLASSIFICATION\")\n\n    if \"PLACEHOLDER\" in ds_name or ds_name == \"SYNTHETIC_CLASSIFICATION\":\n        return _load_synthetic(run_cfg, tokenizer, smoke_test)\n\n    # Hugging-Face dataset path\n    subset = run_cfg[\"dataset\"].get(\"subset\", None)\n    dataset = load_dataset(ds_name, subset)\n\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=run_cfg[\"dataset\"].get(\"seq_length\", 128))\n\n    dataset = dataset.map(tokenize_function, batched=True)\n    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n    # Split if validation not present\n    if \"validation\" not in dataset:\n        dataset[\"train\"], dataset[\"validation\"] = random_split(dataset[\"train\"], [int(0.8 * len(dataset[\"train\"])), len(dataset[\"train\"]) - int(0.8 * len(dataset[\"train\"]))])\n\n    def hf_collate(batch):\n        input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n        attention_mask = torch.stack([x[\"attention_mask\"] for x in batch])\n        labels = torch.tensor([x[\"label\"] for x in batch])\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\n    train_loader = DataLoader(dataset[\"train\"], batch_size=run_cfg[\"dataset\"].get(\"batch_size\", 32), shuffle=True, collate_fn=hf_collate)\n    val_loader = DataLoader(dataset[\"validation\"], batch_size=run_cfg[\"dataset\"].get(\"batch_size\", 32), shuffle=False, collate_fn=hf_collate)\n    return train_loader, val_loader", "pyproject_toml": "[project]\nname = \"pm-drop-common-core\"\nversion = \"0.1.0\"\ndescription = \"Common core foundation for Packed Mask Dropout experiments\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0.0\"\ntransformers = \"\u003e=4.34.0\"\ndatasets = \"\u003e=2.14.0\"\nscikit-learn = \"\u003e=1.1.3\"\npyyaml = \"\u003e=6.0\"\nmatplotlib = \"\u003e=3.7.0\"\nseaborn = \"\u003e=0.12.0\"\npandas = \"\u003e=2.0.3\"\nrich = \"\u003e=13.5.2\"\ntqdm = \"\u003e=4.66.0\"", "smoke_test_yaml": "# Lightweight smoke test configuration. Replace placeholders in the next phase.\nvariations:\n  - run_id: BASELINE_PLACEHOLDER\n    description: \"Baseline with standard Dropout (smoke test)\"\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER: Will be replaced with concrete dataset name\n      seq_length: 32\n      num_samples: 200\n      batch_size: 16\n      num_classes: 2\n    model:\n      type: MODEL_PLACEHOLDER  # PLACEHOLDER: Will be replaced with model type (e.g., hf)\n      name: MODEL_NAME_PLACEHOLDER  # PLACEHOLDER: HF model checkpoint name\n    training:\n      epochs: 1\n      learning_rate: 5e-5\n      use_pmdrop: false\n      fp16: false\n\n  - run_id: PMDROP_PLACEHOLDER\n    description: \"Proposed Packed Mask Dropout (smoke test)\"\n    seed: 42\n    dataset:\n      name: DATASET_PLACEHOLDER  # PLACEHOLDER\n      seq_length: 32\n      num_samples: 200\n      batch_size: 16\n      num_classes: 2\n    model:\n      type: MODEL_PLACEHOLDER\n      name: MODEL_NAME_PLACEHOLDER\n    training:\n      epochs: 1\n      learning_rate: 5e-5\n      use_pmdrop: true\n      fp16: false", "train_py": "import argparse\nimport json\nimport os\nimport random\nimport time\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom rich.console import Console\nfrom rich.table import Table\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nfrom src.preprocess import set_seed, get_tokenizer, load_dataset\nfrom src.model import get_model\n\nconsole = Console()\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Run a single experiment variation.\")\n    parser.add_argument(\"--run-config\", type=str, required=True, help=\"Path to a JSON file containing a single run variation configuration.\")\n    parser.add_argument(\"--results-dir\", type=str, required=True, help=\"Directory in which to store all run outputs.\")\n    parser.add_argument(\"--smoke-test\", action=\"store_true\", help=\"If set, override epochs \u0026 dataset size for faster execution.\")\n    return parser.parse_args()\n\n\ndef build_optimizer(model: nn.Module, lr: float):\n    return torch.optim.AdamW(model.parameters(), lr=lr)\n\n\ndef evaluate(model: nn.Module, dataloader: DataLoader, device: torch.device):\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**{k: v for k, v in batch.items() if k != \"labels\"})\n            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs[0]\n            preds.append(torch.argmax(logits, dim=-1).cpu())\n            labels.append(batch[\"labels\"].cpu())\n    preds = torch.cat(preds).numpy()\n    labels = torch.cat(labels).numpy()\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    return acc, f1\n\n\ndef main():\n    args = parse_args()\n\n    with open(args.run_config, \"r\") as f:\n        run_cfg: Dict[str, Any] = json.load(f)\n\n    run_id = run_cfg[\"run_id\"]\n    results_dir = os.path.join(args.results_dir, run_id)\n    os.makedirs(results_dir, exist_ok=True)\n\n    # Structured run description printed before numerical data (requirement)\n    console.rule(f\"[bold cyan]Experiment Description \u2014 {run_id}\")\n    console.print_json(data=run_cfg)\n\n    # Save run config for posterity\n    with open(os.path.join(results_dir, \"config.json\"), \"w\") as f:\n        json.dump(run_cfg, f, indent=2)\n\n    # Reproducibility\n    seed = run_cfg.get(\"seed\", 42)\n    set_seed(seed)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    tokenizer = get_tokenizer(run_cfg)\n    train_loader, val_loader = load_dataset(run_cfg, tokenizer, smoke_test=args.smoke_test)\n\n    model = get_model(run_cfg, tokenizer)\n    model.to(device)\n\n    optimizer = build_optimizer(model, lr=run_cfg[\"training\"].get(\"learning_rate\", 5e-5))\n    scaler = GradScaler(enabled=run_cfg[\"training\"].get(\"fp16\", False))\n\n    epochs = 1 if args.smoke_test else run_cfg[\"training\"].get(\"epochs\", 3)\n\n    epoch_metrics = []\n    global_step = 0\n    start_time = time.time()\n    torch.cuda.reset_peak_memory_stats(device)\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        epoch_loss, step_count = 0.0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n        for batch in pbar:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            optimizer.zero_grad()\n            with autocast(enabled=run_cfg[\"training\"].get(\"fp16\", False)):\n                outputs = model(**{k: v for k, v in batch.items() if k != \"labels\"}, labels=batch[\"labels\"])\n                loss = outputs.loss if hasattr(outputs, \"loss\") else outputs[0]\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            epoch_loss += loss.item()\n            step_count += 1\n            global_step += 1\n            pbar.set_postfix(loss=loss.item())\n\n        avg_train_loss = epoch_loss / max(step_count, 1)\n        val_acc, val_f1 = evaluate(model, val_loader, device)\n\n        epoch_entry = {\n            \"epoch\": epoch,\n            \"train_loss\": avg_train_loss,\n            \"val_accuracy\": val_acc,\n            \"val_f1\": val_f1,\n        }\n        epoch_metrics.append(epoch_entry)\n\n        # Structured per-epoch metrics to stdout\n        console.print_json(data={\"run_id\": run_id, \"epoch_metrics\": epoch_entry})\n\n    total_training_time = time.time() - start_time\n    peak_memory_mb = (\n        torch.cuda.max_memory_allocated(device) / 1024 ** 2 if torch.cuda.is_available() else 0\n    )\n    throughput_seq_per_sec = len(train_loader.dataset) * epochs / total_training_time\n\n    final_metrics = {\n        \"final_val_accuracy\": epoch_metrics[-1][\"val_accuracy\"],\n        \"final_val_f1\": epoch_metrics[-1][\"val_f1\"],\n        \"best_val_accuracy\": max(m[\"val_accuracy\"] for m in epoch_metrics),\n        \"best_val_f1\": max(m[\"val_f1\"] for m in epoch_metrics),\n        \"peak_memory_mb\": peak_memory_mb,\n        \"throughput_seq_per_sec\": throughput_seq_per_sec,\n        \"training_time_sec\": total_training_time,\n    }\n\n    # Save model checkpoint\n    torch.save(model.state_dict(), os.path.join(results_dir, \"model.pt\"))\n\n    # Persist structured results\n    results_obj = {\n        \"run_id\": run_id,\n        \"config\": run_cfg,\n        \"epoch_metrics\": epoch_metrics,\n        \"final_metrics\": final_metrics,\n        \"timestamp\": datetime.utcnow().isoformat(),\n    }\n\n    with open(os.path.join(results_dir, \"results.json\"), \"w\") as f:\n        json.dump(results_obj, f, indent=2)\n\n    # Print END-OF-RUN numerical data as JSON to stdout\n    console.rule(f\"[bold green]Final Results \u2014 {run_id}\")\n    console.print_json(data=results_obj)\n\n\nif __name__ == \"__main__\":\n    main()"}

Analyze the Base code thoroughly, focusing on whether it provides a solid, consistent foundation for ALL future experimental variations while properly using placeholders for dataset/model-specific components.
Output:
{
    "is_base_code_ready": false,
    "base_code_issue": "Two hard-requirements are violated: 1) The specification mandates that train.py and evaluate.py print plain JSON via print(json.dumps({...})), but the code uses rich.console.print_json which injects ANSI colour codes, so the stdout stream is **not valid raw JSON**. 2) The base skeleton must not contain real dataset/model loading logic; however preprocess.py already implements concrete Hugging-Face dataset/tokeniser loading (beyond synthetic placeholders), thereby prematurely specialising the core.  In addition, the code only implements model saving, not a complementary loading routine, so the “model saving/loading mechanisms” criterion is only partially met. Because of these issues the foundation does not fully satisfy the core generation requirements."
}
