
Input:
You are a cutting-edge AI researcher. Based on the new method described in # New Methods and the experimental policy outlined in # Experiment Strategy, please generate 2 distinct Experiment objects.

# Instructions
- Generate 2 major experimental lines (Experiment objects) based on the experimental strategy.
- Each Experiment (identified by experiment_id) represents a different experimental perspective or validation angle.
- Within each Experiment, run_variations are the variations that will be compared against each other (e.g., ["baseline", "proposed"], ["full-method", "ablation-A", "ablation-B"]).
- Keep run_variations to 3-5 variations per experiment (including baseline and proposed method) to ensure reasonable execution time and resource usage.
- Each Experiment should:
    - Have a unique experiment_id (e.g., "exp-1", "exp-2", "exp-3")
    - Have a clear description of its objective or hypothesis
    - Have a list of run_variations that will be compared within this experiment
    - Cover different aspects of validating the proposed method
- The experiments should be complementary and cover various validation angles such as:
    - Main performance validation
    - Ablation studies
    - Robustness tests
    - Comparison with baselines
    - Hyperparameter sensitivity analysis
    - Computational efficiency analysis
- Each experiment will have its own GitHub branch and code.
- The run_variations within each experiment define different configurations or conditions to test (e.g., different hyperparameters, different baselines, different datasets).

- Design the details of each experiment assuming the execution environment specified in "Experimental Environment."
- The experimental details should include the following for each experiment:
    - Machine learning / deep learning models to be used
        - If necessary, also include baseline models.
    - Datasets
    - Dataset preprocessing methods
    - Data splitting method (train/val/test, cross-validation)
    - Number of repetitions (number of seeds), averaging method, and selection criteria (best-val, last, early stopping)
    - Evaluation metrics
        - Primary and secondary metrics
        - Examples: Accuracy / F1 / AUROC (classification), RMSE / MAE (regression), mAP (detection), mIoU (segmentation), BLEU / ROUGE / METEOR (generation), NDCG / MRR (ranking), ECE / Brier Score (calibration)
    - Comparisons
        - Prior methods (strong baselines, SOTA, simple baselines), etc.
        - If there are implementation or configuration differences, note the adjustments in footnotes.
    - Methods for analyzing important hyperparameters (e.g., learning rate, temperature, k, thresholds)
    - Methods for assessing robustness
        - Resistance to noise injection, distribution shift (OOD), adversarial perturbations, and domain transfer
    - Computation of FLOPs, training/inference time, memory usage, and cost / wall-clock time
    - Example experimental code
- Avoid excessive redundancy across experiments. When a single experiment can cover multiple validation items, integrate them appropriately.
- NO-FALLBACK CONSTRAINT: Never suggest using synthetic/dummy/placeholder data.
- Also provide:
    - expected_models: A list of specific model names/architectures that will be used across all experiments (e.g., ["ResNet-50", "BERT-base", "GPT-3.5-turbo"])
    - expected_datasets: A list of specific dataset names that will be used across all experiments (e.g., ["CIFAR-10", "ImageNet", "IMDB Reviews"])

## Output Format
Please provide:
- experiments: A list of 2 Experiment objects, each with:
    - experiment_id: Unique identifier
    - run_variations: List of variation names/identifiers for this experiment
    - description: Detailed description including all aspects mentioned in the instructions
- expected_models: List of model names/architectures
- expected_datasets: List of dataset names

# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Tempo removes most large activations but still keeps all binary Dropout / Attention masks as 1-byte (uint8) tensors.  For long-sequence Transformers these masks alone can occupy hundreds of MB and become the new memory bottleneck.  The key open problem is: how can we store those strictly-binary tensors with 1-bit precision while keeping back-prop fast and fully compatible with PyTorch autograd?",
    "Methods": "PackedMask Dropout (PM-Drop)\n1. Replace every Dropout (and future Attention, Head & MLP binary gates) by a custom autograd function that bit-packs the Boolean mask.\n   • Forward: generate the standard Boolean mask M (torch.bool), reshape to (N, .., 8) and compress 8 bits into a single uint8 using bitwise   operations (torch.bitwise_or & left-shift).  Store the packed mask P and dropout scale 1⁄(1-p).\n   • Backward: on first use, unpack P back to the Boolean mask with bitwise_and & right-shift.  Multiply the unpacked mask with upstream gradient as usual.\n2. Because only the mask tensor is changed, the objective function and model weights stay untouched—training dynamics are identical.\nTheoretical motivation: masks carry exactly 1 bit of information; storing them in 8 bits is an 8× waste.  Since mask generation and application are element-wise, the extra (un)packing cost is O(#elements) and completely hidden by GPU memory-bandwidth savings for practical batch sizes.",
    "Experimental Setup": "Model: Hugging-face TinyBERT (4-layer) fine-tuned on the MRPC subset of GLUE (to keep the run <5 min on a single GPU).\nBaseline: standard Dropout.\nProposed: replace every torch.nn.Dropout by PM-Drop (one-liner import).\nMetrics:\n• GPU peak memory (torch.cuda.max_memory_allocated).\n• Training throughput (sequences/s).\n• Validation accuracy/F1 to confirm identical learning behaviour.\nExpectation: ~5-10 % lower overall memory (≈8× lower mask memory) with <1 % throughput change.",
    "Experimental Code": "import torch, torch.nn as nn\n\nclass PackedDropoutFn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, p, training):\n        if not training or p == 0.0:\n            ctx.save_for_backward(None)\n            return x\n        keep_prob = 1 - p\n        mask = torch.rand_like(x) < keep_prob  # bool\n        # pack every 8 bits into one byte\n        flat = mask.flatten()\n        pad = (-flat.numel()) % 8  # pad to multiple of 8\n        if pad:\n            flat = torch.cat([flat, flat.new_zeros(pad)])\n        flat = flat.view(-1, 8)\n        byte = sum((flat[:, i].byte() << i) for i in range(8))\n        packed = byte.contiguous()\n        ctx.save_for_backward(packed)\n        ctx.shape = x.shape\n        ctx.p = p\n        return x * mask.div(keep_prob)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        packed, = ctx.saved_tensors\n        if packed is None:\n            return grad_out, None, None\n        # unpack\n        byte = packed.view(-1, 1)\n        bits = (byte >> torch.arange(8, device=byte.device)) & 1\n        mask = bits.flatten()[:grad_out.numel()].view(ctx.shape).bool()\n        grad_in = grad_out * mask.div(1 - ctx.p)\n        return grad_in, None, None\n\nclass PackedDropout(nn.Module):\n    def __init__(self, p=0.1):\n        super().__init__(); self.p = p\n    def forward(self, x):\n        return PackedDropoutFn.apply(x, self.p, self.training)\n\n# --- quick sanity test ---\nif __name__ == \"__main__\":\n    x = torch.randn(2, 3, 4, device='cuda')\n    pd = PackedDropout(0.2).cuda()\n    y = pd(x).sum()\n    y.backward()\n    print('peak mem:', torch.cuda.max_memory_allocated()/1e6, 'MB')",
    "Expected Result": "1. Memory: Mask storage reduced by ≈8×.  For TinyBERT with sequence length 512 and batch 64 this cuts total GPU memory by ~6 %.  For BERT-Large the saving rises to 400–600 MB enabling +10-15 % larger batch size.\n2. Speed: (Un)packing adds ≤0.3 ms per step; throughput change within ±1 %.\n3. Accuracy/F1 on MRPC remains statistically identical (same stochastic graph).",
    "Expected Conclusion": "PM-Drop fixes an overlooked inefficiency in state-of-the-art memory-optimized training: binary masks are stored with 8× too many bits.  Bit-packing is a one-line, framework-friendly change that frees hundreds of MB on typical Transformer workloads, converts directly into larger batch/sequence sizes, and composes seamlessly with existing techniques such as Tempo or activation checkpointing.  The idea is simple, yet delivers measurable acceleration without touching model quality or requiring special hardware."
}

# Experiment Strategy
Overall Experimental Strategy for Validating Packed-Mask Dropout (PM-Drop)

1. Core Hypotheses to Validate
   a. Memory Efficiency: PM-Drop reduces mask-related memory by ≈8× and total GPU memory by ≥5 % on small models and ≥10 % on large/long-sequence models.
   b. Computational Overhead: (Un)packing cost is amortised—training throughput changes by ≤±2 % and GPU utilisation is unaffected.
   c. Training Fidelity: PM-Drop is a strict functional drop-in; learning curves, final accuracy/F1, and gradient statistics are indistinguishable from the standard implementation.
   d. Generalisation & Robustness: Benefits hold across model sizes (Tiny → Large), domains (NLP, vision, multimodal), sequence lengths, precisions (FP32/BF16/FP16), and training regimes (single-GPU, DDP, ZeRO-1/2, activation checkpointing).
   e. Compatibility: PM-Drop composes transparently with other memory/time-saving techniques (Tempo, FlashAttention, LoRA, etc.).

2. Experimental Axes & Required Comparisons
   • Baseline: Torch nn.Dropout / attention masks in uint8.
   • State-of-the-art Memory Toolkits: Tempo + Checkpointing; DeepSpeed-ZeRO-2; xFormers / FlashAttention variants.
   • Ablations: (i) PM-Drop forward only (unpack every step) vs cached-unpack; (ii) different bit-packing widths (4-bit, 2-bit) for stress-testing; (iii) turning packing off for attention masks only or MLP masks only.
   • Scaling Experiments: Multiple models (TinyBERT, BERT-Base, BERT-Large, GPT-J, Longformer, ViT-B/16) and sequence/batch sweeps to map memory/throughput curves.

3. Validation Angles & Metrics
   Quantitative:
     - Peak/average GPU memory (torch.cuda.max_memory_*).
     - Max feasible batch or sequence length on a fixed 80 GB A100.
     - Step-time, tokens-per-second, TFLOPs utilisation from nvprof/Nsight.
     - End-of-training task metrics (accuracy/F1/BLEU/perplexity depending on task).
     - Gradient norm, loss curve overlay, and cosine similarity of parameter updates (ensures identical optimisation dynamics).
   Qualitative:
     - Convergence plots across seeds.
     - Case-study of an OOM-with-baseline vs fits-with-PM-Drop scenario.
     - Code diff (LoC) and integration friction survey.
   Computational Cost:
     - Micro-benchmark the forward/backward kernel time for dropout layers in isolation.
     - PCIe dram-read/write counters to show bandwidth trade-off.

4. Multi-Perspective Demonstrations
   1) Micro Level: Layer-wise synthetic benchmarks isolating dropout to confirm 8× mask compression and ≤0.3 ms overhead.
   2) Model Level: Fine-tune & pre-train workloads showing memory-to-throughput/accuracy Pareto curves.
   3) System Level: Full training stack with ZeRO-2 + activation-checkpointing demonstrating either (a) 10–15 % larger batch or (b) 1.1–1.3× longer context length on the same GPU.
   4) Edge Case Stress Tests: Extremely long-sequence (8k–32k tokens) Transformers where mask memory dominates.
   5) Robustness: 5 random seeds × 3 datasets; report mean±std.

5. Success Criteria
   • Memory: ≥7.5× compression of Boolean masks; ≥5 % end-to-end memory saving on small models; ≥10 % on large/long-sequence.
   • Performance: Training throughput delta within ±2 % of baseline; forward/backward wall-time per step not statistically slower (t-test p>0.05).
   • Accuracy: Task metric difference ≤0.2 σ of baseline run-to-run variance; no divergence observed over 3 seeds.
   • Gradient Check: Autograd gradcheck passes (finite-diff error <1e-5) on representative layers.
   • Scalability: Linear (or better) memory saving trend with sequence length; unlock at least one configuration that baseline cannot fit.
   • Compatibility: No additional code changes needed outside one-line import; framework features (torch.compile, DDP, AMP) operate unmodified.

6. Practical Considerations & Environment Alignment
   • All experiments confined to one or four A100-80GB nodes; batch/sequence sweeps capped to avoid exceeding 80 GB per GPU.
   • Use PyTorch 2.1 + CUDA 12.2; enable torch.autograd.profiler & Nsight Systems for low-overhead tracing.
   • Ensure cudnn_deterministic=True and seed control for fairness in convergence/accuracy comparison.
   • Log with Weights & Biases; publish scripts so any result can be replicated on a single A100 within 2 hours (micro/model-level) or 24 hours (system-level).

This unified strategy guarantees that every subsequent experiment—whether a tiny micro-benchmark or a full-scale long-sequence pre-training run—collects a consistent, multi-angle evidence bundle, enabling a conclusive assessment of PM-Drop’s memory, speed, correctness, and practical impact.

---


# Reference Information from Previous Iteration

Output:
{
    "experiments": [
        {
            "experiment_id": "exp-1-main-perf",
            "run_variations": [
                "baseline-std-drop",
                "pm-drop",
                "pm-drop-no-cache",
                "pm-drop-4bit"
            ],
            "description": "Objective / hypothesis: Quantitatively verify that Packed-Mask Dropout (PM-Drop) compresses dropout / attention masks by ≥7.5×, yields ≥5 % total GPU-memory reduction on small models and ≥10 % on large models, keeps throughput within ±2 %, and preserves task accuracy (Δ≤0.2 σ).  \n\nModels: TinyBERT-4L-312H, BERT-Base, BERT-Large (HuggingFace versions).  \nDatasets & tasks:  \n• GLUE MRPC (TinyBERT) – Accuracy / F1  \n• GLUE SST-2 (BERT-Base) – Accuracy  \n• WikiText-103 fine-tuning (BERT-Large) – Perplexity  \n\nPre-processing: standard WordPiece tokenisation (uncased), max sequence lengths = {128, 512, 1 024}.  \nData splits: official train / validation; test only for MRPC & SST-2 (via GLUE server).  \nSeeds & repetitions: 5 seeds × 3 models × 4 variations = 60 runs; early-stopping on best-val metric with patience = 3.  \n\nEvaluation metrics:  \nPrimary – peak GPU memory (torch.cuda.max_memory_allocated) and task metric (F1 / Accuracy / PPL).  \nSecondary – step time, tokens-per-second, TFLOPs utilisation (Nsight), gradient-norm similarity (cosine w.r.t. baseline).  \n\nComparisons:  \n• baseline-std-drop – stock torch.nn.Dropout  \n• pm-drop – proposed 1-bit packed mask, cached unpack  \n• pm-drop-no-cache – unpacks every forward call (tests compute overhead)  \n• pm-drop-4bit – packs 2 masks into 1 byte (stress-test reduced compression)  \n\nHyper-parameter sensitivity: grid over learning-rate {3e-5, 5e-5, 1e-4}. Report mean±std and plot memory vs throughput vs accuracy Pareto.  \n\nRobustness: inject 5 % random bit-flip noise into packed tensor before unpack (offline only) and show negligible accuracy change (<0.5 %).  \n\nEfficiency analysis: log FLOPs with ptflops, wall-clock train/infer time (time.perf_counter), and GPU memory-bandwidth counters.  \n\nExample core snippet (identical across runs, only flag differs):  \n```python\nfrom pm_drop import PackedDropout, set_mode  # our repo\nif args.variant == 'baseline-std-drop':\n    replace_dropout(model, torch.nn.Dropout)\nelif args.variant == 'pm-drop':\n    replace_dropout(model, PackedDropout, cache_unpacked=True)\nelif args.variant == 'pm-drop-no-cache':\n    replace_dropout(model, PackedDropout, cache_unpacked=False)\nelif args.variant == 'pm-drop-4bit':\n    set_mode(bits_per_mask=4)\n    replace_dropout(model, PackedDropout, cache_unpacked=True)\n```\n\nHardware counters and logs pushed to W&B for reproducibility. All 60 runs fit within 48 GPU-hours on one A100-80GB."
        },
        {
            "experiment_id": "exp-2-system-scale",
            "run_variations": [
                "baseline-zeor2",
                "pm-drop-zeor2",
                "pm-drop-zeor2-flash",
                "pm-drop-zeor2-flash-tempo"
            ],
            "description": "Objective / hypothesis: Demonstrate that PM-Drop remains fully compatible with state-of-the-art memory/time optimisation stacks (DeepSpeed-ZeRO-2, FlashAttention-2, Tempo activation offloading) and unlocks either (a) ≥12 % larger batch size or (b) ≥1.2× longer context length on a single A100-80GB when training long-sequence Transformers without hurting throughput/accuracy.  \n\nModel: Longformer-Base-4096 fine-tuned on Long Range Arena (LRA) ListOps-16k and ViT-B/16 fine-tuned on ImageNet-1k (for vision modality check).  \nDatasets:  \n• LRA ListOps-16k – sequence classification (Accuracy)  \n• ImageNet-1k – image classification (Top-1 / Top-5 accuracy)  \n\nPre-processing:  \n• LRA – tokenize as supplied (16 384 tokens), pad to nearest multiple of 256.  \n• ImageNet – 224×224 center crop, RandAugment. Dropout replaces DropPath + MLP dropout layers.  \n\nData split: 80 / 10 / 10 train/val/test for ListOps; official ImageNet train/val.  \nTraining regime: Mixed-precision (AMP BF16), DDP across 4 × A100 (one node).  \nBatch/seq sweep: automatically scale batch up until OOM; record max feasible.  \nSeeds: 3 per variation; select last checkpoint (no early stop) after fixed 90 epochs (ImageNet) or 30 k steps (ListOps).  \n\nEvaluation metrics:  \nPrimary – max batch or context length, peak memory, wall-clock epoch time.  \nSecondary – Accuracy (ListOps), Top-1/Top-5 (ImageNet), energy cost (nvidia-smi −q | power), and memory-bandwidth utilisation.  \n\nRun variations:  \n1. baseline-zeor2 – DeepSpeed-ZeRO-2 + activation-checkpointing + FlashAttention off  \n2. pm-drop-zeor2 – add PM-Drop  \n3. pm-drop-zeor2-flash – add FlashAttention-2  \n4. pm-drop-zeor2-flash-tempo – add both FlashAttention-2 and Tempo offloading  \n\nHyper-parameter study: for variation 2 run lr ∈ {1e-4, 2e-4} and dropout-p ∈ {0.0, 0.1}. Examine stability.  \n\nRobustness tests:  \n• Distribution shift – evaluate Longformer models on unseen ListOps length = 32 k tokens.  \n• Adversarial – FGSM ε=0.007 on ImageNet validation.  \nPM-Drop vs baseline accuracy drop compared.  \n\nComputational profiling: Nsight Systems & nvprof kernels stats; report cumulative (un)pack time percentage. Compute theoretical vs measured FLOPs and memory BW.  \n\nExample launcher excerpt:  \n```bash\naccelerate launch --multi_gpu --config deepspeed_zero2.yaml train.py \\\n    --model longformer-base-4096 \\\n    --use_flash_attn $FLASH \\\n    --use_tempo $TEMPO \\\n    --drop_impl $DROP_IMPL   # std | pm\n```\n\nExpected outcome: variation 2 shows ~13 % memory saving translating to +2 batch for ListOps and +64 image batch for ImageNet, throughput delta −0.8 % (ns). Variations 3-4 give cumulative gains while PM-Drop overhead stays <1 %. Establishes seamless composability."
        }
    ],
    "expected_models": [
        "TinyBERT-4L-312H",
        "BERT-Base",
        "BERT-Large",
        "Longformer-Base-4096",
        "ViT-B/16"
    ],
    "expected_datasets": [
        "GLUE MRPC",
        "GLUE SST-2",
        "WikiText-103",
        "Long Range Arena ListOps-16k",
        "ImageNet-1k"
    ]
}
